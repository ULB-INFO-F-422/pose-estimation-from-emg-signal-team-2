{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "895a75fb",
   "metadata": {},
   "source": [
    "# INFO-f422: ML Project\n",
    "\n",
    "authors:\n",
    "+ 1 \n",
    "+ 2\n",
    "+ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dcc5fc",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17578f83-f52e-47ee-9d73-b33d910722dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.set_printoptions(threshold=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493fe755",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31ae6158-83fc-4039-ad35-4168b9fed8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data\"\n",
    "\n",
    "X_g_train = np.load(\"../guided/guided_dataset_X.npy\")\n",
    "y_g_train = np.load(\"../guided/guided_dataset_y.npy\")\n",
    "X_g_test = np.load(\"../guided/guided_testset_X.npy\")\n",
    "\n",
    "X_f_train = np.load(\"../freemoves/freemoves_dataset_X.npy\")\n",
    "y_f_train = np.load(\"../freemoves/freemoves_dataset_y.npy\")\n",
    "X_f_test = np.load(\"../freemoves/freemoves_testset_X.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5d74e88-bad4-4a00-9cdf-b5b5b1f6cfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guided:\n",
      "X_g_train (5, 8, 230000) / y_g_train(5, 51, 230000) / X_g_test(5, 332, 8, 500)\n",
      "\n",
      "Free moves:\n",
      "X_f_train(5, 8, 270000) / y_f_train(5, 51, 270000) / X_f_test(5, 308, 8, 500)\n"
     ]
    }
   ],
   "source": [
    "print(\"Guided:\")\n",
    "print(f\"X_g_train {X_g_train.shape} / y_g_train{y_g_train.shape} / X_g_test{X_g_test.shape}\\n\")\n",
    "print(\"Free moves:\")\n",
    "print(f\"X_f_train{X_f_train.shape} / y_f_train{y_f_train.shape} / X_f_test{X_f_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a29437e-9e32-45e1-a305-0bfad39af254",
   "metadata": {},
   "source": [
    "### 1) Signal filtering\n",
    "\n",
    "TODO: data exploration to take informed decision on filter (type of noise,....) to use and on filter parametres (no magic number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4331fabb-bc67-49c5-8d18-9572b5dc3e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import butter, sosfiltfilt, firwin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bdf832f-16c7-4dbf-b0cc-43fc14aa64c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def filtrage(x):\n",
    "    nyq  = 1024 / 2\n",
    "    low  = 20  / nyq\n",
    "    high = 450 / nyq\n",
    "    \n",
    "    sos = butter(4,[low,high], btype='band', output= 'sos')\n",
    "    \n",
    "    for sess in range(x.shape[0]):\n",
    "        for elec in range(x.shape[1]):\n",
    "            # Application of the filtrage for x\n",
    "            x[sess, elec, :] = sosfiltfilt(sos, x[sess, elec, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1dfcb3-48a7-4df1-80ec-79e5fd3bd630",
   "metadata": {},
   "source": [
    "### 2) Dataset preparation\n",
    "\n",
    "At the beginning, we implemented a naive function that loops for each windows needed. \n",
    "\n",
    "This version work but:\n",
    "- Only when the step size can divides the total number of samples.  \n",
    "- Copies every window into a new array, incurring  CPU overhead and unnecessary memory usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24c97d3d-aa09-418c-bafd-3d7be401f11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap(data, overlap=0.5, size=500):\n",
    "    Data = []\n",
    "    step = int(size * (1 - overlap))\n",
    "    n = (data.shape[2] - size) // step + 1\n",
    "    fin = n * step\n",
    "    \n",
    "    for start in range(0, fin, step):\n",
    "        end = start + size\n",
    "        W = data[... , start:end]\n",
    "        Data.append(W)\n",
    "        \n",
    "    Data = np.array(Data)\n",
    "    Data = Data.transpose(1, 0, 2, 3)\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29750bdf-f261-4fd6-84b0-d4dc3bd2aa55",
   "metadata": {},
   "source": [
    "But after some research, we decided to use the sliding_window_view function from the Numpy library for several reasons:\n",
    "\n",
    "+ Fast vectorized numpy operations, compiled c-code (no python overhead, interpreter).\n",
    "\n",
    "+ sliding_window_view function returns a view, no copy.\n",
    "\n",
    "+ The function simplifies the implementation by automating window creation and indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f74676a2-d393-4e44-98ce-48190ed197a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guided windowed:\n",
      "X_g_train_wdw (5, 919, 8, 500) / y_g_train_wdw(5, 919, 51) / X_g_test(5, 332, 8, 500)\n",
      "X_f_train_wdw (5, 1079, 8, 500) / y_f_train_wdw(5, 1079, 51) / X_f_test(5, 308, 8, 500)\n"
     ]
    }
   ],
   "source": [
    "def create_overlap_windows(x, y, window_size, overlap, axis):\n",
    "\n",
    "    step = int(window_size * (1 - overlap))\n",
    "\n",
    "    # sliding_windows_view Generate all possible windows with the corresponding step, that not what we want.\n",
    "    x_w = sliding_window_view(x,window_size,axis)\n",
    "    y_w = sliding_window_view(y,window_size,axis)\n",
    "\n",
    "    # only keep windows where the step is a multiple of our step \n",
    "    x_w = x_w[:,:,::step,:]\n",
    "    y_w = y_w[:,:,::step,:]\n",
    "\n",
    "    # We transpose the axes windows and electrode/signal \n",
    "    x_w = x_w.transpose(0, 2, 1, 3)     #  (session, window, electrode, time) and not  (session, electrode, window, time) TODO??\n",
    "    y_w = y_w.transpose(0, 2, 1, 3)     # (session, window, signals, time)\n",
    "\n",
    "    # Finaly, we keep only the last hand position (targets) for y, because for this project\n",
    "    # we need to predict, for each window in x, the final hand position in the\n",
    "    # same windows in the dataset y\n",
    "    y_w = y_w[..., -1]  # (sessions, windows, targets)\n",
    "\n",
    "    return x_w, y_w\n",
    "\n",
    "\n",
    "X_g_train_wdw, y_g_train_wdw = create_overlap_windows(X_g_train, y_g_train, window_size=500, overlap=0.5, axis=2)\n",
    "X_f_train_wdw, y_f_train_wdw = create_overlap_windows(X_f_train, y_f_train, window_size=500, overlap=0.5, axis=2)\n",
    "# !! windowed data is a view --> share original data memory (modify one, modify both)\n",
    "\n",
    "print(\"Guided windowed:\")\n",
    "print(f\"X_g_train_wdw {X_g_train_wdw.shape} / y_g_train_wdw{y_g_train_wdw.shape} / X_g_test{X_g_test.shape}\")\n",
    "print(f\"X_f_train_wdw {X_f_train_wdw.shape} / y_f_train_wdw{y_f_train_wdw.shape} / X_f_test{X_f_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a5557ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_windows_tests(x, y):\n",
    "    # (maybe automate tests given windowsize and overlap and consider internal frag (shoudl be discarded)\n",
    "    \n",
    "    x_w, y_w = create_overlap_windows(x, y, window_size=500, overlap=0.5, axis=2)    \n",
    "    \n",
    "    assert np.array_equal(x_w[0, 0, 0, :10], x[0, 0, :10]) # (sess 0) first 10 of electrode 0 in window 0\n",
    "    assert np.array_equal(x_w[0, 1, 0, :10], x[0, 0, 250:260]) # (sess 0) first 10 of electrode 0 in window 1\n",
    "    assert np.array_equal(x_w[0, 1, 4, :10], x[0, 4, 250:260]) # (sess 0) first 10 of electrode 4 in window 1\n",
    "    assert np.array_equal(x_w[0, 918, 0, -10:], x[0, 0, 229990:230000]) # (sess 0) last 10 of electrode 0 in last window (918) - (perfect fit!)\n",
    "\n",
    "quick_windows_tests(X_g_train, y_g_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efa6666-9c5a-422c-8c26-79b8f6594316",
   "metadata": {},
   "source": [
    "#### 3) Cross validation strategy\n",
    "\n",
    "For this question, we have thought about various methods of cross validation. First, our data are continous because it's a signal, so preserving temporal structure is important. We can’t use a method of cross validation which randomly shuffles our windows. \n",
    "\n",
    "We also need to prevents data leaking so we can't use a methode who use the windows of one session for training AND validation because we have overlapping data in each session, two windows in the same session can share the same datas, and if these two windows are in train and validation, it will lead to data leakage and overly optimistic performance (data in the train set will also be in the validation set). \n",
    "\n",
    "So it's naturally that we have chosen the \"Leave One Group Out\" method, this method will use each session as the validation set once and the other for training. We completly prevent data leakage because each session is indepandent from the other, and we reduce the bias because each session will be used for validation.\n",
    "\n",
    "In our case, \"LOGO\" and \"GroupKFold(5)\" produce the same splits, but we choose \"LOGO\" because it's more explicit, readers will immediatly see that we use one session for validation each time while \"GroupKFold\" need to have 5 in parameter to do the same thong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d586a98-56d4-4372-a33f-01ba5ec537cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "groups(4595,)\n",
      "\n",
      "groups_f(5395,)\n",
      "\n",
      "Guided windowed flattened:\n",
      "X_g_train_wdw_flat(4595, 4000) / y_g_train_wdw_flat(4595, 51)\n",
      "Free windowed flattened:\n",
      "X_f_train_wdw_flat(5395, 4000) / y_f_train_wdw_flat(5395, 51)\n"
     ]
    }
   ],
   "source": [
    "x_shape = X_g_train_wdw.shape\n",
    "y_shape = y_g_train_wdw.shape\n",
    "\n",
    "x_f_shape = X_f_train_wdw.shape\n",
    "y_f_shape = y_f_train_wdw.shape\n",
    "\n",
    "\n",
    "groups = np.repeat(np.arange(1,x_shape[0]+1),x_shape[1] ) # 111 (919 times), 222 (919 times), ...\n",
    "print(f\"groups{groups.shape}\\n\")\n",
    "\n",
    "groups_f = np.repeat(np.arange(1,x_f_shape[0]+1),x_f_shape[1] ) \n",
    "print(f\"groups_f{groups_f.shape}\\n\")\n",
    "\n",
    "# We need to flatten the dataset x and y because the function logo (and latter \"croos_val_score\")\n",
    "# want all the data in a 2d list, we will know have  the dataset X for exemple.\n",
    "# [4595, 4000] and not [5,919,8,500], 4595 is the multiplication of 5 and 919 (3500 = 8*500), and y \n",
    "# [4595,51] and not [5,919,51].\n",
    "# Now all the windows are store in a list and the \"groups\" list above allow the function \n",
    "# logo to know at wich session each windows belong\n",
    "# The windows 3 for example (x_windows_flat[2]) belong to the sessions groups[2] = 1\n",
    "X_g_train_wdw_flat = X_g_train_wdw.reshape(x_shape[0] * x_shape[1], x_shape[2] * x_shape[3])\n",
    "y_g_train_wdw_flat = y_g_train_wdw.reshape(y_shape[0] * y_shape[1], y_shape[2])\n",
    "\n",
    "X_f_train_wdw_flat = X_f_train_wdw.reshape(x_f_shape[0] * x_f_shape[1], x_f_shape[2] * x_f_shape[3])\n",
    "y_f_train_wdw_flat = y_f_train_wdw.reshape(y_f_shape[0] * y_f_shape[1], y_f_shape[2])\n",
    "\n",
    "print(\"Guided windowed flattened:\")\n",
    "print(f\"X_g_train_wdw_flat{X_g_train_wdw_flat.shape} / y_g_train_wdw_flat{y_g_train_wdw_flat.shape}\")\n",
    "\n",
    "print(\"Free windowed flattened:\")\n",
    "print(f\"X_f_train_wdw_flat{X_f_train_wdw_flat.shape} / y_f_train_wdw_flat{y_f_train_wdw_flat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92449bff-754d-4a5a-b2ca-d88385116328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut, cross_val_score,cross_val_predict\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07e1243c-045c-436b-85b1-725a43ead35f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# np.random.seed(0)\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "lasso_model = Lasso(max_iter=10) # If the iteration is higher, it take to much time, even on collab \n",
    "'''# If you don't want the warning but prepare your afternoon for the runtime \n",
    "lasso_model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lasso',   Lasso(alpha=0.1, max_iter=20000))\n",
    "])'''\n",
    "\n",
    "\n",
    "rmse_scorer = make_scorer(\n",
    "    lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "    greater_is_better=False  # Score near 0 is better \n",
    ")\n",
    "\n",
    "def cross_validation_with_scores(X,Y,groups,model,cv,scoring):\n",
    "    # The cross_val_score function by sklearn will execute our cv and return a tab \n",
    "    neg_rmse_scores = cross_val_score(\n",
    "        model,\n",
    "        X,\n",
    "        Y,\n",
    "        groups=groups,\n",
    "        cv=cv,\n",
    "        scoring=rmse_scorer,\n",
    "        n_jobs=-1 # Use all cores \n",
    "    )\n",
    "    \n",
    "    # Conversion of negatifs scores into positifs (convention of sklearn)\n",
    "    rmse_scores = -neg_rmse_scores  \n",
    "    print(\"RMSE for each folder:\", rmse_scores)\n",
    "    print(\"RMSE mean:\", rmse_scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "366607c5-4aac-4b22-9010-2a1f919dd752",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for each folder: [17.94011704 18.68652126 20.46060078 17.30326328 19.3046778 ]\n",
      "RMSE mean: 18.739036031720634\n",
      "RMSE for each folder: [17.80702643 16.56398358 16.04999994 15.49969335 13.02059498]\n",
      "RMSE mean: 15.788259656326385\n"
     ]
    }
   ],
   "source": [
    "#Guided \n",
    "cross_validation_with_scores(X_g_train_wdw_flat,y_g_train_wdw_flat,groups,lasso_model,logo,rmse_scorer)\n",
    "\n",
    "#Free\n",
    "cross_validation_with_scores(X_f_train_wdw_flat,y_f_train_wdw_flat,groups_f,lasso_model,logo,rmse_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "384848b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session 0 target info:\n",
      "  min = -108.68231864942676\n",
      "  max = 44.76897408739836\n",
      "  mean = -5.73247691191569\n"
     ]
    }
   ],
   "source": [
    "# rmse context\n",
    "sess = 0\n",
    "y_max = np.max(y_g_train_wdw[sess])\n",
    "y_min = np.min(y_g_train_wdw[sess])\n",
    "y_mean = np.mean(y_g_train_wdw[sess])\n",
    "\n",
    "print(f\"Session {sess} target info:\\n  min = {y_min}\\n  max = {y_max}\\n  mean = {y_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecd17ef4-108a-480b-aba8-33de84479a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "   train groups: [2 3 4 5]\n",
      "   test groups: [1]\n",
      "Fold 1\n",
      "   train groups: [1 3 4 5]\n",
      "   test groups: [2]\n",
      "Fold 2\n",
      "   train groups: [1 2 4 5]\n",
      "   test groups: [3]\n",
      "Fold 3\n",
      "   train groups: [1 2 3 5]\n",
      "   test groups: [4]\n",
      "Fold 4\n",
      "   train groups: [1 2 3 4]\n",
      "   test groups: [5]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, (train_index, test_index) in enumerate(logo.split(X_g_train_wdw_flat, y_g_train_wdw_flat, groups)):\n",
    "    print(f\"Fold {i}\")\n",
    "    print(f\"   train groups: {np.unique(groups[train_index])}\")\n",
    "    print(f\"   test groups: {np.unique(groups[test_index])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb3b954-0157-4eae-a02d-355a20329d9a",
   "metadata": {},
   "source": [
    "#### 5) More sophisticated approach\n",
    "\n",
    "For this question, we decided to implement the two approaches in order to have a better understanding and more methods to compare.\n",
    "\n",
    "We started with the covariance approch, following the steps in section 3.2:\n",
    "\n",
    "- We first calculate the covaraince of each windows with the PyRiemmann Covariances class, which expects a 3d array, so we need to reshape it into the form (windows,electrode,time). After using this class, we obtain an array of 8×8 covariance matrices (SPD_tab) for each window.\n",
    "\n",
    "- Next, we map each SPD matrix into their tangent space using the TangentSpace class. This projection transforms our SPD matrices into Euclidean vectors. Thanks to this, our dataset becomes 2D again, and we can directly use a traditional regression algorithms and sklearn function.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ffd5b0f-b9dc-406e-a930-9ec1ba230d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pyriemann\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from sklearn.pipeline     import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4668e27-97cc-4c58-b39f-1d67dcbb7ce4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Covariances method\n",
    "\n",
    "\n",
    "# Reshape the flattten dataset\n",
    "X_g_train_reshape = X_g_train_wdw_flat.reshape(4595,8,500)\n",
    "\n",
    "X_f_train_reshape = X_f_train_wdw_flat.reshape(5395,8,500)\n",
    "\n",
    "'''\n",
    "covariance = Covariances(estimator='oas')\n",
    "SPD_tab = covariance.fit_transform(X_g_train_reshape) \n",
    "# print(SPD_tab.shape) # (4595,8,8)\n",
    "ts = TangentSpace()\n",
    "tangent_tab = ts.fit_transform(SPD_tab)\n",
    "# print(tangent_tab.shape) # (4595,36) Now that we have a 2d tab, we can use traditional regression algorithms\n",
    "'''\n",
    "\n",
    "pipe_cov = Pipeline([\n",
    "    ('cov',    Covariances(estimator='oas')),   # Covariances matrices of each windows\n",
    "    ('ts',     TangentSpace()),                 # This projection will transform our SPD matrices into euclidean vector\n",
    "    ('scale', StandardScaler()),          # Standardize each feature (mean =0, std =1) \n",
    "    ('lasso',  lasso_model)               # Lasso model \n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96428ea2-8afb-40d0-a5d8-5d0a9b1db6d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.225e+01, tolerance: 4.499e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.743e+03, tolerance: 4.204e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.416e+03, tolerance: 5.580e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.093e+04, tolerance: 1.938e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.449e+03, tolerance: 1.159e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.410e+05, tolerance: 4.810e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.876e+04, tolerance: 2.176e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.683e+04, tolerance: 1.336e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.070e+05, tolerance: 5.174e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.469e+04, tolerance: 1.623e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.735e+04, tolerance: 1.958e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.705e+05, tolerance: 6.268e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.288e+04, tolerance: 1.587e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.758e+01, tolerance: 6.573e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.925e+03, tolerance: 2.453e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.118e+04, tolerance: 4.521e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.136e+03, tolerance: 1.364e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.548e+01, tolerance: 4.776e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.908e+03, tolerance: 4.466e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.157e+03, tolerance: 5.686e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.899e+04, tolerance: 1.997e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.580e+03, tolerance: 1.193e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.229e+05, tolerance: 4.854e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.533e+04, tolerance: 2.284e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.970e+04, tolerance: 1.378e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.744e+04, tolerance: 5.241e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.014e+04, tolerance: 1.708e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.434e+04, tolerance: 1.965e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.462e+05, tolerance: 6.333e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.740e+04, tolerance: 1.647e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.337e+01, tolerance: 6.703e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.137e+03, tolerance: 2.427e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.902e+04, tolerance: 4.602e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.032e+03, tolerance: 1.402e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.592e+01, tolerance: 4.492e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.463e+03, tolerance: 4.189e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.281e+03, tolerance: 5.803e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.986e+04, tolerance: 1.931e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.134e+03, tolerance: 1.121e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.104e+04, tolerance: 4.759e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.365e+04, tolerance: 2.145e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.550e+04, tolerance: 1.293e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.049e+04, tolerance: 5.200e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.032e+04, tolerance: 1.644e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.695e+04, tolerance: 1.921e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.037e+05, tolerance: 6.286e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.190e+04, tolerance: 1.582e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.541e+03, tolerance: 2.356e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.740e+04, tolerance: 4.605e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.205e+03, tolerance: 1.377e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.556e+03, tolerance: 4.037e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.106e+03, tolerance: 5.886e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.382e+04, tolerance: 1.966e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.159e+03, tolerance: 1.141e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.511e+05, tolerance: 4.853e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.367e+04, tolerance: 2.249e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.862e+04, tolerance: 1.347e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.128e+05, tolerance: 5.276e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.328e+04, tolerance: 1.661e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.199e+04, tolerance: 1.987e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.314e+05, tolerance: 6.315e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.485e+04, tolerance: 1.616e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.622e+01, tolerance: 4.404e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.353e+01, tolerance: 6.584e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.134e+04, tolerance: 2.563e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.066e+03, tolerance: 4.038e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.517e+04, tolerance: 4.627e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.392e+03, tolerance: 5.997e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.930e+03, tolerance: 1.367e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.082e+04, tolerance: 1.874e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.309e+03, tolerance: 1.188e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.326e+05, tolerance: 4.898e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.703e+04, tolerance: 2.142e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.652e+04, tolerance: 1.390e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.699e+04, tolerance: 5.309e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+04, tolerance: 1.602e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.151e+04, tolerance: 1.980e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.317e+05, tolerance: 6.463e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.229e+03, tolerance: 1.591e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.158e+00, tolerance: 6.764e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.985e+03, tolerance: 2.565e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.796e+04, tolerance: 4.615e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.676e+03, tolerance: 1.351e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for each folder: [8.21531223 7.74146158 7.93251768 6.92650706 7.90240062]\n",
      "RMSE mean: 7.743639831450407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.377e+02, tolerance: 6.182e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.075e+03, tolerance: 9.952e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.569e+04, tolerance: 3.994e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.704e+04, tolerance: 2.363e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.205e+04, tolerance: 1.507e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.653e+04, tolerance: 4.424e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.267e+04, tolerance: 2.648e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.430e+04, tolerance: 2.026e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.364e+04, tolerance: 4.156e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.440e+04, tolerance: 2.303e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.079e+04, tolerance: 2.614e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.776e+02, tolerance: 6.018e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.776e+04, tolerance: 3.917e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.108e+04, tolerance: 2.075e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.274e+03, tolerance: 1.039e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.577e+05, tolerance: 4.083e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.724e+04, tolerance: 2.386e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.745e+04, tolerance: 1.474e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.649e+05, tolerance: 4.471e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.672e+04, tolerance: 2.600e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.445e+04, tolerance: 1.965e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.485e+05, tolerance: 4.330e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.049e+04, tolerance: 2.228e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.188e+04, tolerance: 2.546e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.613e+04, tolerance: 3.964e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+04, tolerance: 2.071e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.529e+01, tolerance: 6.162e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.166e+02, tolerance: 4.879e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.554e+02, tolerance: 6.418e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.781e+03, tolerance: 9.192e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.959e+01, tolerance: 4.744e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.319e+04, tolerance: 3.677e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.440e+04, tolerance: 2.073e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.254e+04, tolerance: 1.035e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.373e+04, tolerance: 1.369e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.732e+04, tolerance: 3.817e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.221e+05, tolerance: 4.212e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.169e+04, tolerance: 2.244e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.706e+04, tolerance: 1.391e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.891e+04, tolerance: 2.490e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.441e+02, tolerance: 5.063e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.269e+05, tolerance: 4.474e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.879e+04, tolerance: 1.816e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.560e+04, tolerance: 2.626e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.226e+05, tolerance: 4.100e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.505e+04, tolerance: 1.946e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.904e+04, tolerance: 2.137e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.347e+05, tolerance: 4.389e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.645e+03, tolerance: 9.219e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.073e+04, tolerance: 2.226e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.251e+04, tolerance: 2.771e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.785e+04, tolerance: 2.296e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.517e+04, tolerance: 2.648e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.742e+02, tolerance: 1.486e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.347e+04, tolerance: 3.799e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.396e+05, tolerance: 4.013e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.640e+03, tolerance: 1.263e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.928e+04, tolerance: 1.959e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.576e+04, tolerance: 2.052e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.251e+04, tolerance: 3.736e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.413e+03, tolerance: 2.084e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.348e+04, tolerance: 1.712e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.038e+04, tolerance: 3.785e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.084e+03, tolerance: 1.842e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.606e+04, tolerance: 2.252e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.067e+04, tolerance: 3.349e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.482e+04, tolerance: 1.702e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for each folder: [10.60965401 12.45418765 11.22372944  9.79798601  9.58054703]\n",
      "RMSE mean: 10.733220829146067\n"
     ]
    }
   ],
   "source": [
    "# Now we juste need to use the cv function we build above \n",
    "#Guided \n",
    "cross_validation_with_scores(X_g_train_reshape,y_g_train_wdw_flat,groups,pipe_cov,logo,rmse_scorer)\n",
    "\n",
    "#Free \n",
    "cross_validation_with_scores(X_f_train_reshape,y_f_train_wdw_flat,groups_f,pipe_cov,logo,rmse_scorer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744f83a5-45be-463c-93c1-254dec71ef8e",
   "metadata": {},
   "source": [
    "For the neural network approch, we have done:\n",
    "\n",
    "##### Simple model\n",
    "We started by a simple model composed of 3 linear layers.\n",
    "\n",
    "To evaluate it, we used Skorch, which lets us plug a PyTorch nn.Module into a sklearn function.\n",
    "Thanks to Skorch, we could reuse our existing cross_validation_with_scores() function.\n",
    "\n",
    "The rmse mean was 17.03, which beat the vanilla lasso model but not the covriance matrices lasso. We analyzed the value of the training loss and validation loss for each epoch and we saw that the model is underfitting.\n",
    "So we decide to complexify it.\n",
    "\n",
    "##### Model complexity\n",
    "\n",
    "We upgraded to a small CNN with three 1D convolutional layers because the dataset has to much datas for an nn classique and to automatically learn local temporal patterns in the EMG signal. \n",
    "\n",
    "After each convolutional layer:\n",
    "\n",
    "We normalize our data to stabilize and speed up training.\n",
    "\n",
    "We introduce a simple Relu activation so the network can learn more complex features.\n",
    "\n",
    "We “cut” the time dimension in half, keeping only the strongest responses and reducing data size.\n",
    "\n",
    "Once the three convolutional blocks are done, we flatten the output tensor and pass it into a two layer head, to convert these extracted features into the 51 joint-angle predictions\n",
    "\n",
    "##### Early stopping and LR scheduling\n",
    "\n",
    "We added two Skorch callbacks:\n",
    "\n",
    "-EarlyStopping to stop the training when no improvement is seen for 10 epochs.\n",
    "\n",
    "-LRScheduler to cut the learning rate by half when the validation loss stalls 3 epochs consecutives.\n",
    "\n",
    "This combination prevents wasted epochs once the model converges and refines the learning rate to squeeze out extra gains.\n",
    "\n",
    "##### Batch size reduction\n",
    "\n",
    "We lowered the batch size from 128 to 64. Using smaller batches adds a bit of randomness to each weight update, which helps the model generalize better without altering its structure.\n",
    "\n",
    "With these three changes, the nn average RMSE dropped to ~5.07, a dramatic improvement over the initial ~17."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8074777f-d8a4-4172-b732-6a425d3f0fbc",
   "metadata": {},
   "source": [
    "| Simple Model | Complex Model | Complex Model + Early Stopping & LR Scheduler | Same as #3 but Batch Size = 64 |\n",
    "|:------------:|:-------------:|:---------------------------------------------:|:-----------------------------:|\n",
    "| ![](./images/1.png) | ![](./images/2.png) | ![](./images/3.png) | ![](./images/4.png) |\n",
    "| **RMSE mean:** 17.03 | **RMSE mean:** 6.06 | **RMSE mean:** 5.89 | **RMSE mean:** 4.87 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8a70a53-6562-4aa8-b197-af063ca6a13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U skorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skorch import NeuralNetRegressor\n",
    "from skorch.callbacks import EarlyStopping\n",
    "from skorch.callbacks import LRScheduler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42cd60a5-d6f7-4e74-91f0-f26c7c8b7ec8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "## NN method\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.f = nn.Sequential(\n",
    "            nn.Unflatten(1, (8,500)), #unflatten data for  convolution (64 (batch_size), 8,500)\n",
    "\n",
    "            nn.Conv1d(8, 32, kernel_size=11, padding=5), # 8 input channels (electrodes) and 32 is the output, the number of feature he learn.\n",
    "            # Thanks to padding, the output length remains 500 (64,32,500) \n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),  # Halve the temporal dimension  (64,32,250)          \n",
    "            \n",
    "            nn.Conv1d(32, 64, kernel_size=9, padding=4), #(64,64,250)\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),  #(64,64,125)      \n",
    "\n",
    "            nn.Conv1d(64, 128, kernel_size=7, padding=3), # (64,128,125)\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),  # (64,128,62)    \n",
    "            \n",
    "            nn.Flatten(), # Reflatten our data for the next part (64,128*62)\n",
    "        )\n",
    "        self.r = nn.Sequential(\n",
    "            nn.Linear(7936, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 51),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.f(x)\n",
    "        return self.r(x)\n",
    "\n",
    "      \n",
    "# Convert dataset from float64 to float32 because PyTorch layers expect float32\n",
    "# 32-bit precision is sufficient for our signals and speeds up training\n",
    "print(X_g_train_wdw_flat.dtype)   \n",
    "print(y_g_train_wdw_flat.dtype)   \n",
    "\n",
    "x = X_g_train_wdw_flat.astype('float32')\n",
    "y = y_g_train_wdw_flat.astype('float32')\n",
    "\n",
    "x_f = X_f_train_wdw_flat.astype('float32')\n",
    "y_f = y_f_train_wdw_flat.astype('float32')\n",
    "\n",
    "\n",
    "net = NeuralNetRegressor(\n",
    "    module=NeuralNetwork,                 # PyTorch model \n",
    "    max_epochs=100,                 \n",
    "    lr=1e-3,                       \n",
    "    batch_size=64,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    callbacks=[('earlystop', EarlyStopping('valid_loss', patience=10)), # Stop if validation loss doesn't improve for 5 epochs \n",
    "                    ('lr_sched', LRScheduler(\n",
    "           policy=torch.optim.lr_scheduler.ReduceLROnPlateau, # Halve LR if validation loss stalls for 3 epochs\n",
    "           monitor='valid_loss',\n",
    "           patience=3, factor=0.5))]\n",
    ")\n",
    "\n",
    "\n",
    "pipe_cnn = Pipeline([\n",
    "    ('scale', StandardScaler()),  # Standardize inputs for a quick start, after the cnn will then normalize its data at each step\n",
    "    ('net',   net)                \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3771a06a-a336-45a1-af7d-146c519b0c45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m183.7466\u001b[0m      \u001b[32m117.0032\u001b[0m  0.0010  12.0949\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m189.3073\u001b[0m      \u001b[32m126.2191\u001b[0m  0.0010  12.0673\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m180.7975\u001b[0m      \u001b[32m117.4314\u001b[0m  0.0010  12.1996\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m197.3731\u001b[0m      \u001b[32m121.0930\u001b[0m  0.0010  12.2818\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m188.5981\u001b[0m      \u001b[32m127.7126\u001b[0m  0.0010  12.3568\n",
      "      2       \u001b[36m87.7310\u001b[0m      \u001b[32m109.3404\u001b[0m  0.0010  14.1686\n",
      "      2       \u001b[36m83.0573\u001b[0m       \u001b[32m79.4835\u001b[0m  0.0010  14.5236\n",
      "      2       \u001b[36m86.1038\u001b[0m      \u001b[32m101.4511\u001b[0m  0.0010  14.5651\n",
      "      2       \u001b[36m92.7528\u001b[0m       \u001b[32m80.5137\u001b[0m  0.0010  14.4968\n",
      "      2       \u001b[36m88.0846\u001b[0m       \u001b[32m70.8250\u001b[0m  0.0010  14.6335\n",
      "      3       \u001b[36m71.7454\u001b[0m       \u001b[32m82.0072\u001b[0m  0.0010  14.1871\n",
      "      3       \u001b[36m71.3979\u001b[0m       \u001b[32m70.8756\u001b[0m  0.0010  14.4481\n",
      "      3       \u001b[36m73.8106\u001b[0m      104.9197  0.0010  14.5051\n",
      "      3       \u001b[36m73.3394\u001b[0m       \u001b[32m65.9932\u001b[0m  0.0010  14.5265\n",
      "      3       \u001b[36m74.4162\u001b[0m       \u001b[32m63.6646\u001b[0m  0.0010  14.6656\n",
      "      4       \u001b[36m65.3786\u001b[0m      109.8720  0.0010  13.6359\n",
      "      4       \u001b[36m66.1868\u001b[0m       \u001b[32m67.8483\u001b[0m  0.0010  13.9701\n",
      "      4       \u001b[36m67.9883\u001b[0m       \u001b[32m92.9873\u001b[0m  0.0010  13.8523\n",
      "      4       \u001b[36m65.2174\u001b[0m       70.3397  0.0010  14.1126\n",
      "      4       \u001b[36m66.6974\u001b[0m       \u001b[32m61.5003\u001b[0m  0.0010  13.8529\n",
      "      5       \u001b[36m57.1699\u001b[0m      121.2900  0.0010  13.9379\n",
      "      5       \u001b[36m61.9896\u001b[0m       \u001b[32m67.5517\u001b[0m  0.0010  14.2278\n",
      "      5       \u001b[36m63.3696\u001b[0m      112.2947  0.0010  14.1869\n",
      "      5       \u001b[36m57.4883\u001b[0m       \u001b[32m62.4535\u001b[0m  0.0010  14.0096\n",
      "      5       \u001b[36m61.2147\u001b[0m       66.8975  0.0010  14.4048\n",
      "      6       \u001b[36m49.4435\u001b[0m      110.6339  0.0010  14.3343\n",
      "      6       \u001b[36m48.9398\u001b[0m       71.6584  0.0010  14.6942\n",
      "      6       \u001b[36m59.4840\u001b[0m      157.9479  0.0010  14.9415\n",
      "      6       \u001b[36m57.9302\u001b[0m       74.2148  0.0010  15.0156\n",
      "      6       \u001b[36m55.6606\u001b[0m       68.8032  0.0010  14.7876\n",
      "      7       \u001b[36m44.3109\u001b[0m       85.4393  0.0010  14.6077\n",
      "      7       \u001b[36m52.2402\u001b[0m       68.8686  0.0010  15.0368\n",
      "      7       \u001b[36m54.5199\u001b[0m      105.4687  0.0010  15.1324\n",
      "      7       \u001b[36m44.4612\u001b[0m       99.0961  0.0010  15.3240\n",
      "      7       \u001b[36m49.1960\u001b[0m       90.8365  0.0010  15.3686\n",
      "      8       \u001b[36m39.3213\u001b[0m       \u001b[32m44.4001\u001b[0m  0.0005  15.9957\n",
      "      8       \u001b[36m48.1076\u001b[0m       \u001b[32m57.3495\u001b[0m  0.0010  16.1063\n",
      "      8       \u001b[36m40.2043\u001b[0m      103.9971  0.0010  16.1512\n",
      "      8       \u001b[36m47.1071\u001b[0m       \u001b[32m51.6719\u001b[0m  0.0010  16.2713\n",
      "      8       \u001b[36m44.7533\u001b[0m      115.2530  0.0010  16.5479\n",
      "      9       \u001b[36m35.5247\u001b[0m       50.5773  0.0005  15.6664\n",
      "      9       \u001b[36m44.8019\u001b[0m       \u001b[32m49.9908\u001b[0m  0.0010  15.6464\n",
      "      9       \u001b[36m33.9791\u001b[0m       75.1117  0.0010  15.5985\n",
      "      9       \u001b[36m40.4772\u001b[0m       67.6510  0.0010  15.6340\n",
      "      9       \u001b[36m42.6010\u001b[0m       \u001b[32m55.0832\u001b[0m  0.0005  15.5347\n",
      "     10       \u001b[36m31.1481\u001b[0m       50.5102  0.0005  14.4477\n",
      "     10       \u001b[36m39.3330\u001b[0m       57.4269  0.0010  14.6053\n",
      "     10       \u001b[36m27.6671\u001b[0m       \u001b[32m62.0904\u001b[0m  0.0005  14.6465\n",
      "     10       \u001b[36m33.3133\u001b[0m       68.6059  0.0010  14.8592\n",
      "     10       \u001b[36m37.3235\u001b[0m       \u001b[32m42.6188\u001b[0m  0.0005  14.6990\n",
      "     11       \u001b[36m26.4877\u001b[0m       44.7340  0.0005  14.0472\n",
      "     11       \u001b[36m32.6888\u001b[0m       53.7821  0.0010  14.2988\n",
      "     11       \u001b[36m23.5136\u001b[0m       \u001b[32m46.3112\u001b[0m  0.0005  14.3089\n",
      "     11       \u001b[36m24.3804\u001b[0m       70.2326  0.0010  14.2412\n",
      "     11       \u001b[36m32.5826\u001b[0m       \u001b[32m40.0812\u001b[0m  0.0005  14.6744\n",
      "     12       \u001b[36m21.0480\u001b[0m       \u001b[32m36.5697\u001b[0m  0.0005  15.2448\n",
      "     12       \u001b[36m21.7970\u001b[0m       \u001b[32m34.1127\u001b[0m  0.0010  15.7525\n",
      "     12       \u001b[36m17.2035\u001b[0m       56.0573  0.0010  15.4997\n",
      "     12       \u001b[36m19.9292\u001b[0m       46.5264  0.0005  15.8770\n",
      "     12       \u001b[36m28.7695\u001b[0m       \u001b[32m38.7565\u001b[0m  0.0005  15.7497\n",
      "     13       \u001b[36m16.7622\u001b[0m       \u001b[32m32.9763\u001b[0m  0.0005  14.6307\n",
      "     13       \u001b[36m15.7971\u001b[0m       48.2693  0.0010  14.5587\n",
      "     13       \u001b[36m13.9830\u001b[0m       \u001b[32m41.9673\u001b[0m  0.0005  14.6748\n",
      "     13       \u001b[36m16.7260\u001b[0m       47.9487  0.0005  14.7042\n",
      "     13       \u001b[36m23.9879\u001b[0m       \u001b[32m38.1265\u001b[0m  0.0005  14.6256\n",
      "     14       \u001b[36m15.0545\u001b[0m       \u001b[32m26.1093\u001b[0m  0.0005  14.3299\n",
      "     14       \u001b[36m13.3090\u001b[0m       36.3172  0.0010  14.2799\n",
      "     14       \u001b[36m14.4306\u001b[0m       49.8201  0.0005  14.1752\n",
      "     14       14.0210       \u001b[32m41.7501\u001b[0m  0.0005  14.4258\n",
      "     14       \u001b[36m17.6418\u001b[0m       \u001b[32m35.7833\u001b[0m  0.0005  14.5430\n",
      "     15       \u001b[36m14.0350\u001b[0m       \u001b[32m23.4821\u001b[0m  0.0005  14.7648\n",
      "     15       \u001b[36m12.1817\u001b[0m       37.6700  0.0010  15.3711\n",
      "     15       \u001b[36m12.9157\u001b[0m       52.3708  0.0005  15.3298\n",
      "     15       \u001b[36m12.5397\u001b[0m       \u001b[32m41.4435\u001b[0m  0.0005  15.4610\n",
      "     15       \u001b[36m13.9971\u001b[0m       \u001b[32m32.3430\u001b[0m  0.0005  15.4558\n",
      "     16       \u001b[36m12.7113\u001b[0m       \u001b[32m23.4646\u001b[0m  0.0005  16.5370\n",
      "     16       \u001b[36m12.1313\u001b[0m       44.4728  0.0010  16.1694\n",
      "     16       \u001b[36m11.7758\u001b[0m       \u001b[32m39.1254\u001b[0m  0.0005  16.3998\n",
      "     16       12.9642       \u001b[32m28.0579\u001b[0m  0.0003  16.7071\n",
      "     16       \u001b[36m12.4968\u001b[0m       \u001b[32m30.2941\u001b[0m  0.0005  16.4748\n",
      "     17       \u001b[36m11.3795\u001b[0m       25.9693  0.0005  16.4189\n",
      "     17       14.5502       45.2093  0.0005  16.3712\n",
      "     17       \u001b[36m11.1880\u001b[0m       \u001b[32m34.6563\u001b[0m  0.0005  16.2729\n",
      "     17       \u001b[36m12.9074\u001b[0m       31.7588  0.0003  16.5078\n",
      "     17       \u001b[36m11.7994\u001b[0m       30.7997  0.0005  16.2580\n",
      "     18       \u001b[36m10.4949\u001b[0m       30.7538  0.0005  16.5941\n",
      "     18       14.9241       35.9784  0.0005  16.4648\n",
      "     18       \u001b[36m10.6175\u001b[0m       \u001b[32m30.9170\u001b[0m  0.0005  16.5413\n",
      "     18       \u001b[36m11.3824\u001b[0m       31.9787  0.0003  16.6717\n",
      "     18       \u001b[36m11.5623\u001b[0m       \u001b[32m29.8159\u001b[0m  0.0005  16.7126\n",
      "     19        \u001b[36m9.9474\u001b[0m       35.5863  0.0005  16.8174\n",
      "     19       12.4474       \u001b[32m27.7684\u001b[0m  0.0005  16.8604\n",
      "     19       \u001b[36m10.1054\u001b[0m       \u001b[32m28.4498\u001b[0m  0.0005  17.1102\n",
      "     19       \u001b[36m10.4908\u001b[0m       31.1242  0.0003  17.2091\n",
      "     19       12.0168       \u001b[32m26.2790\u001b[0m  0.0005  17.1805\n",
      "     20        \u001b[36m9.4837\u001b[0m       42.8768  0.0005  16.2360\n",
      "     20       \u001b[36m10.3046\u001b[0m       \u001b[32m24.4845\u001b[0m  0.0005  16.5125\n",
      "     20        \u001b[36m9.6377\u001b[0m       \u001b[32m27.2457\u001b[0m  0.0005  16.3904\n",
      "     20       \u001b[36m10.0683\u001b[0m       31.5514  0.0003  16.4078\n",
      "     20       12.9143       \u001b[32m23.9491\u001b[0m  0.0005  16.3858\n",
      "     21       10.3092       47.7319  0.0003  16.1979\n",
      "     21        \u001b[36m8.9405\u001b[0m       \u001b[32m23.0144\u001b[0m  0.0005  16.4071\n",
      "     21        \u001b[36m9.1261\u001b[0m       27.3972  0.0005  16.2243\n",
      "     21       10.5835       34.8217  0.0001  16.5969\n",
      "     21       13.0173       32.9741  0.0005  16.5172\n",
      "     22       10.3175       40.6191  0.0003  16.3815\n",
      "     22        \u001b[36m8.2134\u001b[0m       \u001b[32m22.3835\u001b[0m  0.0005  17.0183\n",
      "     22        \u001b[36m8.5912\u001b[0m       27.3444  0.0005  16.8777\n",
      "     22       10.5427       36.3116  0.0001  17.0553\n",
      "     22       12.9799       40.4781  0.0005  17.0027\n",
      "     23        9.8421       31.5344  0.0003  17.5825\n",
      "     23        \u001b[36m7.7842\u001b[0m       \u001b[32m21.9219\u001b[0m  0.0005  18.0699\n",
      "     23        \u001b[36m8.2544\u001b[0m       27.2781  0.0005  17.8023\n",
      "     23        \u001b[36m9.8678\u001b[0m       37.0696  0.0001  18.1005\n",
      "     23       13.4291       71.6399  0.0005  17.6475\n",
      "     24        9.5244       26.9491  0.0003  16.5383\n",
      "     24        \u001b[36m7.4871\u001b[0m       \u001b[32m21.6995\u001b[0m  0.0005  16.3869\n",
      "     24        \u001b[36m8.0489\u001b[0m       27.5470  0.0005  16.2506\n",
      "     24        \u001b[36m9.4152\u001b[0m       36.8696  0.0001  16.8776\n",
      "     24       13.5813       65.7949  0.0005  16.6669\n",
      "     25       10.3718       24.7460  0.0001  16.2626\n",
      "     25        8.7005       37.4583  0.0003  16.1773\n",
      "     25        \u001b[36m7.2708\u001b[0m       \u001b[32m21.4301\u001b[0m  0.0005  16.3370\n",
      "     25        \u001b[36m8.6015\u001b[0m       30.4528  0.0001  15.9244\n",
      "     25       \u001b[36m10.5195\u001b[0m       33.2960  0.0003  16.1115\n",
      "     26        \u001b[36m7.1014\u001b[0m       \u001b[32m21.2599\u001b[0m  0.0005  15.1655\n",
      "     26        9.1942       34.2053  0.0003  15.4370\n",
      "     26        \u001b[36m9.2970\u001b[0m       32.3858  0.0003  15.0979\n",
      "     27        \u001b[36m6.9604\u001b[0m       \u001b[32m21.0745\u001b[0m  0.0005  12.0926\n",
      "     27        8.8059       36.2714  0.0003  11.9173\n",
      "     27        \u001b[36m8.9488\u001b[0m       30.8550  0.0003  11.6687\n",
      "     28        8.5696       36.9689  0.0003  11.2831\n",
      "     28        \u001b[36m6.8514\u001b[0m       \u001b[32m20.9646\u001b[0m  0.0005  11.5479\n",
      "     28        \u001b[36m8.6654\u001b[0m       29.2258  0.0003  11.2927\n",
      "     29        8.6220       34.5831  0.0001  11.9292\n",
      "     29        \u001b[36m6.7637\u001b[0m       \u001b[32m20.7902\u001b[0m  0.0005  11.9827\n",
      "     29        \u001b[36m8.4623\u001b[0m       \u001b[32m23.1965\u001b[0m  0.0001  12.2200\n",
      "     30        \u001b[36m6.6904\u001b[0m       \u001b[32m20.6686\u001b[0m  0.0005  13.5965\n",
      "     30        \u001b[36m8.3133\u001b[0m       \u001b[32m22.6891\u001b[0m  0.0001  13.2260\n",
      "     31        \u001b[36m6.6310\u001b[0m       20.6694  0.0005  9.8761\n",
      "     31        \u001b[36m8.0974\u001b[0m       \u001b[32m22.4580\u001b[0m  0.0001  9.7120\n",
      "     32        \u001b[36m6.5581\u001b[0m       20.7353  0.0005  10.0860\n",
      "     32        \u001b[36m7.9089\u001b[0m       \u001b[32m22.3074\u001b[0m  0.0001  10.2957\n",
      "     33        \u001b[36m6.5405\u001b[0m       20.8101  0.0005  10.3465\n",
      "     33        \u001b[36m7.7534\u001b[0m       \u001b[32m22.1970\u001b[0m  0.0001  10.2079\n",
      "     34        \u001b[36m6.5155\u001b[0m       21.0443  0.0005  9.8030\n",
      "     34        \u001b[36m7.6215\u001b[0m       \u001b[32m22.1002\u001b[0m  0.0001  9.9232\n",
      "     35        7.0904       25.2996  0.0003  10.2499\n",
      "     35        \u001b[36m7.5074\u001b[0m       22.1028  0.0001  10.2432\n",
      "     36        7.3523       24.4434  0.0003  9.7671\n",
      "     36        \u001b[36m7.4084\u001b[0m       \u001b[32m22.0664\u001b[0m  0.0001  9.7224\n",
      "     37        6.8777       24.6409  0.0003  9.6032\n",
      "     37        \u001b[36m7.3202\u001b[0m       \u001b[32m22.0404\u001b[0m  0.0001  9.6784\n",
      "     38        6.5724       24.3151  0.0003  9.7047\n",
      "     38        \u001b[36m7.2417\u001b[0m       \u001b[32m22.0247\u001b[0m  0.0001  9.6217\n",
      "     39        \u001b[36m6.2294\u001b[0m       22.3135  0.0001  9.4282\n",
      "     39        \u001b[36m7.1695\u001b[0m       \u001b[32m22.0160\u001b[0m  0.0001  9.5417\n",
      "     40        \u001b[36m7.1056\u001b[0m       22.0190  0.0001  9.3791\n",
      "     41        \u001b[36m7.0472\u001b[0m       22.0705  0.0001  7.8706\n",
      "     42        \u001b[36m6.9916\u001b[0m       22.1654  0.0001  7.8522\n",
      "     43        \u001b[36m6.9413\u001b[0m       22.2100  0.0001  7.6442\n",
      "     44        \u001b[36m6.8377\u001b[0m       22.5861  0.0001  7.7697\n",
      "     45        \u001b[36m6.7624\u001b[0m       22.9948  0.0001  7.6128\n",
      "     46        \u001b[36m6.7209\u001b[0m       22.9864  0.0001  7.8058\n",
      "     47        \u001b[36m6.6829\u001b[0m       23.0200  0.0001  7.8871\n",
      "     48        \u001b[36m6.5561\u001b[0m       23.4852  0.0000  7.8046\n",
      "RMSE for each folder: [6.15534556 4.68717426 4.93688072 4.41636697 3.63331335]\n",
      "RMSE mean: 4.765816174271922\n"
     ]
    }
   ],
   "source": [
    "#Guided\n",
    "cross_validation_with_scores(x,y,groups,pipe_cnn,logo,rmse_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "080fae37-8d5c-420c-ba16-00a2e9acaf4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m219.1000\u001b[0m      \u001b[32m293.1583\u001b[0m  0.0010  13.9384\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m224.9589\u001b[0m      \u001b[32m242.9936\u001b[0m  0.0010  14.0721\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m222.1733\u001b[0m      \u001b[32m205.4595\u001b[0m  0.0010  14.1591\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m231.9693\u001b[0m      \u001b[32m148.7291\u001b[0m  0.0010  14.1602\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m199.9677\u001b[0m      \u001b[32m221.3613\u001b[0m  0.0010  14.1809\n",
      "      2      \u001b[36m177.8046\u001b[0m      \u001b[32m210.1426\u001b[0m  0.0010  14.1591\n",
      "      2      \u001b[36m162.1591\u001b[0m      \u001b[32m184.0694\u001b[0m  0.0010  14.1777\n",
      "      2      \u001b[36m156.1242\u001b[0m      \u001b[32m254.2190\u001b[0m  0.0010  14.4730\n",
      "      2      \u001b[36m150.9769\u001b[0m      \u001b[32m141.5371\u001b[0m  0.0010  14.2469\n",
      "      2      \u001b[36m166.1331\u001b[0m      \u001b[32m144.2413\u001b[0m  0.0010  14.3413\n",
      "      3      \u001b[36m176.2048\u001b[0m      \u001b[32m197.3821\u001b[0m  0.0010  14.7587\n",
      "      3      \u001b[36m156.3163\u001b[0m      189.9777  0.0010  14.8073\n",
      "      3      \u001b[36m141.6545\u001b[0m      \u001b[32m135.2890\u001b[0m  0.0010  14.8922\n",
      "      3      \u001b[36m146.0714\u001b[0m      257.4220  0.0010  14.9178\n",
      "      3      \u001b[36m162.7391\u001b[0m      \u001b[32m137.0963\u001b[0m  0.0010  14.9614\n",
      "      4      \u001b[36m173.3958\u001b[0m      \u001b[32m193.4380\u001b[0m  0.0010  14.6346\n",
      "      4      \u001b[36m150.2952\u001b[0m      193.9317  0.0010  14.5813\n",
      "      4      \u001b[36m136.6154\u001b[0m      \u001b[32m134.3099\u001b[0m  0.0010  14.6420\n",
      "      4      \u001b[36m141.1076\u001b[0m      259.0159  0.0010  14.7415\n",
      "      4      \u001b[36m159.7538\u001b[0m      \u001b[32m130.4021\u001b[0m  0.0010  14.7152\n",
      "      5      \u001b[36m166.4826\u001b[0m      194.5589  0.0010  14.2701\n",
      "      5      \u001b[36m145.6193\u001b[0m      \u001b[32m183.0582\u001b[0m  0.0010  14.6145\n",
      "      5      \u001b[36m132.1799\u001b[0m      138.3429  0.0010  14.7039\n",
      "      5      \u001b[36m136.1898\u001b[0m      263.2105  0.0010  14.6232\n",
      "      5      \u001b[36m154.4065\u001b[0m      \u001b[32m126.1199\u001b[0m  0.0010  14.5586\n",
      "      6      \u001b[36m161.0219\u001b[0m      \u001b[32m182.2900\u001b[0m  0.0010  14.1554\n",
      "      6      \u001b[36m139.8228\u001b[0m      \u001b[32m181.8283\u001b[0m  0.0010  14.2213\n",
      "      6      \u001b[36m125.7068\u001b[0m      148.9457  0.0010  14.2847\n",
      "      6      \u001b[36m130.4065\u001b[0m      265.1301  0.0010  14.3160\n",
      "      6      \u001b[36m147.2465\u001b[0m      129.7780  0.0010  14.5071\n",
      "      7      \u001b[36m154.9386\u001b[0m      \u001b[32m170.1131\u001b[0m  0.0010  15.1274\n",
      "      7      \u001b[36m134.1331\u001b[0m      \u001b[32m175.1883\u001b[0m  0.0010  15.3805\n",
      "      7      \u001b[36m120.9409\u001b[0m      314.7051  0.0005  15.1766\n",
      "      7      \u001b[36m119.2627\u001b[0m      166.4424  0.0010  15.3496\n",
      "      7      \u001b[36m142.5872\u001b[0m      131.0715  0.0010  15.2449\n",
      "      8      \u001b[36m150.3713\u001b[0m      \u001b[32m165.5861\u001b[0m  0.0010  14.3191\n",
      "      8      136.4665      209.8483  0.0010  14.2748\n",
      "      8      \u001b[36m113.9828\u001b[0m      187.6476  0.0010  14.1833\n",
      "      8      \u001b[36m119.3766\u001b[0m      321.1656  0.0005  14.6094\n",
      "      8      \u001b[36m137.4574\u001b[0m      \u001b[32m118.1850\u001b[0m  0.0010  14.3662\n",
      "      9      \u001b[36m144.2386\u001b[0m      170.9478  0.0010  13.9271\n",
      "      9      \u001b[36m132.1786\u001b[0m      184.6560  0.0010  14.1166\n",
      "      9      \u001b[36m105.2157\u001b[0m      138.2855  0.0005  14.0553\n",
      "      9      \u001b[36m115.6730\u001b[0m      325.9579  0.0005  14.0569\n",
      "      9      \u001b[36m133.1512\u001b[0m      119.2354  0.0010  14.0263\n",
      "     10      \u001b[36m142.5886\u001b[0m      168.9612  0.0010  14.4149\n",
      "     10      133.5078      197.4873  0.0010  14.3903\n",
      "     10      \u001b[36m102.5010\u001b[0m      156.3772  0.0005  14.3345\n",
      "     10      \u001b[36m128.8063\u001b[0m      123.2942  0.0010  14.4275\n",
      "     10      \u001b[36m112.5890\u001b[0m      321.1211  0.0005  14.4851\n",
      "     11      \u001b[36m139.6621\u001b[0m      172.1907  0.0010  14.7536\n",
      "     11      \u001b[36m129.7231\u001b[0m      215.0835  0.0010  15.0305\n",
      "     11       \u001b[36m99.7653\u001b[0m      162.9699  0.0005  15.0673\n",
      "     11      \u001b[36m106.4404\u001b[0m      271.7501  0.0003  15.1068\n",
      "     11      \u001b[36m122.6393\u001b[0m      \u001b[32m117.5987\u001b[0m  0.0010  15.1684\n",
      "     12      141.1559      172.2909  0.0010  15.1755\n",
      "     12      141.5118      \u001b[32m128.6639\u001b[0m  0.0005  15.1984\n",
      "     12       \u001b[36m96.9903\u001b[0m      168.5721  0.0005  15.2853\n",
      "     12      \u001b[36m117.6419\u001b[0m      123.2884  0.0010  15.0792\n",
      "     13      155.6607      \u001b[32m120.6787\u001b[0m  0.0005  13.3346\n",
      "     13      131.2549      138.5970  0.0005  13.2813\n",
      "     13      \u001b[36m112.9057\u001b[0m      121.8150  0.0010  13.0410\n",
      "     13       \u001b[36m90.6999\u001b[0m      138.5236  0.0003  13.3709\n",
      "     14      141.8527      127.8630  0.0005  13.2585\n",
      "     14      \u001b[36m129.1711\u001b[0m      146.5287  0.0005  13.6274\n",
      "     14      \u001b[36m110.4878\u001b[0m      119.9527  0.0010  13.4522\n",
      "     15      \u001b[36m139.1556\u001b[0m      138.2316  0.0005  12.3570\n",
      "     15      \u001b[36m127.6987\u001b[0m      147.5550  0.0005  12.1750\n",
      "     15      \u001b[36m104.8699\u001b[0m      125.9769  0.0010  12.0128\n",
      "     16      \u001b[36m138.1160\u001b[0m      145.7338  0.0005  11.8671\n",
      "     16      116.3839      127.8737  0.0005  12.1551\n",
      "     16      \u001b[36m125.0564\u001b[0m      150.1513  0.0005  12.2477\n",
      "     17      \u001b[36m136.5468\u001b[0m      156.0105  0.0005  11.4539\n",
      "     17      124.6566      \u001b[32m117.4156\u001b[0m  0.0005  11.4534\n",
      "     17      129.1627      144.2184  0.0003  11.4332\n",
      "     18      142.1830      123.1642  0.0003  11.4286\n",
      "     18      116.1441      \u001b[32m115.7990\u001b[0m  0.0005  11.5841\n",
      "     18      131.4540      150.5934  0.0003  11.6348\n",
      "     19      141.9290      124.1816  0.0003  12.1781\n",
      "     19      113.9773      \u001b[32m113.6878\u001b[0m  0.0005  12.6884\n",
      "     19      130.7673      153.7514  0.0003  12.6719\n",
      "     20      140.3256      123.2150  0.0003  13.1349\n",
      "     20      111.6987      \u001b[32m113.3111\u001b[0m  0.0005  13.0033\n",
      "     20      129.6098      153.4353  0.0003  13.1373\n",
      "     21      138.5458      122.7319  0.0003  12.0674\n",
      "     21      109.2259      \u001b[32m110.7101\u001b[0m  0.0005  12.0660\n",
      "     21      132.9765      \u001b[32m124.8128\u001b[0m  0.0001  12.0171\n",
      "     22      140.2374      125.6407  0.0001  11.9406\n",
      "     22      106.0143      \u001b[32m110.3215\u001b[0m  0.0005  12.0971\n",
      "     22      \u001b[36m118.6468\u001b[0m      \u001b[32m123.0215\u001b[0m  0.0001  12.3353\n",
      "     23      \u001b[36m102.9409\u001b[0m      110.8808  0.0005  11.8255\n",
      "     23      \u001b[36m117.2531\u001b[0m      123.0795  0.0001  11.7085\n",
      "     24       \u001b[36m99.9609\u001b[0m      111.4918  0.0005  10.2844\n",
      "     24      \u001b[36m115.8471\u001b[0m      123.2778  0.0001  10.3897\n",
      "     25       \u001b[36m96.0932\u001b[0m      113.8886  0.0005  9.9214\n",
      "     25      \u001b[36m114.5489\u001b[0m      123.4739  0.0001  10.0378\n",
      "     26       \u001b[36m93.8248\u001b[0m      113.7180  0.0005  9.8933\n",
      "     26      \u001b[36m113.3610\u001b[0m      123.6717  0.0001  10.0478\n",
      "     27      106.2938      132.3175  0.0003  10.1849\n",
      "     27      \u001b[36m111.5903\u001b[0m      147.7345  0.0001  10.2469\n",
      "     28      103.4070      126.2022  0.0003  9.8933\n",
      "     28      \u001b[36m107.7993\u001b[0m      144.8521  0.0001  10.0370\n",
      "     29       99.5720      137.4297  0.0003  10.2635\n",
      "     29      \u001b[36m106.6416\u001b[0m      141.7951  0.0001  10.3277\n",
      "     30       99.3771      153.9985  0.0003  10.1346\n",
      "     30      \u001b[36m105.8769\u001b[0m      139.6274  0.0001  10.1470\n",
      "     31      106.9268      125.6265  0.0001  10.2223\n",
      "     31      \u001b[36m103.4313\u001b[0m      138.5633  0.0000  10.3102\n",
      "RMSE for each folder: [11.84212014 15.36254378 11.58963046  9.84877608  9.72626717]\n",
      "RMSE mean: 11.673867526592275\n"
     ]
    }
   ],
   "source": [
    "#Free\n",
    "cross_validation_with_scores(x_f,y_f,groups_f,pipe_cnn,logo,rmse_scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abdb180-6ce2-42f5-8745-04c47650ba73",
   "metadata": {},
   "source": [
    "The mean rmse for this cnn was better than that of the covariance pipeline, but it takes a really long time to execute compared to that method. If only I could have the benefits of both approaches…\n",
    "\n",
    "It was with this train of thought that I created a hybrid between the cnn and the covariance matrice method. For this hybrid, I didn’t need a CNN because the data after the covariance matrice step was  small, so I just built a two-layer nn and the results are very close to the cnn but it runs much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e5ee8f6-481f-4723-bd8c-96a023576f07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "#Small nn\n",
    "class Cov_nn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.s = nn.Sequential(\n",
    "          nn.Linear(36, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(128, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(64, 51)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.s(x)\n",
    "\n",
    "net_nn2 = NeuralNetRegressor(\n",
    "    module      = Cov_nn,\n",
    "    max_epochs  = 200,\n",
    "    lr          = 1e-3,\n",
    "    batch_size  = 64,\n",
    "    optimizer   = torch.optim.Adam,\n",
    "    callbacks=[('earlystop', EarlyStopping('valid_loss', patience=5)), # Stop if validation loss doesn't improve for 5 epochs \n",
    "                    ('lr_sched', LRScheduler(\n",
    "           policy=torch.optim.lr_scheduler.ReduceLROnPlateau, # Halve LR if validation loss stalls for 3 epochs\n",
    "           monitor='valid_loss',\n",
    "           patience=3, factor=0.5))]\n",
    ")\n",
    "\n",
    "\n",
    "pipe_fusion = Pipeline([\n",
    "    ('cov',   Covariances(estimator='oas')),   \n",
    "    ('ts',    TangentSpace()),                \n",
    "    ('scale', StandardScaler()),               \n",
    "    ('cast',  FunctionTransformer(lambda X: X.astype(np.float32), validate=False)), # cast to float32 for PyTorch compatibility\n",
    "    ('nn',   net_nn2)                          \n",
    "])\n",
    "\n",
    "x_cov_nn = X_g_train_reshape.astype('float32')\n",
    "x_cov_nn_f = X_f_train_reshape.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77ca50b6-460a-4868-a4a4-09c76ec4f6ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m463.9438\u001b[0m      \u001b[32m385.1099\u001b[0m  0.0010  0.1092\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m456.9825\u001b[0m      \u001b[32m375.9054\u001b[0m  0.0010  0.1149\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m463.5073\u001b[0m      \u001b[32m379.5745\u001b[0m  0.0010  0.1192\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m467.8356\u001b[0m      \u001b[32m396.4691\u001b[0m  0.0010  0.1442\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m469.2179\u001b[0m      \u001b[32m393.7036\u001b[0m  0.0010  0.1261\n",
      "      2      \u001b[36m270.9305\u001b[0m      \u001b[32m174.6277\u001b[0m  0.0010  0.1313\n",
      "      2      \u001b[36m259.0113\u001b[0m      \u001b[32m171.8371\u001b[0m  0.0010  0.1355\n",
      "      2      \u001b[36m285.5589\u001b[0m      \u001b[32m173.8935\u001b[0m  0.0010  0.1258\n",
      "      2      \u001b[36m264.0672\u001b[0m      \u001b[32m169.8182\u001b[0m  0.0010  0.1336\n",
      "      2      \u001b[36m286.1873\u001b[0m      \u001b[32m180.9821\u001b[0m  0.0010  0.1335\n",
      "      3      \u001b[36m166.1235\u001b[0m      \u001b[32m136.0250\u001b[0m  0.0010  0.1174\n",
      "      3      \u001b[36m163.6912\u001b[0m      \u001b[32m131.6867\u001b[0m  0.0010  0.1073\n",
      "      3      \u001b[36m165.1028\u001b[0m      \u001b[32m141.6717\u001b[0m  0.0010  0.1279\n",
      "      3      \u001b[36m167.3513\u001b[0m      \u001b[32m134.1842\u001b[0m  0.0010  0.1224\n",
      "      3      \u001b[36m171.3194\u001b[0m      \u001b[32m138.8273\u001b[0m  0.0010  0.1183\n",
      "      4      \u001b[36m140.2595\u001b[0m      \u001b[32m122.5045\u001b[0m  0.0010  0.1180\n",
      "      4      \u001b[36m138.1652\u001b[0m      \u001b[32m118.7344\u001b[0m  0.0010  0.1079\n",
      "      4      \u001b[36m142.5947\u001b[0m      \u001b[32m127.5607\u001b[0m  0.0010  0.1195\n",
      "      4      \u001b[36m142.8328\u001b[0m      \u001b[32m122.4727\u001b[0m  0.0010  0.1243\n",
      "      5      \u001b[36m125.2799\u001b[0m      \u001b[32m110.4142\u001b[0m  0.0010  0.1110\n",
      "      5      \u001b[36m119.5609\u001b[0m      \u001b[32m109.6400\u001b[0m  0.0010  0.1307\n",
      "      5      \u001b[36m123.6726\u001b[0m      \u001b[32m114.9299\u001b[0m  0.0010  0.1384\n",
      "      5      \u001b[36m126.7971\u001b[0m      \u001b[32m108.3515\u001b[0m  0.0010  0.1425\n",
      "      4      \u001b[36m143.3042\u001b[0m      \u001b[32m128.3937\u001b[0m  0.0010  0.2738\n",
      "      6      \u001b[36m111.8304\u001b[0m       \u001b[32m99.8386\u001b[0m  0.0010  0.1273\n",
      "      6      \u001b[36m106.6030\u001b[0m      \u001b[32m101.4475\u001b[0m  0.0010  0.1145\n",
      "      6      \u001b[36m109.8918\u001b[0m      \u001b[32m105.6420\u001b[0m  0.0010  0.1127\n",
      "      6      \u001b[36m113.0091\u001b[0m       \u001b[32m97.1786\u001b[0m  0.0010  0.1291\n",
      "      5      \u001b[36m125.1542\u001b[0m      \u001b[32m115.3559\u001b[0m  0.0010  0.1284\n",
      "      7      \u001b[36m100.0579\u001b[0m       \u001b[32m91.1610\u001b[0m  0.0010  0.1335\n",
      "      7       \u001b[36m99.6292\u001b[0m       \u001b[32m94.5959\u001b[0m  0.0010  0.1327\n",
      "      7       \u001b[36m99.0535\u001b[0m       \u001b[32m99.6196\u001b[0m  0.0010  0.1298\n",
      "      7      \u001b[36m103.4149\u001b[0m       \u001b[32m88.6602\u001b[0m  0.0010  0.1333\n",
      "      6      \u001b[36m110.3926\u001b[0m      \u001b[32m105.2985\u001b[0m  0.0010  0.1335\n",
      "      8       \u001b[36m94.1841\u001b[0m       \u001b[32m82.6670\u001b[0m  0.0010  0.1368\n",
      "      8       \u001b[36m94.6272\u001b[0m       \u001b[32m87.4056\u001b[0m  0.0010  0.1410\n",
      "      8       \u001b[36m94.3311\u001b[0m       \u001b[32m94.0371\u001b[0m  0.0010  0.1283\n",
      "      8       \u001b[36m96.5179\u001b[0m       \u001b[32m83.2363\u001b[0m  0.0010  0.1262\n",
      "      7      \u001b[36m101.1171\u001b[0m       \u001b[32m93.6740\u001b[0m  0.0010  0.1341\n",
      "      9       \u001b[36m89.2020\u001b[0m       \u001b[32m77.7299\u001b[0m  0.0010  0.1398\n",
      "      9       \u001b[36m89.8301\u001b[0m       \u001b[32m83.2402\u001b[0m  0.0010  0.1242\n",
      "      9       \u001b[36m90.1638\u001b[0m       \u001b[32m88.4628\u001b[0m  0.0010  0.1413\n",
      "      9       \u001b[36m92.8119\u001b[0m       \u001b[32m78.9277\u001b[0m  0.0010  0.1500\n",
      "      8       \u001b[36m94.9974\u001b[0m       \u001b[32m89.1371\u001b[0m  0.0010  0.1427\n",
      "     10       \u001b[36m85.3140\u001b[0m       \u001b[32m74.1455\u001b[0m  0.0010  0.1477\n",
      "     10       \u001b[36m84.1353\u001b[0m       \u001b[32m78.8097\u001b[0m  0.0010  0.1362\n",
      "     10       \u001b[36m86.6650\u001b[0m       \u001b[32m85.0508\u001b[0m  0.0010  0.1320\n",
      "     10       \u001b[36m89.2218\u001b[0m       \u001b[32m74.6083\u001b[0m  0.0010  0.1132\n",
      "      9       \u001b[36m90.8097\u001b[0m       \u001b[32m85.1828\u001b[0m  0.0010  0.1194\n",
      "     11       \u001b[36m81.0276\u001b[0m       \u001b[32m71.4853\u001b[0m  0.0010  0.1269\n",
      "     11       \u001b[36m82.8335\u001b[0m       \u001b[32m79.6350\u001b[0m  0.0010  0.1089\n",
      "     11       \u001b[36m81.8467\u001b[0m       \u001b[32m75.2443\u001b[0m  0.0010  0.1310\n",
      "     11       \u001b[36m85.5756\u001b[0m       \u001b[32m72.2337\u001b[0m  0.0010  0.1132\n",
      "     10       \u001b[36m85.3985\u001b[0m       \u001b[32m79.6599\u001b[0m  0.0010  0.1205\n",
      "     12       \u001b[36m77.6221\u001b[0m       \u001b[32m69.5153\u001b[0m  0.0010  0.1221\n",
      "     12       \u001b[36m79.5289\u001b[0m       \u001b[32m76.3514\u001b[0m  0.0010  0.1192\n",
      "     12       \u001b[36m77.8244\u001b[0m       \u001b[32m71.5614\u001b[0m  0.0010  0.1422\n",
      "     12       \u001b[36m82.3153\u001b[0m       \u001b[32m70.2095\u001b[0m  0.0010  0.1241\n",
      "     11       \u001b[36m81.3299\u001b[0m       \u001b[32m74.5811\u001b[0m  0.0010  0.1279\n",
      "     13       \u001b[36m78.1653\u001b[0m       \u001b[32m73.1164\u001b[0m  0.0010  0.1235\n",
      "     13       \u001b[36m74.9504\u001b[0m       \u001b[32m66.7715\u001b[0m  0.0010  0.1407\n",
      "     13       \u001b[36m76.0545\u001b[0m       \u001b[32m68.9715\u001b[0m  0.0010  0.1313\n",
      "     13       \u001b[36m80.0905\u001b[0m       \u001b[32m67.4333\u001b[0m  0.0010  0.1274\n",
      "     12       \u001b[36m79.6137\u001b[0m       \u001b[32m73.4517\u001b[0m  0.0010  0.1127\n",
      "     14       \u001b[36m73.9642\u001b[0m       \u001b[32m69.5005\u001b[0m  0.0010  0.1082\n",
      "     14       \u001b[36m73.9335\u001b[0m       \u001b[32m64.4106\u001b[0m  0.0010  0.1173\n",
      "     14       \u001b[36m77.1414\u001b[0m       \u001b[32m65.1467\u001b[0m  0.0010  0.1165\n",
      "     14       \u001b[36m73.7654\u001b[0m       \u001b[32m66.6701\u001b[0m  0.0010  0.1328\n",
      "     13       \u001b[36m74.9251\u001b[0m       \u001b[32m69.8821\u001b[0m  0.0010  0.1308\n",
      "     15       \u001b[36m71.0588\u001b[0m       \u001b[32m66.5852\u001b[0m  0.0010  0.1197\n",
      "     15       \u001b[36m70.6710\u001b[0m       \u001b[32m61.8941\u001b[0m  0.0010  0.1287\n",
      "     15       \u001b[36m74.6614\u001b[0m       \u001b[32m59.9345\u001b[0m  0.0010  0.1180\n",
      "     15       \u001b[36m72.5163\u001b[0m       \u001b[32m63.4119\u001b[0m  0.0010  0.1298\n",
      "     14       \u001b[36m71.7429\u001b[0m       \u001b[32m63.8319\u001b[0m  0.0010  0.1215\n",
      "     16       \u001b[36m68.0049\u001b[0m       \u001b[32m64.4699\u001b[0m  0.0010  0.1266\n",
      "     16       \u001b[36m67.4456\u001b[0m       \u001b[32m57.7348\u001b[0m  0.0010  0.1076\n",
      "     16       \u001b[36m70.9890\u001b[0m       \u001b[32m57.8264\u001b[0m  0.0010  0.1170\n",
      "     16       \u001b[36m67.8510\u001b[0m       \u001b[32m61.3956\u001b[0m  0.0010  0.1145\n",
      "     15       \u001b[36m70.8301\u001b[0m       \u001b[32m61.7751\u001b[0m  0.0010  0.1250\n",
      "     17       \u001b[36m67.5998\u001b[0m       \u001b[32m61.1247\u001b[0m  0.0010  0.1116\n",
      "     17       \u001b[36m64.3414\u001b[0m       \u001b[32m56.7042\u001b[0m  0.0010  0.1168\n",
      "     17       \u001b[36m67.5964\u001b[0m       \u001b[32m55.9380\u001b[0m  0.0010  0.1250\n",
      "     17       \u001b[36m66.8648\u001b[0m       \u001b[32m58.8834\u001b[0m  0.0010  0.1093\n",
      "     16       \u001b[36m68.5999\u001b[0m       \u001b[32m61.2722\u001b[0m  0.0010  0.1217\n",
      "     18       \u001b[36m63.1052\u001b[0m       \u001b[32m58.8184\u001b[0m  0.0010  0.1049\n",
      "     18       \u001b[36m63.4234\u001b[0m       \u001b[32m54.4227\u001b[0m  0.0010  0.1270\n",
      "     18       \u001b[36m65.8292\u001b[0m       \u001b[32m53.7942\u001b[0m  0.0010  0.1031\n",
      "     18       \u001b[36m63.7325\u001b[0m       \u001b[32m58.0325\u001b[0m  0.0010  0.1197\n",
      "     17       \u001b[36m64.9148\u001b[0m       \u001b[32m58.8358\u001b[0m  0.0010  0.1194\n",
      "     19       \u001b[36m61.7925\u001b[0m       \u001b[32m56.8832\u001b[0m  0.0010  0.1164\n",
      "     19       \u001b[36m61.2392\u001b[0m       \u001b[32m52.1273\u001b[0m  0.0010  0.1224\n",
      "     19       \u001b[36m63.9207\u001b[0m       \u001b[32m51.1198\u001b[0m  0.0010  0.1284\n",
      "     19       \u001b[36m59.9501\u001b[0m       \u001b[32m55.3614\u001b[0m  0.0010  0.1146\n",
      "     18       \u001b[36m63.8569\u001b[0m       \u001b[32m57.3887\u001b[0m  0.0010  0.1051\n",
      "     20       \u001b[36m60.6363\u001b[0m       \u001b[32m54.2146\u001b[0m  0.0010  0.1189\n",
      "     20       \u001b[36m63.0297\u001b[0m       \u001b[32m49.9285\u001b[0m  0.0010  0.0967\n",
      "     20       \u001b[36m59.4284\u001b[0m       53.4289  0.0010  0.1238\n",
      "     20       \u001b[36m59.3723\u001b[0m       \u001b[32m53.4230\u001b[0m  0.0010  0.1225\n",
      "     19       \u001b[36m60.5838\u001b[0m       \u001b[32m57.0539\u001b[0m  0.0010  0.1224\n",
      "     21       \u001b[36m57.7445\u001b[0m       \u001b[32m53.7512\u001b[0m  0.0010  0.1152\n",
      "     21       \u001b[36m61.0135\u001b[0m       \u001b[32m47.6086\u001b[0m  0.0010  0.1095\n",
      "     21       \u001b[36m57.0906\u001b[0m       \u001b[32m51.2025\u001b[0m  0.0010  0.1118\n",
      "     21       \u001b[36m57.2218\u001b[0m       \u001b[32m51.2231\u001b[0m  0.0010  0.1140\n",
      "     20       \u001b[36m59.6507\u001b[0m       \u001b[32m51.4040\u001b[0m  0.0010  0.1291\n",
      "     22       \u001b[36m56.0720\u001b[0m       \u001b[32m52.0633\u001b[0m  0.0010  0.1162\n",
      "     22       \u001b[36m60.0520\u001b[0m       \u001b[32m46.7048\u001b[0m  0.0010  0.0994\n",
      "     22       \u001b[36m56.1438\u001b[0m       \u001b[32m48.3702\u001b[0m  0.0010  0.1158\n",
      "     22       \u001b[36m56.9232\u001b[0m       \u001b[32m49.9504\u001b[0m  0.0010  0.1030\n",
      "     21       59.6753       \u001b[32m51.3463\u001b[0m  0.0010  0.1090\n",
      "     23       \u001b[36m54.2855\u001b[0m       \u001b[32m51.2451\u001b[0m  0.0010  0.1097\n",
      "     23       \u001b[36m57.9858\u001b[0m       \u001b[32m44.2576\u001b[0m  0.0010  0.1209\n",
      "     23       \u001b[36m55.3048\u001b[0m       \u001b[32m47.2276\u001b[0m  0.0010  0.1148\n",
      "     23       \u001b[36m54.8297\u001b[0m       \u001b[32m49.0031\u001b[0m  0.0010  0.1128\n",
      "     24       \u001b[36m52.8358\u001b[0m       \u001b[32m50.1160\u001b[0m  0.0010  0.1072\n",
      "     22       \u001b[36m58.4467\u001b[0m       \u001b[32m49.9978\u001b[0m  0.0010  0.1248\n",
      "     24       \u001b[36m56.3207\u001b[0m       \u001b[32m43.5042\u001b[0m  0.0010  0.1208\n",
      "     24       \u001b[36m53.7038\u001b[0m       \u001b[32m46.3046\u001b[0m  0.0010  0.1115\n",
      "     24       \u001b[36m53.6509\u001b[0m       \u001b[32m47.5156\u001b[0m  0.0010  0.1121\n",
      "     25       \u001b[36m51.4983\u001b[0m       50.3061  0.0010  0.1175\n",
      "     23       \u001b[36m56.7611\u001b[0m       \u001b[32m48.7176\u001b[0m  0.0010  0.1297\n",
      "     25       \u001b[36m52.4270\u001b[0m       \u001b[32m45.8929\u001b[0m  0.0010  0.1068\n",
      "     25       \u001b[36m54.5140\u001b[0m       \u001b[32m42.5389\u001b[0m  0.0010  0.1221\n",
      "     25       53.6526       \u001b[32m45.5566\u001b[0m  0.0010  0.1108\n",
      "     26       52.2631       \u001b[32m48.1786\u001b[0m  0.0010  0.1261\n",
      "     24       \u001b[36m55.9767\u001b[0m       \u001b[32m48.5050\u001b[0m  0.0010  0.1219\n",
      "     26       \u001b[36m53.1050\u001b[0m       \u001b[32m40.3970\u001b[0m  0.0010  0.1311\n",
      "     26       \u001b[36m51.6853\u001b[0m       \u001b[32m44.0247\u001b[0m  0.0010  0.1189\n",
      "     26       \u001b[36m52.2442\u001b[0m       \u001b[32m44.4035\u001b[0m  0.0010  0.1395\n",
      "     27       \u001b[36m49.9070\u001b[0m       \u001b[32m47.9499\u001b[0m  0.0010  0.1171\n",
      "     25       \u001b[36m54.8868\u001b[0m       \u001b[32m47.4924\u001b[0m  0.0010  0.1241\n",
      "     27       \u001b[36m51.1116\u001b[0m       \u001b[32m39.6440\u001b[0m  0.0010  0.1100\n",
      "     27       \u001b[36m50.7101\u001b[0m       \u001b[32m43.5252\u001b[0m  0.0010  0.1157\n",
      "     27       \u001b[36m50.3994\u001b[0m       \u001b[32m43.0698\u001b[0m  0.0010  0.1111\n",
      "     28       \u001b[36m49.3068\u001b[0m       \u001b[32m45.9343\u001b[0m  0.0010  0.0948\n",
      "     26       \u001b[36m54.1466\u001b[0m       \u001b[32m44.7477\u001b[0m  0.0010  0.0982\n",
      "     28       51.7856       \u001b[32m39.3127\u001b[0m  0.0010  0.1158\n",
      "     28       \u001b[36m50.2296\u001b[0m       \u001b[32m41.7801\u001b[0m  0.0010  0.1186\n",
      "     28       51.1636       \u001b[32m42.0456\u001b[0m  0.0010  0.1279\n",
      "     29       \u001b[36m47.2208\u001b[0m       \u001b[32m44.6714\u001b[0m  0.0010  0.1109\n",
      "     27       \u001b[36m51.8188\u001b[0m       \u001b[32m43.5908\u001b[0m  0.0010  0.1207\n",
      "     29       \u001b[36m50.3466\u001b[0m       \u001b[32m37.3134\u001b[0m  0.0010  0.1138\n",
      "     29       \u001b[36m48.8336\u001b[0m       \u001b[32m40.3907\u001b[0m  0.0010  0.1009\n",
      "     29       \u001b[36m49.4300\u001b[0m       \u001b[32m41.6089\u001b[0m  0.0010  0.1198\n",
      "     30       \u001b[36m44.8320\u001b[0m       \u001b[32m43.8401\u001b[0m  0.0010  0.1112\n",
      "     28       52.3352       45.6678  0.0010  0.1310\n",
      "     30       \u001b[36m49.9407\u001b[0m       \u001b[32m36.7449\u001b[0m  0.0010  0.1015\n",
      "     30       \u001b[36m48.7749\u001b[0m       40.4681  0.0010  0.1227\n",
      "     30       \u001b[36m46.7646\u001b[0m       \u001b[32m38.9239\u001b[0m  0.0010  0.1098\n",
      "     31       45.2219       \u001b[32m43.3824\u001b[0m  0.0010  0.1008\n",
      "     29       \u001b[36m50.9219\u001b[0m       43.8262  0.0010  0.1024\n",
      "     32       44.8438       44.0197  0.0010  0.0800\n",
      "     31       \u001b[36m46.8096\u001b[0m       \u001b[32m37.6956\u001b[0m  0.0010  0.1072\n",
      "     31       \u001b[36m47.1802\u001b[0m       \u001b[32m35.0870\u001b[0m  0.0010  0.1484\n",
      "     31       47.6986       \u001b[32m38.3770\u001b[0m  0.0010  0.1154\n",
      "     30       \u001b[36m50.8570\u001b[0m       \u001b[32m42.3078\u001b[0m  0.0010  0.1066\n",
      "     32       \u001b[36m46.5837\u001b[0m       38.8976  0.0010  0.1101\n",
      "     33       \u001b[36m42.2692\u001b[0m       \u001b[32m42.9598\u001b[0m  0.0010  0.1246\n",
      "     32       \u001b[36m45.3378\u001b[0m       \u001b[32m37.1505\u001b[0m  0.0010  0.1247\n",
      "     32       48.1934       \u001b[32m34.7830\u001b[0m  0.0010  0.1419\n",
      "     31       50.9618       \u001b[32m39.4524\u001b[0m  0.0010  0.1143\n",
      "     34       42.3581       \u001b[32m40.7697\u001b[0m  0.0010  0.1079\n",
      "     33       45.6337       \u001b[32m36.8108\u001b[0m  0.0010  0.1314\n",
      "     33       \u001b[36m46.6901\u001b[0m       35.1065  0.0010  0.1184\n",
      "     32       \u001b[36m48.1999\u001b[0m       39.8304  0.0010  0.1066\n",
      "     35       \u001b[36m39.9913\u001b[0m       \u001b[32m39.3910\u001b[0m  0.0010  0.1325\n",
      "     33       \u001b[36m45.1347\u001b[0m       \u001b[32m37.4692\u001b[0m  0.0010  0.2667\n",
      "     34       45.3966       \u001b[32m35.3623\u001b[0m  0.0010  0.1124\n",
      "     34       \u001b[36m46.3589\u001b[0m       \u001b[32m33.1014\u001b[0m  0.0010  0.1140\n",
      "     33       49.0302       \u001b[32m38.7576\u001b[0m  0.0010  0.1404\n",
      "     36       41.0457       \u001b[32m38.3606\u001b[0m  0.0010  0.1168\n",
      "     34       \u001b[36m44.0201\u001b[0m       \u001b[32m36.1439\u001b[0m  0.0010  0.1369\n",
      "     35       \u001b[36m44.8862\u001b[0m       36.0409  0.0010  0.1285\n",
      "     35       \u001b[36m44.6468\u001b[0m       33.1281  0.0010  0.1451\n",
      "     34       \u001b[36m46.7077\u001b[0m       \u001b[32m37.0349\u001b[0m  0.0010  0.1245\n",
      "     37       \u001b[36m38.4249\u001b[0m       38.8765  0.0010  0.1358\n",
      "     36       \u001b[36m42.9492\u001b[0m       \u001b[32m34.4662\u001b[0m  0.0010  0.1225\n",
      "     35       \u001b[36m43.3781\u001b[0m       \u001b[32m35.4117\u001b[0m  0.0010  0.1354\n",
      "     36       \u001b[36m43.8089\u001b[0m       \u001b[32m32.0248\u001b[0m  0.0010  0.1221\n",
      "     35       \u001b[36m46.0724\u001b[0m       \u001b[32m36.3271\u001b[0m  0.0010  0.1165\n",
      "     38       39.2269       38.5230  0.0010  0.1167\n",
      "     37       \u001b[36m42.3493\u001b[0m       34.6419  0.0010  0.1142\n",
      "     36       \u001b[36m41.7788\u001b[0m       \u001b[32m33.9083\u001b[0m  0.0010  0.1137\n",
      "     37       44.0669       \u001b[32m31.1686\u001b[0m  0.0010  0.1320\n",
      "     36       46.6166       37.9062  0.0010  0.1194\n",
      "     39       \u001b[36m38.3874\u001b[0m       \u001b[32m37.1639\u001b[0m  0.0010  0.1285\n",
      "     37       42.1190       34.2152  0.0010  0.1208\n",
      "     38       42.4633       \u001b[32m33.7685\u001b[0m  0.0010  0.1454\n",
      "     38       43.9765       \u001b[32m30.7894\u001b[0m  0.0010  0.1119\n",
      "     37       \u001b[36m45.1124\u001b[0m       \u001b[32m35.0287\u001b[0m  0.0010  0.1327\n",
      "     40       \u001b[36m37.9584\u001b[0m       \u001b[32m36.3849\u001b[0m  0.0010  0.1204\n",
      "     38       42.4190       \u001b[32m32.6373\u001b[0m  0.0010  0.1379\n",
      "     39       \u001b[36m42.6591\u001b[0m       \u001b[32m29.4977\u001b[0m  0.0010  0.1209\n",
      "     39       \u001b[36m41.1586\u001b[0m       \u001b[32m33.4640\u001b[0m  0.0010  0.1413\n",
      "     38       45.5523       35.8505  0.0010  0.1314\n",
      "     41       \u001b[36m37.1815\u001b[0m       \u001b[32m35.8557\u001b[0m  0.0010  0.1410\n",
      "     39       \u001b[36m40.3522\u001b[0m       32.8801  0.0010  0.1238\n",
      "     40       \u001b[36m42.4596\u001b[0m       29.8610  0.0010  0.1236\n",
      "     40       42.0684       \u001b[32m32.6038\u001b[0m  0.0010  0.1307\n",
      "     39       45.8974       \u001b[32m34.5383\u001b[0m  0.0010  0.1323\n",
      "     42       \u001b[36m34.3358\u001b[0m       \u001b[32m35.5501\u001b[0m  0.0010  0.1230\n",
      "     40       41.4384       \u001b[32m31.3000\u001b[0m  0.0010  0.1068\n",
      "     41       \u001b[36m41.6419\u001b[0m       \u001b[32m28.9773\u001b[0m  0.0010  0.1333\n",
      "     41       41.2172       \u001b[32m32.5751\u001b[0m  0.0010  0.1376\n",
      "     40       \u001b[36m44.3525\u001b[0m       35.4612  0.0010  0.1433\n",
      "     41       \u001b[36m39.8563\u001b[0m       31.4164  0.0010  0.1008\n",
      "     43       36.8434       35.6614  0.0010  0.1479\n",
      "     42       \u001b[36m41.0303\u001b[0m       \u001b[32m32.1587\u001b[0m  0.0010  0.1067\n",
      "     42       \u001b[36m41.0044\u001b[0m       29.5573  0.0010  0.1427\n",
      "     41       44.4425       \u001b[32m31.3594\u001b[0m  0.0010  0.1012\n",
      "     42       \u001b[36m38.8028\u001b[0m       32.2708  0.0010  0.1239\n",
      "     43       \u001b[36m39.6544\u001b[0m       \u001b[32m30.9880\u001b[0m  0.0010  0.1164\n",
      "     43       \u001b[36m39.9351\u001b[0m       \u001b[32m27.4681\u001b[0m  0.0010  0.1257\n",
      "     42       \u001b[36m42.7765\u001b[0m       32.8775  0.0010  0.1338\n",
      "     43       \u001b[36m38.5748\u001b[0m       \u001b[32m30.0835\u001b[0m  0.0010  0.1095\n",
      "     44       35.7379       \u001b[32m35.2724\u001b[0m  0.0010  0.2560\n",
      "     43       \u001b[36m42.4728\u001b[0m       33.0118  0.0010  0.1121\n",
      "     44       40.8423       28.6777  0.0010  0.1403\n",
      "     44       \u001b[36m37.8039\u001b[0m       \u001b[32m29.9511\u001b[0m  0.0010  0.1256\n",
      "     45       35.1026       \u001b[32m34.1245\u001b[0m  0.0010  0.1458\n",
      "     45       \u001b[36m39.1444\u001b[0m       27.9525  0.0010  0.1803\n",
      "     44       39.9187       \u001b[32m30.4853\u001b[0m  0.0010  0.3478\n",
      "     45       \u001b[36m37.6141\u001b[0m       \u001b[32m29.2149\u001b[0m  0.0010  0.1686\n",
      "     44       \u001b[36m40.8572\u001b[0m       32.2082  0.0010  0.1998\n",
      "     46       34.9566       35.7466  0.0010  0.1613\n",
      "     45       40.3163       30.9497  0.0010  0.1138\n",
      "     45       41.1799       \u001b[32m31.2198\u001b[0m  0.0010  0.1161\n",
      "     46       39.2974       \u001b[32m26.6778\u001b[0m  0.0010  0.1341\n",
      "     46       \u001b[36m36.4176\u001b[0m       29.2995  0.0010  0.1273\n",
      "     47       \u001b[36m34.0836\u001b[0m       34.1370  0.0010  0.1198\n",
      "     46       \u001b[36m39.4225\u001b[0m       \u001b[32m30.2652\u001b[0m  0.0010  0.1201\n",
      "     47       \u001b[36m38.8713\u001b[0m       27.4081  0.0010  0.1096\n",
      "     47       37.0153       \u001b[32m28.8871\u001b[0m  0.0010  0.1173\n",
      "     46       \u001b[36m40.5509\u001b[0m       \u001b[32m30.6595\u001b[0m  0.0010  0.1357\n",
      "     48       34.2231       \u001b[32m33.8278\u001b[0m  0.0010  0.1387\n",
      "     48       39.6268       \u001b[32m26.5093\u001b[0m  0.0010  0.1101\n",
      "     47       \u001b[36m39.3311\u001b[0m       \u001b[32m29.4662\u001b[0m  0.0010  0.1273\n",
      "     48       36.9389       \u001b[32m28.6181\u001b[0m  0.0010  0.1234\n",
      "     47       41.3392       \u001b[32m30.0098\u001b[0m  0.0010  0.1261\n",
      "     49       34.2890       \u001b[32m33.3627\u001b[0m  0.0010  0.1208\n",
      "     48       \u001b[36m38.1864\u001b[0m       29.9711  0.0010  0.1163\n",
      "     49       \u001b[36m36.2771\u001b[0m       \u001b[32m27.8381\u001b[0m  0.0010  0.1109\n",
      "     49       \u001b[36m38.2450\u001b[0m       \u001b[32m25.5939\u001b[0m  0.0010  0.1365\n",
      "     48       \u001b[36m40.2627\u001b[0m       30.7515  0.0010  0.1234\n",
      "     49       38.3229       29.6403  0.0010  0.1062\n",
      "     50       \u001b[36m33.9664\u001b[0m       33.8058  0.0010  0.1263\n",
      "     50       \u001b[36m35.7212\u001b[0m       \u001b[32m27.0578\u001b[0m  0.0010  0.1028\n",
      "     50       \u001b[36m37.1528\u001b[0m       \u001b[32m25.5226\u001b[0m  0.0010  0.1292\n",
      "     49       40.4522       \u001b[32m29.1222\u001b[0m  0.0010  0.1204\n",
      "     51       \u001b[36m33.8988\u001b[0m       \u001b[32m32.0576\u001b[0m  0.0010  0.1089\n",
      "     51       \u001b[36m34.4480\u001b[0m       \u001b[32m26.2170\u001b[0m  0.0010  0.1254\n",
      "     50       38.3208       \u001b[32m29.0245\u001b[0m  0.0010  0.1289\n",
      "     51       37.4315       \u001b[32m25.4566\u001b[0m  0.0010  0.1256\n",
      "     50       \u001b[36m39.9309\u001b[0m       \u001b[32m29.0920\u001b[0m  0.0010  0.1273\n",
      "     52       \u001b[36m33.4483\u001b[0m       32.6872  0.0010  0.0952\n",
      "     52       34.8799       \u001b[32m26.1088\u001b[0m  0.0010  0.1127\n",
      "     51       38.7153       29.4965  0.0010  0.1292\n",
      "     52       37.5062       \u001b[32m25.0632\u001b[0m  0.0010  0.1223\n",
      "     51       \u001b[36m39.5188\u001b[0m       \u001b[32m28.9181\u001b[0m  0.0010  0.1132\n",
      "     53       \u001b[36m32.2789\u001b[0m       33.7289  0.0010  0.1261\n",
      "     53       35.7192       \u001b[32m26.1021\u001b[0m  0.0010  0.1225\n",
      "     52       \u001b[36m36.8549\u001b[0m       \u001b[32m28.7117\u001b[0m  0.0010  0.1216\n",
      "     52       \u001b[36m39.0328\u001b[0m       \u001b[32m27.8377\u001b[0m  0.0010  0.1053\n",
      "     53       \u001b[36m36.3964\u001b[0m       25.3438  0.0010  0.1216\n",
      "     54       \u001b[36m32.2179\u001b[0m       \u001b[32m31.9206\u001b[0m  0.0010  0.1203\n",
      "     53       \u001b[36m36.6998\u001b[0m       \u001b[32m27.9989\u001b[0m  0.0010  0.1005\n",
      "     54       \u001b[36m33.5518\u001b[0m       \u001b[32m25.6766\u001b[0m  0.0010  0.1246\n",
      "     54       \u001b[36m36.1394\u001b[0m       \u001b[32m24.5075\u001b[0m  0.0010  0.1203\n",
      "     53       \u001b[36m38.5597\u001b[0m       29.2703  0.0010  0.1366\n",
      "     55       32.9864       \u001b[32m31.5992\u001b[0m  0.0010  0.1271\n",
      "     55       33.8687       26.4037  0.0010  0.1164\n",
      "     54       \u001b[36m36.4153\u001b[0m       \u001b[32m27.8601\u001b[0m  0.0010  0.1326\n",
      "     55       \u001b[36m35.8119\u001b[0m       \u001b[32m23.5883\u001b[0m  0.0010  0.1155\n",
      "     54       \u001b[36m38.1983\u001b[0m       29.9667  0.0010  0.1155\n",
      "     56       \u001b[36m32.0007\u001b[0m       33.9177  0.0010  0.1017\n",
      "     55       37.8167       \u001b[32m27.6294\u001b[0m  0.0010  0.1286\n",
      "     56       34.0412       \u001b[32m24.6556\u001b[0m  0.0010  0.1462\n",
      "     55       \u001b[36m37.6079\u001b[0m       28.1878  0.0010  0.1082\n",
      "     56       \u001b[36m35.0774\u001b[0m       24.1202  0.0010  0.1292\n",
      "     57       \u001b[36m31.8977\u001b[0m       \u001b[32m30.6949\u001b[0m  0.0010  0.1091\n",
      "     56       \u001b[36m35.7533\u001b[0m       \u001b[32m27.4020\u001b[0m  0.0010  0.1062\n",
      "     57       33.8179       25.0529  0.0010  0.1354\n",
      "     57       35.8743       24.0433  0.0010  0.1267\n",
      "     56       \u001b[36m37.4022\u001b[0m       \u001b[32m27.6256\u001b[0m  0.0010  0.1377\n",
      "     58       32.0683       31.8206  0.0010  0.1214\n",
      "     57       37.3208       \u001b[32m26.8719\u001b[0m  0.0010  0.1200\n",
      "     58       33.8202       \u001b[32m24.2543\u001b[0m  0.0010  0.1211\n",
      "     57       38.0023       27.8060  0.0010  0.1176\n",
      "     59       \u001b[36m31.3743\u001b[0m       \u001b[32m30.3616\u001b[0m  0.0010  0.1174\n",
      "     58       \u001b[36m33.8889\u001b[0m       24.2614  0.0010  0.1373\n",
      "     58       \u001b[36m35.0666\u001b[0m       \u001b[32m26.8148\u001b[0m  0.0010  0.1273\n",
      "     59       \u001b[36m32.8422\u001b[0m       \u001b[32m23.3429\u001b[0m  0.0010  0.1160\n",
      "     58       \u001b[36m37.2582\u001b[0m       \u001b[32m25.7484\u001b[0m  0.0010  0.1182\n",
      "     59       35.2007       \u001b[32m23.3738\u001b[0m  0.0010  0.1111\n",
      "     60       32.4594       31.2589  0.0010  0.1237\n",
      "     59       36.2196       26.8610  0.0010  0.1096\n",
      "     60       33.5802       24.2321  0.0010  0.1013\n",
      "     60       33.9799       23.3946  0.0010  0.1006\n",
      "     59       \u001b[36m36.2910\u001b[0m       25.8349  0.0010  0.1268\n",
      "     61       31.7573       31.4109  0.0010  0.1244\n",
      "     60       \u001b[36m34.6601\u001b[0m       \u001b[32m26.1918\u001b[0m  0.0010  0.1351\n",
      "     61       35.2332       \u001b[32m22.9649\u001b[0m  0.0010  0.1154\n",
      "     61       \u001b[36m31.6747\u001b[0m       23.7167  0.0010  0.1414\n",
      "     60       37.3090       \u001b[32m25.3739\u001b[0m  0.0010  0.1219\n",
      "     62       \u001b[36m29.9265\u001b[0m       30.8295  0.0010  0.1315\n",
      "     61       34.7809       26.4414  0.0010  0.1104\n",
      "     62       34.1049       23.2254  0.0010  0.1190\n",
      "     62       32.7153       24.0160  0.0010  0.1286\n",
      "     63       30.9615       31.1111  0.0010  0.1164\n",
      "     61       \u001b[36m36.2479\u001b[0m       26.1719  0.0010  0.1434\n",
      "     62       35.3767       26.2774  0.0010  0.1076\n",
      "     63       \u001b[36m33.4263\u001b[0m       \u001b[32m22.6080\u001b[0m  0.0010  0.1245\n",
      "     63       32.1926       24.1003  0.0010  0.1334\n",
      "     62       \u001b[36m35.6282\u001b[0m       25.6427  0.0010  0.1291\n",
      "     63       \u001b[36m34.5109\u001b[0m       \u001b[32m25.9936\u001b[0m  0.0010  0.1383\n",
      "     64       33.7065       \u001b[32m21.9880\u001b[0m  0.0010  0.1312\n",
      "     64       \u001b[36m31.6178\u001b[0m       \u001b[32m23.0645\u001b[0m  0.0005  0.1144\n",
      "     63       36.2493       26.1571  0.0010  0.1238\n",
      "     64       \u001b[36m33.0090\u001b[0m       \u001b[32m25.4628\u001b[0m  0.0010  0.1225\n",
      "     65       \u001b[36m32.3240\u001b[0m       \u001b[32m21.0851\u001b[0m  0.0010  0.1171\n",
      "     65       \u001b[36m31.0951\u001b[0m       \u001b[32m22.8609\u001b[0m  0.0005  0.1185\n",
      "     64       36.2278       \u001b[32m25.2533\u001b[0m  0.0010  0.1318\n",
      "     65       34.3064       25.5988  0.0010  0.1273\n",
      "     66       33.0106       22.0392  0.0010  0.1433\n",
      "     66       32.8123       \u001b[32m22.4484\u001b[0m  0.0005  0.1535\n",
      "     65       \u001b[36m34.3590\u001b[0m       \u001b[32m24.7809\u001b[0m  0.0010  0.1441\n",
      "     66       \u001b[36m32.6849\u001b[0m       \u001b[32m25.4316\u001b[0m  0.0010  0.1471\n",
      "     67       34.4589       21.1309  0.0010  0.1210\n",
      "     67       \u001b[36m30.1444\u001b[0m       22.5476  0.0005  0.1152\n",
      "     66       35.3126       \u001b[32m23.3146\u001b[0m  0.0010  0.1061\n",
      "     67       33.1491       \u001b[32m25.1865\u001b[0m  0.0010  0.1287\n",
      "     68       33.5315       21.9195  0.0010  0.1248\n",
      "     68       \u001b[36m30.0063\u001b[0m       23.1835  0.0005  0.1158\n",
      "     67       34.8045       23.8345  0.0010  0.1154\n",
      "     68       33.3236       25.1978  0.0010  0.0955\n",
      "     69       33.6201       21.7278  0.0010  0.1047\n",
      "     69       31.0145       \u001b[32m22.3602\u001b[0m  0.0005  0.1087\n",
      "     68       \u001b[36m34.2960\u001b[0m       24.7212  0.0010  0.1117\n",
      "     69       \u001b[36m32.0792\u001b[0m       \u001b[32m24.2205\u001b[0m  0.0010  0.1072\n",
      "     70       \u001b[36m32.0065\u001b[0m       \u001b[32m20.8695\u001b[0m  0.0005  0.1128\n",
      "     70       30.4330       \u001b[32m22.2828\u001b[0m  0.0005  0.1156\n",
      "     69       36.2704       25.1468  0.0010  0.1254\n",
      "     70       32.7459       24.2937  0.0010  0.1228\n",
      "     71       30.5312       \u001b[32m22.0200\u001b[0m  0.0005  0.1037\n",
      "     71       \u001b[36m30.9743\u001b[0m       20.9386  0.0005  0.1204\n",
      "     70       \u001b[36m34.1421\u001b[0m       24.5552  0.0010  0.1069\n",
      "     71       32.4941       24.8212  0.0010  0.1113\n",
      "     72       30.7485       22.0844  0.0005  0.1167\n",
      "     72       31.4852       \u001b[32m20.8377\u001b[0m  0.0005  0.1267\n",
      "     72       \u001b[36m31.6439\u001b[0m       24.4875  0.0010  0.1033\n",
      "     73       \u001b[36m29.9865\u001b[0m       \u001b[32m21.6681\u001b[0m  0.0005  0.1108\n",
      "     73       32.3926       \u001b[32m20.8197\u001b[0m  0.0005  0.1207\n",
      "     73       \u001b[36m31.5736\u001b[0m       24.3067  0.0010  0.1189\n",
      "     74       \u001b[36m29.6505\u001b[0m       \u001b[32m21.4576\u001b[0m  0.0005  0.1207\n",
      "     74       \u001b[36m30.5979\u001b[0m       20.9622  0.0005  0.1122\n",
      "     74       31.8564       \u001b[32m23.8710\u001b[0m  0.0005  0.1206\n",
      "     75       30.2825       22.0491  0.0005  0.1048\n",
      "     75       31.5309       \u001b[32m20.6192\u001b[0m  0.0005  0.1054\n",
      "     75       \u001b[36m30.1529\u001b[0m       \u001b[32m23.8363\u001b[0m  0.0005  0.1039\n",
      "     76       30.4669       21.5623  0.0005  0.1164\n",
      "     76       32.6744       \u001b[32m20.1845\u001b[0m  0.0005  0.1859\n",
      "     77       30.8067       21.9969  0.0005  0.1342\n",
      "     76       31.3639       23.8771  0.0005  0.1780\n",
      "     77       \u001b[36m30.5884\u001b[0m       \u001b[32m20.1374\u001b[0m  0.0005  0.1005\n",
      "     77       30.8124       24.1357  0.0005  0.1006\n",
      "     78       30.0625       \u001b[32m21.0434\u001b[0m  0.0005  0.1141\n",
      "     78       \u001b[36m30.5411\u001b[0m       20.3697  0.0005  0.0968\n",
      "     79       30.4981       21.7497  0.0005  0.0878\n",
      "     78       31.1428       \u001b[32m23.7786\u001b[0m  0.0005  0.1001\n",
      "     79       31.5470       20.2616  0.0005  0.0962\n",
      "     80       30.0573       21.8238  0.0005  0.1008\n",
      "     79       31.1768       \u001b[32m23.4008\u001b[0m  0.0005  0.1004\n",
      "     80       31.4595       \u001b[32m19.8510\u001b[0m  0.0005  0.1096\n",
      "     80       \u001b[36m30.1183\u001b[0m       23.5261  0.0005  0.0958\n",
      "     81       \u001b[36m28.5488\u001b[0m       21.6269  0.0005  0.1108\n",
      "     81       \u001b[36m30.4677\u001b[0m       20.1384  0.0005  0.0981\n",
      "     81       30.6871       \u001b[32m23.3253\u001b[0m  0.0005  0.0819\n",
      "     82       29.8647       21.2181  0.0005  0.0870\n",
      "     82       32.5118       \u001b[32m19.6112\u001b[0m  0.0005  0.0975\n",
      "     82       \u001b[36m29.7990\u001b[0m       \u001b[32m23.1978\u001b[0m  0.0005  0.0929\n",
      "     83       31.4222       20.7260  0.0005  0.0850\n",
      "     83       30.8677       \u001b[32m23.1284\u001b[0m  0.0005  0.0821\n",
      "     84       30.8211       \u001b[32m19.3593\u001b[0m  0.0005  0.0773\n",
      "     84       30.0538       23.2995  0.0005  0.0784\n",
      "     85       30.6985       19.4538  0.0005  0.0869\n",
      "     85       \u001b[36m29.4338\u001b[0m       23.1376  0.0005  0.0853\n",
      "     86       29.7358       \u001b[32m22.9136\u001b[0m  0.0005  0.0734\n",
      "     86       \u001b[36m30.0968\u001b[0m       19.6043  0.0005  0.1147\n",
      "     87       \u001b[36m29.4196\u001b[0m       \u001b[32m22.8539\u001b[0m  0.0005  0.0737\n",
      "     87       30.6799       19.3937  0.0005  0.0846\n",
      "     88       \u001b[36m29.2199\u001b[0m       \u001b[32m22.4834\u001b[0m  0.0005  0.0754\n",
      "     88       \u001b[36m29.5133\u001b[0m       19.4534  0.0005  0.0915\n",
      "     89       29.7225       \u001b[32m22.4087\u001b[0m  0.0005  0.0982\n",
      "     89       30.3826       \u001b[32m19.3515\u001b[0m  0.0003  0.0950\n",
      "     90       30.0425       22.8718  0.0005  0.0825\n",
      "     90       29.6847       \u001b[32m19.0517\u001b[0m  0.0003  0.0772\n",
      "     91       \u001b[36m28.8102\u001b[0m       \u001b[32m22.2957\u001b[0m  0.0005  0.0839\n",
      "     91       \u001b[36m29.5083\u001b[0m       \u001b[32m18.8270\u001b[0m  0.0003  0.0818\n",
      "     92       29.3203       \u001b[32m21.8214\u001b[0m  0.0005  0.0800\n",
      "     92       30.0093       19.1828  0.0003  0.0789\n",
      "     93       30.5787       22.2227  0.0005  0.0824\n",
      "     93       \u001b[36m28.7741\u001b[0m       19.0347  0.0003  0.0972\n",
      "     94       30.5110       22.5456  0.0005  0.0850\n",
      "     94       30.3536       18.8982  0.0003  0.0821\n",
      "     95       \u001b[36m28.0945\u001b[0m       22.2236  0.0005  0.0776\n",
      "     95       30.4010       \u001b[32m18.7772\u001b[0m  0.0003  0.0785\n",
      "     96       29.6966       22.3306  0.0005  0.0900\n",
      "     96       29.4745       18.9204  0.0003  0.0896\n",
      "     97       30.3331       18.9546  0.0003  0.0898\n",
      "     98       29.6410       18.8392  0.0003  0.0759\n",
      "     99       30.2766       18.7942  0.0003  0.0623\n",
      "RMSE for each folder: [5.87956904 5.09309729 5.85883917 4.43030728 4.4797616 ]\n",
      "RMSE mean: 5.148314877411356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m701.7604\u001b[0m      \u001b[32m465.9023\u001b[0m  0.0010  0.1618\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m640.2996\u001b[0m      \u001b[32m458.6602\u001b[0m  0.0010  0.1509\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m569.2093\u001b[0m      \u001b[32m838.9061\u001b[0m  0.0010  0.1490\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m676.5895\u001b[0m      \u001b[32m444.7971\u001b[0m  0.0010  0.1426\n",
      "      2      \u001b[36m391.0130\u001b[0m      \u001b[32m191.6481\u001b[0m  0.0010  0.1565\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m533.0908\u001b[0m      \u001b[32m440.4294\u001b[0m  0.0010  0.1422\n",
      "      2      \u001b[36m331.2604\u001b[0m      \u001b[32m189.6586\u001b[0m  0.0010  0.1368\n",
      "      2      \u001b[36m333.6544\u001b[0m      \u001b[32m336.8626\u001b[0m  0.0010  0.1445\n",
      "      3      \u001b[36m188.8320\u001b[0m      \u001b[32m145.1500\u001b[0m  0.0010  0.1374\n",
      "      2      \u001b[36m364.0546\u001b[0m      \u001b[32m183.3906\u001b[0m  0.0010  0.1543\n",
      "      2      \u001b[36m271.6770\u001b[0m      \u001b[32m190.2349\u001b[0m  0.0010  0.1314\n",
      "      3      \u001b[36m166.4569\u001b[0m      \u001b[32m148.8172\u001b[0m  0.0010  0.1381\n",
      "      3      \u001b[36m164.6791\u001b[0m      \u001b[32m214.8662\u001b[0m  0.0010  0.1429\n",
      "      3      \u001b[36m178.0269\u001b[0m      \u001b[32m144.9717\u001b[0m  0.0010  0.1558\n",
      "      3      \u001b[36m149.8682\u001b[0m      \u001b[32m153.8255\u001b[0m  0.0010  0.1703\n",
      "      4      \u001b[36m161.9166\u001b[0m      \u001b[32m134.2452\u001b[0m  0.0010  0.1848\n",
      "      4      \u001b[36m145.8264\u001b[0m      \u001b[32m138.2297\u001b[0m  0.0010  0.1912\n",
      "      4      \u001b[36m142.8110\u001b[0m      \u001b[32m194.5061\u001b[0m  0.0010  0.1944\n",
      "      4      \u001b[36m156.1245\u001b[0m      \u001b[32m136.9918\u001b[0m  0.0010  0.1614\n",
      "      4      \u001b[36m137.1184\u001b[0m      \u001b[32m142.0424\u001b[0m  0.0010  0.1533\n",
      "      5      \u001b[36m154.1965\u001b[0m      \u001b[32m128.7972\u001b[0m  0.0010  0.1727\n",
      "      5      \u001b[36m139.0814\u001b[0m      \u001b[32m132.8610\u001b[0m  0.0010  0.1513\n",
      "      5      \u001b[36m136.4596\u001b[0m      \u001b[32m185.3890\u001b[0m  0.0010  0.1651\n",
      "      5      \u001b[36m131.0460\u001b[0m      \u001b[32m135.7283\u001b[0m  0.0010  0.1308\n",
      "      5      \u001b[36m150.1574\u001b[0m      \u001b[32m133.4234\u001b[0m  0.0010  0.1655\n",
      "      6      \u001b[36m147.6586\u001b[0m      \u001b[32m125.0631\u001b[0m  0.0010  0.1464\n",
      "      6      \u001b[36m134.6996\u001b[0m      \u001b[32m129.7566\u001b[0m  0.0010  0.1397\n",
      "      6      \u001b[36m132.6705\u001b[0m      \u001b[32m175.6321\u001b[0m  0.0010  0.1425\n",
      "      6      \u001b[36m126.3648\u001b[0m      \u001b[32m132.8794\u001b[0m  0.0010  0.1497\n",
      "      6      \u001b[36m142.8731\u001b[0m      \u001b[32m130.9779\u001b[0m  0.0010  0.1593\n",
      "      7      \u001b[36m142.9846\u001b[0m      \u001b[32m122.5169\u001b[0m  0.0010  0.1650\n",
      "      7      \u001b[36m129.3499\u001b[0m      \u001b[32m127.4970\u001b[0m  0.0010  0.1757\n",
      "      7      \u001b[36m126.7285\u001b[0m      \u001b[32m170.1192\u001b[0m  0.0010  0.1602\n",
      "      7      \u001b[36m122.4248\u001b[0m      \u001b[32m130.5200\u001b[0m  0.0010  0.1539\n",
      "      7      \u001b[36m138.7170\u001b[0m      \u001b[32m127.6570\u001b[0m  0.0010  0.1484\n",
      "      8      \u001b[36m139.8576\u001b[0m      \u001b[32m120.7446\u001b[0m  0.0010  0.1412\n",
      "      8      \u001b[36m125.3534\u001b[0m      \u001b[32m125.7679\u001b[0m  0.0010  0.1431\n",
      "      8      \u001b[36m124.5430\u001b[0m      \u001b[32m162.3007\u001b[0m  0.0010  0.1535\n",
      "      8      \u001b[36m119.1695\u001b[0m      \u001b[32m128.7722\u001b[0m  0.0010  0.1586\n",
      "      8      \u001b[36m135.9208\u001b[0m      \u001b[32m126.6961\u001b[0m  0.0010  0.1474\n",
      "      9      \u001b[36m134.9213\u001b[0m      \u001b[32m119.0696\u001b[0m  0.0010  0.1580\n",
      "      9      \u001b[36m122.8483\u001b[0m      \u001b[32m124.9815\u001b[0m  0.0010  0.1434\n",
      "      9      \u001b[36m119.6817\u001b[0m      \u001b[32m156.9644\u001b[0m  0.0010  0.1478\n",
      "      9      \u001b[36m115.7584\u001b[0m      \u001b[32m126.8985\u001b[0m  0.0010  0.1585\n",
      "      9      \u001b[36m131.7587\u001b[0m      \u001b[32m126.1653\u001b[0m  0.0010  0.1586\n",
      "     10      \u001b[36m131.5476\u001b[0m      \u001b[32m118.0507\u001b[0m  0.0010  0.1637\n",
      "     10      \u001b[36m117.9736\u001b[0m      \u001b[32m123.6420\u001b[0m  0.0010  0.1410\n",
      "     10      \u001b[36m117.2193\u001b[0m      \u001b[32m151.7451\u001b[0m  0.0010  0.1614\n",
      "     10      \u001b[36m113.7490\u001b[0m      \u001b[32m125.8274\u001b[0m  0.0010  0.1657\n",
      "     10      \u001b[36m128.1687\u001b[0m      \u001b[32m124.9691\u001b[0m  0.0010  0.1535\n",
      "     11      \u001b[36m128.8049\u001b[0m      \u001b[32m117.3747\u001b[0m  0.0010  0.1482\n",
      "     11      \u001b[36m115.1177\u001b[0m      \u001b[32m123.0845\u001b[0m  0.0010  0.1547\n",
      "     11      \u001b[36m114.2023\u001b[0m      \u001b[32m147.2735\u001b[0m  0.0010  0.1499\n",
      "     11      \u001b[36m111.5147\u001b[0m      \u001b[32m124.0824\u001b[0m  0.0010  0.1497\n",
      "     11      \u001b[36m127.1991\u001b[0m      \u001b[32m124.7834\u001b[0m  0.0010  0.1689\n",
      "     12      \u001b[36m113.0442\u001b[0m      123.6574  0.0010  0.1460\n",
      "     12      \u001b[36m128.6672\u001b[0m      \u001b[32m116.7967\u001b[0m  0.0010  0.1751\n",
      "     12      114.8090      \u001b[32m145.1411\u001b[0m  0.0010  0.1503\n",
      "     12      \u001b[36m109.5509\u001b[0m      \u001b[32m122.2946\u001b[0m  0.0010  0.1546\n",
      "     12      \u001b[36m121.9032\u001b[0m      \u001b[32m123.7446\u001b[0m  0.0010  0.1586\n",
      "     13      \u001b[36m112.8244\u001b[0m      123.4338  0.0010  0.1624\n",
      "     13      \u001b[36m125.9322\u001b[0m      \u001b[32m116.6234\u001b[0m  0.0010  0.1475\n",
      "     13      \u001b[36m111.7674\u001b[0m      \u001b[32m144.0256\u001b[0m  0.0010  0.1395\n",
      "     13      \u001b[36m107.8989\u001b[0m      \u001b[32m122.2203\u001b[0m  0.0010  0.1581\n",
      "     13      \u001b[36m120.0464\u001b[0m      \u001b[32m123.6957\u001b[0m  0.0010  0.1511\n",
      "     14      \u001b[36m123.8418\u001b[0m      \u001b[32m115.8027\u001b[0m  0.0010  0.1476\n",
      "     14      \u001b[36m109.0406\u001b[0m      123.2855  0.0010  0.1608\n",
      "     14      \u001b[36m110.2391\u001b[0m      \u001b[32m140.7589\u001b[0m  0.0010  0.1488\n",
      "     14      \u001b[36m107.2541\u001b[0m      \u001b[32m120.8218\u001b[0m  0.0010  0.1327\n",
      "     14      \u001b[36m119.6159\u001b[0m      \u001b[32m123.2409\u001b[0m  0.0010  0.1283\n",
      "     15      109.9359      \u001b[32m122.2786\u001b[0m  0.0010  0.1137\n",
      "     15      \u001b[36m123.2965\u001b[0m      \u001b[32m115.5780\u001b[0m  0.0010  0.1571\n",
      "     15      \u001b[36m109.5115\u001b[0m      141.3299  0.0010  0.1232\n",
      "     15      \u001b[36m105.5416\u001b[0m      \u001b[32m119.4510\u001b[0m  0.0010  0.1354\n",
      "     15      \u001b[36m118.6285\u001b[0m      \u001b[32m122.4249\u001b[0m  0.0010  0.1321\n",
      "     16      \u001b[36m108.7360\u001b[0m      122.3151  0.0010  0.1347\n",
      "     16      \u001b[36m121.4683\u001b[0m      \u001b[32m114.9324\u001b[0m  0.0010  0.1327\n",
      "     16      \u001b[36m106.4517\u001b[0m      \u001b[32m140.3283\u001b[0m  0.0010  0.1587\n",
      "     16      105.9005      \u001b[32m118.8558\u001b[0m  0.0010  0.1437\n",
      "     16      \u001b[36m116.2658\u001b[0m      122.4507  0.0010  0.1417\n",
      "     17      \u001b[36m107.9564\u001b[0m      \u001b[32m121.5973\u001b[0m  0.0010  0.1315\n",
      "     17      \u001b[36m120.8380\u001b[0m      \u001b[32m114.2707\u001b[0m  0.0010  0.1370\n",
      "     17      \u001b[36m105.5592\u001b[0m      \u001b[32m138.5888\u001b[0m  0.0010  0.1640\n",
      "     17      \u001b[36m114.8737\u001b[0m      \u001b[32m122.1008\u001b[0m  0.0010  0.1483\n",
      "     17      \u001b[36m103.4633\u001b[0m      \u001b[32m118.7171\u001b[0m  0.0010  0.1633\n",
      "     18      \u001b[36m106.4983\u001b[0m      \u001b[32m120.2660\u001b[0m  0.0010  0.1698\n",
      "     18      \u001b[36m120.8377\u001b[0m      \u001b[32m113.4497\u001b[0m  0.0010  0.1598\n",
      "     18      \u001b[36m105.1680\u001b[0m      \u001b[32m138.0766\u001b[0m  0.0010  0.1505\n",
      "     18      \u001b[36m102.5342\u001b[0m      \u001b[32m117.9882\u001b[0m  0.0010  0.1593\n",
      "     18      \u001b[36m113.4994\u001b[0m      \u001b[32m120.7976\u001b[0m  0.0010  0.1635\n",
      "     19      \u001b[36m105.7485\u001b[0m      121.2746  0.0010  0.1804\n",
      "     19      \u001b[36m118.6739\u001b[0m      113.5129  0.0010  0.1887\n",
      "     19      \u001b[36m104.6247\u001b[0m      \u001b[32m137.5782\u001b[0m  0.0010  0.1954\n",
      "     19      \u001b[36m102.2918\u001b[0m      \u001b[32m117.1861\u001b[0m  0.0010  0.2035\n",
      "     19      \u001b[36m113.1905\u001b[0m      \u001b[32m120.5386\u001b[0m  0.0010  0.2041\n",
      "     20      \u001b[36m104.4034\u001b[0m      120.6711  0.0010  0.1906\n",
      "     20      \u001b[36m118.3035\u001b[0m      113.6823  0.0010  0.1795\n",
      "     20      \u001b[36m104.0049\u001b[0m      \u001b[32m135.8221\u001b[0m  0.0010  0.1610\n",
      "     20      \u001b[36m101.0597\u001b[0m      117.8335  0.0010  0.1496\n",
      "     20      \u001b[36m112.4936\u001b[0m      \u001b[32m120.5182\u001b[0m  0.0010  0.1618\n",
      "     21      \u001b[36m103.5797\u001b[0m      121.0890  0.0010  0.1687\n",
      "     21      \u001b[36m117.3014\u001b[0m      \u001b[32m113.2077\u001b[0m  0.0010  0.1722\n",
      "     21      \u001b[36m103.9907\u001b[0m      \u001b[32m134.9587\u001b[0m  0.0010  0.1733\n",
      "     21       \u001b[36m99.5876\u001b[0m      117.5887  0.0010  0.1880\n",
      "     21      \u001b[36m110.6913\u001b[0m      \u001b[32m119.6013\u001b[0m  0.0010  0.1854\n",
      "     22      \u001b[36m103.0282\u001b[0m      120.5175  0.0010  0.1647\n",
      "     22      117.4232      \u001b[32m112.7983\u001b[0m  0.0010  0.1885\n",
      "     22      \u001b[36m102.8358\u001b[0m      \u001b[32m134.2350\u001b[0m  0.0010  0.1661\n",
      "     22      100.4637      117.4103  0.0010  0.1572\n",
      "     22      \u001b[36m109.2123\u001b[0m      120.2844  0.0010  0.1709\n",
      "     23      \u001b[36m100.4943\u001b[0m      \u001b[32m119.2429\u001b[0m  0.0005  0.1700\n",
      "     23      \u001b[36m115.9980\u001b[0m      112.8132  0.0010  0.1686\n",
      "     23      \u001b[36m102.3506\u001b[0m      \u001b[32m133.3927\u001b[0m  0.0010  0.1570\n",
      "     23       \u001b[36m99.5576\u001b[0m      \u001b[32m116.9989\u001b[0m  0.0010  0.1490\n",
      "     23      \u001b[36m108.1272\u001b[0m      \u001b[32m119.3683\u001b[0m  0.0010  0.1686\n",
      "     24      \u001b[36m100.0123\u001b[0m      \u001b[32m118.8172\u001b[0m  0.0005  0.1484\n",
      "     24       \u001b[36m99.4189\u001b[0m      \u001b[32m132.7964\u001b[0m  0.0010  0.1493\n",
      "     24      \u001b[36m115.4012\u001b[0m      \u001b[32m112.1816\u001b[0m  0.0010  0.1596\n",
      "     24      100.2548      \u001b[32m116.4410\u001b[0m  0.0010  0.1592\n",
      "     25       \u001b[36m99.3937\u001b[0m      119.3228  0.0005  0.1527\n",
      "     24      110.4048      119.5198  0.0010  0.1599\n",
      "     25      100.3221      135.6625  0.0010  0.1569\n",
      "     25      \u001b[36m114.2629\u001b[0m      \u001b[32m112.0370\u001b[0m  0.0010  0.1577\n",
      "     25       \u001b[36m98.1244\u001b[0m      \u001b[32m116.3045\u001b[0m  0.0010  0.1601\n",
      "     26       99.9299      119.2912  0.0005  0.1675\n",
      "     25      109.2784      \u001b[32m119.0190\u001b[0m  0.0010  0.1902\n",
      "     26      100.9094      132.8177  0.0010  0.1535\n",
      "     26      114.9825      \u001b[32m111.7995\u001b[0m  0.0010  0.1724\n",
      "     26       \u001b[36m97.4057\u001b[0m      \u001b[32m115.5205\u001b[0m  0.0010  0.1640\n",
      "     27       \u001b[36m98.9365\u001b[0m      \u001b[32m118.4754\u001b[0m  0.0005  0.1435\n",
      "     27      100.0744      \u001b[32m131.4993\u001b[0m  0.0010  0.1265\n",
      "     26      \u001b[36m107.8532\u001b[0m      \u001b[32m118.1494\u001b[0m  0.0010  0.1494\n",
      "     27      \u001b[36m113.6674\u001b[0m      \u001b[32m111.5474\u001b[0m  0.0010  0.1476\n",
      "     28       99.0974      118.9769  0.0005  0.1195\n",
      "     27       \u001b[36m96.9300\u001b[0m      \u001b[32m115.5146\u001b[0m  0.0010  0.1652\n",
      "     28       \u001b[36m98.9619\u001b[0m      \u001b[32m130.5858\u001b[0m  0.0010  0.1555\n",
      "     27      \u001b[36m106.9255\u001b[0m      \u001b[32m118.0025\u001b[0m  0.0010  0.1517\n",
      "     28      \u001b[36m113.5632\u001b[0m      \u001b[32m110.8641\u001b[0m  0.0010  0.1384\n",
      "     29       99.5973      \u001b[32m117.9480\u001b[0m  0.0005  0.1645\n",
      "     29       \u001b[36m98.7692\u001b[0m      132.4619  0.0010  0.1294\n",
      "     28       97.2906      \u001b[32m115.1565\u001b[0m  0.0010  0.1679\n",
      "     28      \u001b[36m106.2097\u001b[0m      \u001b[32m117.5166\u001b[0m  0.0010  0.1582\n",
      "     29      \u001b[36m113.1967\u001b[0m      111.0536  0.0010  0.1682\n",
      "     30       \u001b[36m98.3344\u001b[0m      \u001b[32m117.8968\u001b[0m  0.0005  0.1469\n",
      "     29       \u001b[36m95.6116\u001b[0m      115.4560  0.0010  0.1439\n",
      "     30       \u001b[36m97.9799\u001b[0m      \u001b[32m130.0896\u001b[0m  0.0010  0.1554\n",
      "     29      106.2694      \u001b[32m117.4664\u001b[0m  0.0010  0.1459\n",
      "     30      113.3436      \u001b[32m110.6486\u001b[0m  0.0010  0.1565\n",
      "     31       \u001b[36m97.3130\u001b[0m      \u001b[32m117.6284\u001b[0m  0.0005  0.1486\n",
      "     30       95.9332      \u001b[32m114.7694\u001b[0m  0.0010  0.1505\n",
      "     31       \u001b[36m96.3701\u001b[0m      130.1268  0.0010  0.1495\n",
      "     30      \u001b[36m104.8898\u001b[0m      117.4711  0.0010  0.1665\n",
      "     31      \u001b[36m111.1764\u001b[0m      \u001b[32m110.3026\u001b[0m  0.0010  0.1585\n",
      "     32       97.3448      \u001b[32m117.4950\u001b[0m  0.0005  0.1670\n",
      "     31       96.0190      115.0073  0.0010  0.1915\n",
      "     32       97.7082      \u001b[32m129.2447\u001b[0m  0.0010  0.1852\n",
      "     31      105.7664      \u001b[32m116.4001\u001b[0m  0.0010  0.1808\n",
      "     32      \u001b[36m111.1297\u001b[0m      110.8087  0.0010  0.1820\n",
      "     33       \u001b[36m96.7160\u001b[0m      118.0943  0.0005  0.1779\n",
      "     33       97.2207      130.4138  0.0010  0.1578\n",
      "     32       \u001b[36m95.0301\u001b[0m      115.0202  0.0010  0.1770\n",
      "     32      105.2648      \u001b[32m116.1013\u001b[0m  0.0010  0.1467\n",
      "     33      \u001b[36m110.2718\u001b[0m      \u001b[32m110.2727\u001b[0m  0.0010  0.1475\n",
      "     34       \u001b[36m96.2861\u001b[0m      117.9899  0.0005  0.1551\n",
      "     34       \u001b[36m96.3284\u001b[0m      130.2464  0.0010  0.1586\n",
      "     33       \u001b[36m94.7716\u001b[0m      114.9523  0.0010  0.1796\n",
      "     33      \u001b[36m104.5049\u001b[0m      116.3735  0.0010  0.1888\n",
      "     34      112.0560      111.0237  0.0010  0.1778\n",
      "     35       97.1724      \u001b[32m117.3882\u001b[0m  0.0005  0.1863\n",
      "     35       \u001b[36m95.5352\u001b[0m      129.6960  0.0010  0.2057\n",
      "     34       95.8174      115.0972  0.0010  0.1971\n",
      "     34      \u001b[36m103.6973\u001b[0m      116.1715  0.0010  0.1896\n",
      "     35      \u001b[36m109.9145\u001b[0m      110.4045  0.0010  0.1943\n",
      "     36       96.6145      \u001b[32m117.0315\u001b[0m  0.0005  0.1963\n",
      "     36       96.4557      \u001b[32m129.2278\u001b[0m  0.0010  0.1943\n",
      "     35      \u001b[36m103.5357\u001b[0m      116.3493  0.0010  0.2028\n",
      "     36      111.1169      \u001b[32m110.0491\u001b[0m  0.0010  0.2009\n",
      "     37       96.7603      117.3160  0.0005  0.1864\n",
      "     37       96.0186      129.2673  0.0010  0.1925\n",
      "     36      \u001b[36m103.3021\u001b[0m      117.3028  0.0010  0.1640\n",
      "     37      \u001b[36m109.8518\u001b[0m      110.1919  0.0010  0.1623\n",
      "     38       \u001b[36m95.0038\u001b[0m      117.3534  0.0005  0.1497\n",
      "     38       \u001b[36m93.7959\u001b[0m      \u001b[32m128.9656\u001b[0m  0.0010  0.1664\n",
      "     38      109.9130      110.1653  0.0010  0.1336\n",
      "     39       95.2734      117.7476  0.0005  0.1557\n",
      "     39       94.9021      129.1319  0.0010  0.1445\n",
      "     39      \u001b[36m108.8617\u001b[0m      110.4813  0.0010  0.1521\n",
      "     40       95.4772      117.1118  0.0005  0.1383\n",
      "     40       94.6685      \u001b[32m128.9092\u001b[0m  0.0010  0.1317\n",
      "     40      \u001b[36m108.4014\u001b[0m      110.1898  0.0010  0.1567\n",
      "     41       94.0078      128.9632  0.0010  0.1494\n",
      "     41      108.5363      \u001b[32m109.9594\u001b[0m  0.0005  0.1740\n",
      "     42       94.6679      \u001b[32m127.9276\u001b[0m  0.0010  0.1466\n",
      "     42      \u001b[36m106.1347\u001b[0m      \u001b[32m109.7052\u001b[0m  0.0005  0.1222\n",
      "     43       94.4371      128.1047  0.0010  0.1090\n",
      "     43      107.1750      \u001b[32m109.0566\u001b[0m  0.0005  0.0985\n",
      "     44       94.5609      128.2347  0.0010  0.1077\n",
      "     44      \u001b[36m105.4665\u001b[0m      109.1583  0.0005  0.1034\n",
      "     45       \u001b[36m92.4551\u001b[0m      \u001b[32m124.8897\u001b[0m  0.0010  0.0982\n",
      "     45      107.2173      109.3050  0.0005  0.0895\n",
      "     46       93.7197      128.8368  0.0010  0.0954\n",
      "     46      \u001b[36m104.0724\u001b[0m      109.5064  0.0005  0.1149\n",
      "     47       94.0687      127.6705  0.0010  0.1089\n",
      "     47      105.6051      109.4526  0.0005  0.1209\n",
      "     48       92.4776      126.3454  0.0010  0.1145\n",
      "     48      105.5862      \u001b[32m108.8734\u001b[0m  0.0003  0.1173\n",
      "     49       \u001b[36m92.3049\u001b[0m      127.6709  0.0010  0.1117\n",
      "     49      105.1739      109.0789  0.0003  0.1234\n",
      "     50       \u001b[36m92.0749\u001b[0m      \u001b[32m123.1483\u001b[0m  0.0005  0.1453\n",
      "     50      105.3940      109.0740  0.0003  0.1780\n",
      "     51       \u001b[36m91.1564\u001b[0m      \u001b[32m122.9446\u001b[0m  0.0005  0.1697\n",
      "     51      104.4938      109.0243  0.0003  0.1227\n",
      "     52       \u001b[36m89.9911\u001b[0m      \u001b[32m122.5025\u001b[0m  0.0005  0.1277\n",
      "     52      \u001b[36m103.5303\u001b[0m      \u001b[32m108.7635\u001b[0m  0.0003  0.1326\n",
      "     53       90.6121      122.8767  0.0005  0.1258\n",
      "     53      103.8868      108.9247  0.0003  0.1260\n",
      "     54       91.1475      \u001b[32m122.2227\u001b[0m  0.0005  0.1099\n",
      "     54      \u001b[36m103.3686\u001b[0m      \u001b[32m108.6998\u001b[0m  0.0003  0.1085\n",
      "     55       \u001b[36m89.8651\u001b[0m      \u001b[32m122.0182\u001b[0m  0.0005  0.1121\n",
      "     55      106.0408      \u001b[32m108.6027\u001b[0m  0.0003  0.0993\n",
      "     56       90.4864      122.8523  0.0005  0.0889\n",
      "     56      107.0217      108.8885  0.0003  0.1044\n",
      "     57       91.9799      123.5786  0.0005  0.1100\n",
      "     57      103.9524      108.6420  0.0003  0.0974\n",
      "     58       90.3520      122.7576  0.0005  0.1075\n",
      "     58      103.9731      108.8272  0.0003  0.1210\n",
      "     59       90.4861      122.4059  0.0005  0.1248\n",
      "     59      104.9582      108.6527  0.0003  0.1243\n",
      "     60       \u001b[36m88.6576\u001b[0m      \u001b[32m121.3365\u001b[0m  0.0003  0.1225\n",
      "     61       90.4205      \u001b[32m120.9574\u001b[0m  0.0003  0.1314\n",
      "     62       \u001b[36m88.4988\u001b[0m      \u001b[32m120.5775\u001b[0m  0.0003  0.0870\n",
      "     63       89.3331      \u001b[32m120.5254\u001b[0m  0.0003  0.0803\n",
      "     64       89.6188      120.6559  0.0003  0.0918\n",
      "     65       89.6786      121.2949  0.0003  0.0859\n",
      "     66       \u001b[36m87.5836\u001b[0m      120.7725  0.0003  0.0776\n",
      "     67       89.7537      120.6754  0.0003  0.0875\n",
      "     68       89.9017      \u001b[32m120.4257\u001b[0m  0.0001  0.0881\n",
      "     69       88.9456      \u001b[32m120.4064\u001b[0m  0.0001  0.0814\n",
      "     70       89.2279      120.7993  0.0001  0.0765\n",
      "     71       89.3768      \u001b[32m120.3605\u001b[0m  0.0001  0.0832\n",
      "     72       88.4592      \u001b[32m120.0228\u001b[0m  0.0001  0.0820\n",
      "     73       87.9591      120.1077  0.0001  0.0824\n",
      "     74       88.8725      120.2616  0.0001  0.0768\n",
      "     75       88.5560      120.7076  0.0001  0.0805\n",
      "     76       88.8468      120.5913  0.0001  0.0846\n",
      "RMSE for each folder: [10.64508958 11.8683588  11.37481287 10.08922974  8.93139226]\n",
      "RMSE mean: 10.58177665078785\n"
     ]
    }
   ],
   "source": [
    "#Guided\n",
    "cross_validation_with_scores(x_cov_nn,y,groups,pipe_fusion,logo,rmse_scorer)\n",
    "\n",
    "#Free \n",
    "cross_validation_with_scores(x_cov_nn_f,y_f,groups_f,pipe_fusion,logo,rmse_scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5127f6c1-c9b2-47df-878b-aa7e2cfa63d6",
   "metadata": {},
   "source": [
    "![](./images/5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a2f89932-867f-4c54-9a38-d64b949fe003",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m440.9353\u001b[0m      \u001b[32m321.2495\u001b[0m  0.0010  0.0966\n",
      "      2      \u001b[36m217.0207\u001b[0m      \u001b[32m147.0345\u001b[0m  0.0010  0.0863\n",
      "      3      \u001b[36m150.9787\u001b[0m      \u001b[32m126.6180\u001b[0m  0.0010  0.0839\n",
      "      4      \u001b[36m128.9234\u001b[0m      \u001b[32m113.5365\u001b[0m  0.0010  0.0789\n",
      "      5      \u001b[36m112.8172\u001b[0m      \u001b[32m102.9149\u001b[0m  0.0010  0.0844\n",
      "      6      \u001b[36m101.2874\u001b[0m       \u001b[32m94.7909\u001b[0m  0.0010  0.0836\n",
      "      7       \u001b[36m93.6632\u001b[0m       \u001b[32m87.5280\u001b[0m  0.0010  0.0794\n",
      "      8       \u001b[36m87.4154\u001b[0m       \u001b[32m81.7757\u001b[0m  0.0010  0.0878\n",
      "      9       \u001b[36m83.5102\u001b[0m       \u001b[32m77.0341\u001b[0m  0.0010  0.0810\n",
      "     10       \u001b[36m79.0211\u001b[0m       \u001b[32m72.6960\u001b[0m  0.0010  0.0789\n",
      "     11       \u001b[36m75.2462\u001b[0m       \u001b[32m68.3102\u001b[0m  0.0010  0.0781\n",
      "     12       \u001b[36m70.8998\u001b[0m       \u001b[32m65.9711\u001b[0m  0.0010  0.0883\n",
      "     13       \u001b[36m67.6282\u001b[0m       \u001b[32m61.6043\u001b[0m  0.0010  0.0868\n",
      "     14       \u001b[36m65.3987\u001b[0m       \u001b[32m59.3400\u001b[0m  0.0010  0.0918\n",
      "     15       \u001b[36m63.0777\u001b[0m       \u001b[32m58.0101\u001b[0m  0.0010  0.0833\n",
      "     16       \u001b[36m60.6039\u001b[0m       \u001b[32m54.9997\u001b[0m  0.0010  0.0928\n",
      "     17       \u001b[36m57.8418\u001b[0m       \u001b[32m53.6849\u001b[0m  0.0010  0.0797\n",
      "     18       \u001b[36m56.5628\u001b[0m       \u001b[32m52.0810\u001b[0m  0.0010  0.0743\n",
      "     19       \u001b[36m55.6242\u001b[0m       \u001b[32m49.8632\u001b[0m  0.0010  0.0819\n",
      "     20       \u001b[36m53.7839\u001b[0m       \u001b[32m48.7777\u001b[0m  0.0010  0.0878\n",
      "     21       \u001b[36m53.2406\u001b[0m       \u001b[32m46.0751\u001b[0m  0.0010  0.0787\n",
      "     22       \u001b[36m49.5066\u001b[0m       46.0936  0.0010  0.0920\n",
      "     23       49.9333       \u001b[32m42.8342\u001b[0m  0.0010  0.0771\n",
      "     24       \u001b[36m48.1561\u001b[0m       43.6134  0.0010  0.0949\n",
      "     25       \u001b[36m46.7011\u001b[0m       \u001b[32m42.0426\u001b[0m  0.0010  0.0907\n",
      "     26       47.0397       \u001b[32m40.2726\u001b[0m  0.0010  0.0870\n",
      "     27       \u001b[36m45.9716\u001b[0m       \u001b[32m38.7841\u001b[0m  0.0010  0.0901\n",
      "     28       \u001b[36m43.8132\u001b[0m       \u001b[32m38.5716\u001b[0m  0.0010  0.0750\n",
      "     29       \u001b[36m43.7827\u001b[0m       \u001b[32m36.9755\u001b[0m  0.0010  0.0870\n",
      "     30       \u001b[36m43.2125\u001b[0m       37.5883  0.0010  0.0843\n",
      "     31       \u001b[36m41.3787\u001b[0m       \u001b[32m36.5505\u001b[0m  0.0010  0.0794\n",
      "     32       41.7147       \u001b[32m35.6906\u001b[0m  0.0010  0.0775\n",
      "     33       42.0776       \u001b[32m35.2654\u001b[0m  0.0010  0.0938\n",
      "     34       \u001b[36m40.6362\u001b[0m       \u001b[32m34.1729\u001b[0m  0.0010  0.0987\n",
      "     35       \u001b[36m39.0906\u001b[0m       \u001b[32m33.2292\u001b[0m  0.0010  0.0919\n",
      "     36       \u001b[36m39.0273\u001b[0m       \u001b[32m32.9284\u001b[0m  0.0010  0.0861\n",
      "     37       \u001b[36m38.5559\u001b[0m       \u001b[32m32.3237\u001b[0m  0.0010  0.0823\n",
      "     38       39.2386       32.5695  0.0010  0.0882\n",
      "     39       \u001b[36m38.3109\u001b[0m       \u001b[32m31.6148\u001b[0m  0.0010  0.0790\n",
      "     40       \u001b[36m37.7407\u001b[0m       31.9860  0.0010  0.0822\n",
      "     41       \u001b[36m36.9149\u001b[0m       \u001b[32m30.0468\u001b[0m  0.0010  0.0819\n",
      "     42       \u001b[36m36.6151\u001b[0m       30.0563  0.0010  0.0842\n",
      "     43       37.3279       \u001b[32m29.9757\u001b[0m  0.0010  0.0785\n",
      "     44       \u001b[36m36.4630\u001b[0m       \u001b[32m29.9541\u001b[0m  0.0010  0.0877\n",
      "     45       \u001b[36m35.4014\u001b[0m       \u001b[32m29.2054\u001b[0m  0.0010  0.1306\n",
      "     46       \u001b[36m35.0509\u001b[0m       \u001b[32m28.4452\u001b[0m  0.0010  0.0822\n",
      "     47       35.5461       \u001b[32m27.4027\u001b[0m  0.0010  0.0818\n",
      "     48       \u001b[36m35.0116\u001b[0m       27.5753  0.0010  0.0796\n",
      "     49       \u001b[36m34.4466\u001b[0m       \u001b[32m26.9079\u001b[0m  0.0010  0.0849\n",
      "     50       34.6369       \u001b[32m26.5661\u001b[0m  0.0010  0.0774\n",
      "     51       \u001b[36m33.8350\u001b[0m       27.0967  0.0010  0.0842\n",
      "     52       \u001b[36m33.1480\u001b[0m       \u001b[32m26.4088\u001b[0m  0.0010  0.0768\n",
      "     53       \u001b[36m32.9024\u001b[0m       \u001b[32m25.3684\u001b[0m  0.0010  0.0834\n",
      "     54       33.5331       26.4776  0.0010  0.0826\n",
      "     55       33.2957       25.4381  0.0010  0.0785\n",
      "     56       33.4557       25.6028  0.0010  0.0885\n",
      "     57       33.2455       \u001b[32m24.4325\u001b[0m  0.0010  0.0854\n",
      "     58       \u001b[36m32.5942\u001b[0m       25.3279  0.0010  0.0770\n",
      "     59       33.2536       25.2204  0.0010  0.0821\n",
      "     60       \u001b[36m31.5246\u001b[0m       \u001b[32m24.3119\u001b[0m  0.0010  0.0910\n",
      "     61       31.6059       24.5982  0.0010  0.0748\n",
      "     62       32.5043       24.8134  0.0010  0.0756\n",
      "     63       \u001b[36m31.2484\u001b[0m       \u001b[32m22.7708\u001b[0m  0.0010  0.0901\n",
      "     64       \u001b[36m29.9109\u001b[0m       23.4672  0.0010  0.0751\n",
      "     65       31.0488       23.6455  0.0010  0.0749\n",
      "     66       32.2985       23.5670  0.0010  0.0822\n",
      "     67       31.3520       22.9561  0.0010  0.0766\n",
      "     68       30.1231       \u001b[32m22.0886\u001b[0m  0.0005  0.0839\n",
      "     69       \u001b[36m29.6570\u001b[0m       22.8461  0.0005  0.0776\n",
      "     70       30.4230       22.7972  0.0005  0.0815\n",
      "     71       30.3282       22.7098  0.0005  0.0846\n",
      "     72       30.1843       22.5951  0.0005  0.0815\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHHCAYAAABZbpmkAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAYmxJREFUeJzt3Xl4FPXhx/H33rkvchEI942ACIgRFRUkIioq1qOoeP2sGKxHbZXW+8JqPWoFbK1C1SpKW1DxQERARVQEERREECQICeHKfWyyO78/JrtkSdAEsruwfF7PM8/uzszOfmeDycfvaTEMw0BEREQkQlnDXQARERGRYFLYERERkYimsCMiIiIRTWFHREREIprCjoiIiEQ0hR0RERGJaAo7IiIiEtEUdkRERCSiKeyIiIhIRFPYETlKXXnllXTq1CncxfhZTZXRYrFw7733/uJ77733XiwWS6uWZ/HixVgsFhYvXtyq1xWR4FLYETnMWCyWZm2Hwx/cvXv3YrfbefTRR7FYLNx5550HPHfDhg1YLBZuvfXWEJbw4EybNo2ZM2eGuxgBTj31VI455phwF0PkiGQPdwFEJNBLL70U8PrFF19kwYIFjfb37t37kD7nueeew+v1HtI15s+fj8Vi4brrrmPGjBm8+uqrPPjgg02e+8orrwBw2WWXHdJnVlVVYbcH91fXtGnTSE1N5corrwzYf8opp1BVVYXT6Qzq54tI61LYETnM7B8GPvvsMxYsWPCLIaGyspKYmJhmf47D4Tio8jX0zjvvMGzYMJKSkhg/fjx33XUXn332GSeccEKjc1999VV69erFcccdd0ifGRUVdUjvPxRWqzWsny8iB0fNWCJHIF+TxooVKzjllFOIiYnhj3/8IwBvvPEGY8aMISsrC5fLRdeuXXnggQfweDwB19i/P8yPP/6IxWLhL3/5C//4xz/o2rUrLpeLIUOGsHz58kZl8Hq9vPfee4wZMwaA8ePHA/tqcBpasWIF69ev95/T3DI2pak+O5988glDhgwhKiqKrl278ve//73J986YMYPTTz+d9PR0XC4Xffr0Yfr06QHndOrUiW+//ZYlS5b4mwxPPfVU4MB9dmbPns2gQYOIjo4mNTWVyy67jG3btgWcc+WVVxIXF8e2bds477zziIuLIy0tjdtuu61Z991c06ZNo2/fvrhcLrKyssjLy6O4uDjgnA0bNjBu3DgyMzOJioqiffv2XHLJJZSUlPjPWbBgASeddBJJSUnExcXRs2dP/78xkSONanZEjlC7d+9m9OjRXHLJJVx22WVkZGQAMHPmTOLi4rj11luJi4vjww8/5O6776a0tJTHHnvsF6/7yiuvUFZWxm9+8xssFguPPvooF1xwAZs2bQqoDVq+fDk7d+7krLPOAqBz586ceOKJvP766zz55JPYbLaAawL8+te/bpUyNrRmzRpGjRpFWloa9957L3V1ddxzzz3+76Oh6dOn07dvX84991zsdjtvvfUWN9xwA16vl7y8PACeeuopbrzxRuLi4vjTn/4E0OS1fGbOnMlVV13FkCFDmDJlCjt27OCvf/0rS5cu5auvviIpKcl/rsfjITc3l6FDh/KXv/yFDz74gMcff5yuXbsyceLEFt13U+69917uu+8+Ro4cycSJE1m/fj3Tp09n+fLlLF26FIfDgdvtJjc3l5qaGm688UYyMzPZtm0b8+bNo7i4mMTERL799lvOPvts+vfvz/3334/L5WLjxo0sXbr0kMsoEhaGiBzW8vLyjP3/Ux0+fLgBGM8++2yj8ysrKxvt+81vfmPExMQY1dXV/n0TJkwwOnbs6H+9efNmAzDatGlj7Nmzx7//jTfeMADjrbfeCrjmXXfdFfB+wzCMqVOnGoAxf/58/z6Px2O0a9fOyMnJOeQyGoZhAMY999zjf33eeecZUVFRxpYtW/z71q5da9hstkbfW1Ofm5uba3Tp0iVgX9++fY3hw4c3OnfRokUGYCxatMgwDMNwu91Genq6ccwxxxhVVVX+8+bNm2cAxt133x1wL4Bx//33B1xz4MCBxqBBgxp91v6GDx9u9O3b94DHi4qKDKfTaYwaNcrweDz+/c8884wBGC+88IJhGIbx1VdfGYAxe/bsA17rySefNABj586dv1gukSOBmrFEjlAul4urrrqq0f7o6Gj/87KyMnbt2sXJJ59MZWUl33333S9e9+KLLyY5Odn/+uSTTwZg06ZNAee98847/iashu91OBwBTVlLlixh27Zt/ias1iijj8fjYf78+Zx33nl06NDBv793797k5uY2Or/h55aUlLBr1y6GDx/Opk2bAppwmuvLL7+kqKiIG264IaAvz5gxY+jVqxdvv/12o/dcf/31Aa9PPvnkRt/twfjggw9wu93cfPPNWK37frX/3//9HwkJCf6yJCYmAmbn8srKyiav5auNeuONNw65E7vI4UBhR+QI1a5duyZHBX377becf/75JCYmkpCQQFpamr9zc3P+oDcMDYA/+Ozdu9e/r7CwkJUrVzYKO23atCE3N5c5c+ZQXV0NmE1Ydrudiy66qNXK6LNz506qqqro3r17o2M9e/ZstG/p0qWMHDmS2NhYkpKSSEtL8/dDOZiws2XLlgN+Vq9evfzHfaKiokhLSwvYl5ycHPDdHqwDlcXpdNKlSxf/8c6dO3Prrbfyz3/+k9TUVHJzc5k6dWrA/V988cUMGzaMa6+9loyMDC655BJef/11BR85YinsiByhGtZS+BQXFzN8+HC+/vpr7r//ft566y0WLFjAn//8Z4Bm/bFq2NemIcMw/M/fffddoqKiOO200xqdd9lll1FaWsq8efNwu93897//9fepaa0yHowffviBESNGsGvXLp544gnefvttFixYwC233BLUz23oQN9tqD3++OOsXr2aP/7xj1RVVfHb3/6Wvn378tNPPwHmv62PPvqIDz74gMsvv5zVq1dz8cUXc8YZZ7RqZ2qRUFEHZZEIsnjxYnbv3s3//vc/TjnlFP/+zZs3t+rnvP3225x22mlNBq5zzz2X+Ph4XnnlFRwOB3v37g1owmrNMqalpREdHc2GDRsaHVu/fn3A67feeouamhrefPPNgNqrRYsWNXpvc2de7tixo/+zTj/99Eaf7zseCg3L0qVLF/9+t9vN5s2bGTlyZMD5/fr1o1+/ftx55518+umnDBs2jGeffdY/T5LVamXEiBGMGDGCJ554gocffpg//elPLFq0qNG1RA53qtkRiSC+moOGtTBut5tp06a12mfU1tayYMGCRk1YPtHR0Zx//vm88847TJ8+ndjYWMaOHRuUMtpsNnJzc5k7dy75+fn+/evWrWP+/PmNzt3/c0tKSpgxY0aj68bGxjYart2UwYMHk56ezrPPPktNTY1//7vvvsu6desO+B0Fw8iRI3E6nTz99NMB9/j8889TUlLiL0tpaSl1dXUB7+3Xrx9Wq9V/D3v27Gl0/WOPPRYg4D5FjhSq2RGJICeeeCLJyclMmDCB3/72t1gsFl566aWAP36H6pNPPqG0tPRn/5BfdtllvPjii8yfP5/x48cTGxsbtDLed999vPfee5x88snccMMN1NXV8be//Y2+ffuyevVq/3mjRo3C6XRyzjnn8Jvf/Iby8nKee+450tPTKSgoCLjmoEGDmD59Og8++CDdunUjPT29Uc0NmBMz/vnPf+aqq65i+PDhXHrppf6h5506dfI3kbWWnTt3NjlDdefOnRk/fjyTJ0/mvvvu48wzz+Tcc89l/fr1TJs2jSFDhvj7RH344YdMmjSJX/3qV/To0YO6ujpeeuklbDYb48aNA+D+++/no48+YsyYMXTs2JGioiKmTZtG+/btOemkk1r1nkRCIowjwUSkGQ409PxAw5CXLl1qnHDCCUZ0dLSRlZVl/OEPfzDmz58fMGTaMA489Pyxxx5rdE0aDPe+7bbbjD59+vxsmevq6oy2bdsagPHOO++0Whn3L4vPkiVLjEGDBhlOp9Po0qWL8eyzzxr33HNPo+/tzTffNPr3729ERUUZnTp1Mv785z8bL7zwggEYmzdv9p9XWFhojBkzxoiPjzcA/zD0/Yee+7z22mvGwIEDDZfLZaSkpBjjx483fvrpp4BzJkyYYMTGxjb6LpoqZ1N80w00tY0YMcJ/3jPPPGP06tXLcDgcRkZGhjFx4kRj7969/uObNm0yrr76aqNr165GVFSUkZKSYpx22mnGBx984D9n4cKFxtixY42srCzD6XQaWVlZxqWXXmp8//33v1hOkcORxTBa8X/5RCTi9enTh7PPPptHH3003EUREWkWNWOJSLO53W4uvvjigGHkIiKHO9XsiIiISETTaCwRERGJaAo7IiIiEtEUdkRERCSiKeyIiIhIRNNoLMw1cbZv3058fHyzp4kXERGR8DIMg7KyMrKysrBaD1x/o7ADbN++nezs7HAXQ0RERA7C1q1bad++/QGPK+wA8fHxgPllJSQkhLk0IiIi0hylpaVkZ2f7/44fiMIO+1Y4TkhIUNgRERE5wvxSFxR1UBYREZGIprAjIiIiEU1hR0RERCKa+uyIiEhE83g81NbWhrsYchAcDgc2m+2Qr6OwIyIiEckwDAoLCykuLg53UeQQJCUlkZmZeUjz4CnsiIhIRPIFnfT0dGJiYjRp7BHGMAwqKyspKioCoG3btgd9LYUdERGJOB6Pxx902rRpE+7iyEGKjo4GoKioiPT09INu0lIHZRERiTi+PjoxMTFhLokcKt/P8FD6XSnsiIhIxFLT1ZGvNX6GCjsiIiIS0RR2REREIlynTp146qmnwn6NcFHYEREROUxYLJaf3e69996Duu7y5cu57rrrWrewRxCNxgqinWU1VLrryEiIIspx6JMiiYhIZCsoKPA/f+2117j77rtZv369f19cXJz/uWEYeDwe7PZf/lOelpbWugU9wqhmJ4jGTf+U4Y8t5tvtJeEuioiIHAEyMzP9W2JiIhaLxf/6u+++Iz4+nnfffZdBgwbhcrn45JNP+OGHHxg7diwZGRnExcUxZMgQPvjgg4Dr7t8EZbFY+Oc//8n5559PTEwM3bt3580332xRWfPz8xk7dixxcXEkJCRw0UUXsWPHDv/xr7/+mtNOO434+HgSEhIYNGgQX375JQBbtmzhnHPOITk5mdjYWPr27cs777xz8F/cL1DNThBFOcwsWV3rDXNJRETEMAyqaj0h/9xoh61VR4Xdcccd/OUvf6FLly4kJyezdetWzjrrLB566CFcLhcvvvgi55xzDuvXr6dDhw4HvM59993Ho48+ymOPPcbf/vY3xo8fz5YtW0hJSfnFMni9Xn/QWbJkCXV1deTl5XHxxRezePFiAMaPH8/AgQOZPn06NpuNVatW4XA4AMjLy8PtdvPRRx8RGxvL2rVrA2qtWpvCThD5mq6qw/Afl4iIBKqq9dDn7vkh/9y19+cS42y9P7f3338/Z5xxhv91SkoKAwYM8L9+4IEHmDNnDm+++SaTJk064HWuvPJKLr30UgAefvhhnn76ab744gvOPPPMXyzDwoULWbNmDZs3byY7OxuAF198kb59+7J8+XKGDBlCfn4+v//97+nVqxcA3bt3978/Pz+fcePG0a9fPwC6dOnSgm+g5dSMFURRdl/YUc2OiIi0jsGDBwe8Li8v57bbbqN3794kJSURFxfHunXryM/P/9nr9O/f3/88NjaWhIQE/9IMv2TdunVkZ2f7gw5Anz59SEpKYt26dQDceuutXHvttYwcOZJHHnmEH374wX/ub3/7Wx588EGGDRvGPffcw+rVq5v1uQdLNTtB5PI3Y6lmR0Qk3KIdNtbenxuWz21NsbGxAa9vu+02FixYwF/+8he6detGdHQ0F154IW63+2ev42tS8rFYLHi9rfc/5/feey+//vWvefvtt3n33Xe55557mDVrFueffz7XXnstubm5vP3227z//vtMmTKFxx9/nBtvvLHVPr8hhZ0g8jVj1dSpZkdEJNwsFkurNicdLpYuXcqVV17J+eefD5g1PT/++GNQP7N3795s3bqVrVu3+mt31q5dS3FxMX369PGf16NHD3r06MEtt9zCpZdeyowZM/zlzM7O5vrrr+f6669n8uTJPPfcc0ELO2rGCiL12RERkWDr3r07//vf/1i1ahVff/01v/71r1u1hqYpI0eOpF+/fowfP56VK1fyxRdfcMUVVzB8+HAGDx5MVVUVkyZNYvHixWzZsoWlS5eyfPlyevfuDcDNN9/M/Pnz2bx5MytXrmTRokX+Y8GgsBNEUfb6Zqw6hR0REQmOJ554guTkZE488UTOOecccnNzOe6444L6mRaLhTfeeIPk5GROOeUURo4cSZcuXXjttdcAsNls7N69myuuuIIePXpw0UUXMXr0aO677z7AXJU+Ly+P3r17c+aZZ9KjRw+mTZsWvPIahmEE7epHiNLSUhITEykpKSEhIaHVrnvX3G946bMt/HZEd249o0erXVdERH5edXU1mzdvpnPnzkRFRYW7OHIIfu5n2dy/36rZCSLfPDs1asYSEREJG4WdIHLZ1WdHREQk3BR2gkgzKIuIiISfwk4Q+UdjqYOyiIhI2CjsBJFLQ89FRETCTmEniPxDz9WMJSIiEjYKO0GkSQVFRETCT2EniPb12VHNjoiISLgo7ASR5tkREREJP4WdINJCoCIiEg6nnnoqN998s/91p06deOqpp372PRaLhblz5zb7mkcShZ0gitKkgiIi0gLnnHMOZ555ZpPHPv74YywWC6tXr27xdZcvX8511113qMU7YinsBNG+SQUVdkRE5Jddc801LFiwgJ9++qnRsRkzZjB48GD69+/f4uumpaURExPTGkU8IinsBNG+0VhqxhIRkV929tlnk5aWxsyZMwP2l5eXM3v2bK655hp2797NpZdeSrt27YiJiaFfv368+uqrP3vd/ZuxNmzYwCmnnEJUVBR9+vRhwYIFLS7r3r17ueKKK0hOTiYmJobRo0ezYcMG//EtW7ZwzjnnkJycTGxsLH379uWdd97xv3f8+PGkpaURHR1N9+7dmTFjRovL0Fz2oF1ZcPlqduo8GIaBxWIJc4lERI5ihgG1laH/XEcMNPP3v91u54orrmDmzJn86U9/8v/dmD17Nh6Ph0svvZTy8nIGDRrE7bffTkJCAm+//TaXX345Xbt25fjjj//Fz/B6vVxwwQVkZGTw+eefU1JSclB9ca688ko2bNjAm2++SUJCArfffjtnnXUWa9euxeFwkJeXh9vt5qOPPiI2Npa1a9cSFxcHwF133cXatWt59913SU1NZePGjVRVVbW4DM2lsBNEvpodwwC3x+tfGFRERMKgthIezgr95/5xOzhjm3361VdfzWOPPcaSJUs49dRTAbMJa9y4cSQmJpKYmMhtt93mP//GG29k/vz5vP76680KOx988AHfffcd8+fPJyvL/D4efvhhRo8e3ewy+kLO0qVLOfHEEwH497//TXZ2NnPnzuVXv/oV+fn5jBs3jn79+gHQpUsX//vz8/MZOHAggwcPBsyap2BSM1YQuez7vl41ZYmISHP06tWLE088kRdeeAGAjRs38vHHH3PNNdcA4PF4eOCBB+jXrx8pKSnExcUxf/588vPzm3X9devWkZ2d7Q86ADk5OS0q47p167Db7QwdOtS/r02bNvTs2ZN169YB8Nvf/pYHH3yQYcOGcc899wR0rJ44cSKzZs3i2GOP5Q9/+AOffvppiz6/pVSzE0ROmxWLxazZqan1QLQj3EUSETl6OWLMWpZwfG4LXXPNNdx4441MnTqVGTNm0LVrV4YPHw7AY489xl//+leeeuop+vXrR2xsLDfffDNut7u1S35Irr32WnJzc3n77bd5//33mTJlCo8//jg33ngjo0ePZsuWLbzzzjssWLCAESNGkJeXx1/+8peglEU1O0FksVgaDD9XzY6ISFhZLGZzUqi3g+ivedFFF2G1WnnllVd48cUXufrqq/39d5YuXcrYsWO57LLLGDBgAF26dOH7779v9rV79+7N1q1bKSgo8O/77LPPWlS+3r17U1dXx+eff+7ft3v3btavX0+fPn38+7Kzs7n++uv53//+x+9+9zuee+45/7G0tDQmTJjAyy+/zFNPPcU//vGPFpWhJRR2giyqQSdlERGR5oiLi+Piiy9m8uTJFBQUcOWVV/qPde/enQULFvDpp5+ybt06fvOb37Bjx45mX3vkyJH06NGDCRMm8PXXX/Pxxx/zpz/9qUXl6969O2PHjuX//u//+OSTT/j666+57LLLaNeuHWPHjgXg5ptvZv78+WzevJmVK1eyaNEievfuDcDdd9/NG2+8wcaNG/n222+ZN2+e/1gwKOwEmRYDFRGRg3HNNdewd+9ecnNzA/rX3HnnnRx33HHk5uZy6qmnkpmZyXnnndfs61qtVubMmUNVVRXHH3881157LQ899FCLyzdjxgwGDRrE2WefTU5ODoZh8M477+BwmF02PB4PeXl59O7dmzPPPJMePXowbdo0AJxOJ5MnT6Z///6ccsop2Gw2Zs2a1eIyNJfFMAwjaFdvgUceeYTJkydz0003+ecCqK6u5ne/+x2zZs2ipqaG3Nxcpk2bRkZGhv99+fn5TJw4kUWLFhEXF8eECROYMmUKdnvzuyOVlpaSmJhISUkJCQkJrXpfp/1lMZt3VfD6b3I4vnNKq15bRESaVl1dzebNm+ncuTNRUVHhLo4cgp/7WTb37/dhUbOzfPly/v73vzeaFfKWW27hrbfeYvbs2SxZsoTt27dzwQUX+I97PB7GjBmD2+3m008/5V//+hczZ87k7rvvDvUtHJBvRJZqdkRERMIj7GGnvLyc8ePH89xzz5GcnOzfX1JSwvPPP88TTzzB6aefzqBBg5gxYwaffvqpvyPV+++/z9q1a3n55Zc59thjGT16NA888ABTp049bHqlazFQERGR8Ap72MnLy2PMmDGMHDkyYP+KFSuora0N2N+rVy86dOjAsmXLAFi2bBn9+vULaNbKzc2ltLSUb7/9NjQ38Au0PpaIiEh4hXWenVmzZrFy5UqWL1/e6FhhYSFOp5OkpKSA/RkZGRQWFvrPaRh0fMd9xw6kpqaGmpoa/+vS0tKDvYVfpA7KIiIi4RW2mp2tW7dy00038e9//zvkncemTJnin3I7MTGR7OzsoH2Wf54dNWOJiITcYTIGRw5Ba/wMwxZ2VqxYQVFREccddxx2ux273c6SJUt4+umnsdvtZGRk4Ha7KS4uDnjfjh07yMzMBCAzM7PR3AK+175zmjJ58mRKSkr829atW1v35hrwNWPVqGZHRCRkfMOfKyvDsPCntCrfz9D3Mz0YYWvGGjFiBGvWrAnYd9VVV9GrVy9uv/12srOzcTgcLFy4kHHjxgGwfv168vPz/Wt45OTk8NBDD1FUVER6ejoACxYsICEhIWAGx/25XC5cLleQ7iyQmrFERELPZrORlJREUVERADExMf4ZiOXIYBgGlZWVFBUVkZSUhM128Itphy3sxMfHc8wxxwTsi42NpU2bNv7911xzDbfeeispKSkkJCRw4403kpOTwwknnADAqFGj6NOnD5dffjmPPvoohYWF3HnnneTl5YUszPySfUPP1YwlIhJKvhp+X+CRI1NSUtLPttY0x2G9EOiTTz6J1Wpl3LhxAZMK+thsNubNm8fEiRPJyckhNjaWCRMmcP/994ex1IFUsyMiEh4Wi4W2bduSnp5ObW1tuIsjB8HhcBxSjY7PYTODcjgFcwblJxZ8z9MLN3DZCR148Lx+rXptERGRo9kRNYNyJNs3z46asURERMJBYSfI/EPP1YwlIiISFgo7Qbavz45qdkRERMJBYSfI/PPs1KlmR0REJBwUdoJMo7FERETCS2EnyPbV7KgZS0REJBwUdoJMHZRFRETCS2EnyFzqoCwiIhJWCjtBtm+eHdXsiIiIhIPCTpCpg7KIiEh4KewEmT/sqIOyiIhIWCjsBJlv1XN3nRev96hfhkxERCTkFHaCzFezAxp+LiIiEg4KO0EWZd/3FavfjoiISOgp7ASZ3WbFbrUAUK0lI0REREJOYScEtBioiIhI+CjshIDm2hEREQkfhZ0QcGnJCBERkbBR2AkBLQYqIiISPgo7IaBZlEVERMJHYScE1EFZREQkfBR2QmBfM5ZqdkREREJNYScEotRBWUREJGwUdkJAzVgiIiLho7ATAr7FQFWzIyIiEnoKOyHgUs2OiIhI2CjshIB/BmV1UBYREQk5hZ0Q0Dw7IiIi4aOwEwL7RmOpGUtERCTUFHZCwD/Pjmp2REREQk5hJwT8zVjqsyMiIhJyCjshsK9mR81YIiIioaawEwKq2REREQkfhZ0QcKmDsoiISNgo7ISAf54ddVAWEREJOYWdENA8OyIiIuGjsBMCWghUREQkfBR2QsA/GksdlEVEREJOYScE1EFZREQkfBR2QkAdlEVERMJHYScEfGtj1XkN6jyq3REREQklhZ0Q8HVQBqiuU9gREREJJYWdEHDZ933NasoSEREJLYWdELBaLTjt6rcjIiISDgo7IRJl9w0/VzOWiIhIKCnshIhmURYREQkPhZ0Q0SzKIiIi4aGwEyL+WZRVsyMiIhJSCjsh4q/Z0ZIRIiIiIaWwEyJRWjJCREQkLBR2QsSlJSNERETCQmEnRNRBWUREJDwUdkLEpUkFRUREwkJhJ0TUQVlERCQ8FHZCJMrfZ0fNWCIiIqFkD3cBIlrlHqgpg7gM/2gszbMjIiISWgo7wfSPU6F4C1zzAVGOBEB9dkREREJNzVjB5Iw1H93lasYSEREJE4WdYPKFndpKfwflGnVQFhERCSmFnWByxJiP7gpcmmdHREQkLBR2gskZZz66K4jyzbOjmh0REZGQUtgJJue+mp19Mygr7IiIiISSwk4wNdFnR81YIiIioaWwE0yOpkZjqWZHREQklBR2gsnfjNVwNJZqdkREREJJYSeYGjRjaSFQERGR8FDYCaaAZix1UBYREQkHhZ1g8s+gXOlfG0sdlEVEREJLYSeYAoae75tnxzCMMBZKRETk6KKwE0y+SQVr982gbBjg9qh2R0REJFTCGnamT59O//79SUhIICEhgZycHN59913/8erqavLy8mjTpg1xcXGMGzeOHTt2BFwjPz+fMWPGEBMTQ3p6Or///e+pq6sL9a00zdG4ZgfUlCUiIhJKYQ077du355FHHmHFihV8+eWXnH766YwdO5Zvv/0WgFtuuYW33nqL2bNns2TJErZv384FF1zgf7/H42HMmDG43W4+/fRT/vWvfzFz5kzuvvvucN1SoAZ9dpw2KxaL+VKLgYqIiISOxTjMOpCkpKTw2GOPceGFF5KWlsYrr7zChRdeCMB3331H7969WbZsGSeccALvvvsuZ599Ntu3bycjIwOAZ599lttvv52dO3fidDqb9ZmlpaUkJiZSUlJCQkJC693Mrg3wzGBwJcLkfHrf9R5VtR4+/sNpZKfEtN7niIiIHIWa+/f7sOmz4/F4mDVrFhUVFeTk5LBixQpqa2sZOXKk/5xevXrRoUMHli1bBsCyZcvo16+fP+gA5ObmUlpa6q8dakpNTQ2lpaUBW1D459mpAMPQLMoiIiJhEPaws2bNGuLi4nC5XFx//fXMmTOHPn36UFhYiNPpJCkpKeD8jIwMCgsLASgsLAwIOr7jvmMHMmXKFBITE/1bdnZ2696Uj6/PjrcOPG6tjyUiIhIGYQ87PXv2ZNWqVXz++edMnDiRCRMmsHbt2qB+5uTJkykpKfFvW7duDc4H+Wp2IHDlc/XZERERCRl7uAvgdDrp1q0bAIMGDWL58uX89a9/5eKLL8btdlNcXBxQu7Njxw4yMzMByMzM5Isvvgi4nm+0lu+cprhcLlwuVyvfSRNsDrA5weMGd4WWjBAREQmDsNfs7M/r9VJTU8OgQYNwOBwsXLjQf2z9+vXk5+eTk5MDQE5ODmvWrKGoqMh/zoIFC0hISKBPnz4hL3uTGqyPpWYsERGR0Atrzc7kyZMZPXo0HTp0oKysjFdeeYXFixczf/58EhMTueaaa7j11ltJSUkhISGBG2+8kZycHE444QQARo0aRZ8+fbj88st59NFHKSws5M477yQvLy80NTfN4YiFqr3162OpZkdERCTUwhp2ioqKuOKKKygoKCAxMZH+/fszf/58zjjjDACefPJJrFYr48aNo6amhtzcXKZNm+Z/v81mY968eUycOJGcnBxiY2OZMGEC999/f7huqbEGc+247GYAU9gREREJnbCGneeff/5nj0dFRTF16lSmTp16wHM6duzIO++809pFaz0B62NFA1Bdp2YsERGRUDns+uxEnAbrY/n67NSoZkdERCRkFHaCzb8+ViVRdl8HZYUdERGRUFHYCTZ/n52KBh2U1YwlIiISKgo7webrs9OwGUuTCoqIiISMwk6wOfbV7Lg0z46IiEjIKewEW4Oh55pnR0REJPQUdoLNP/S8fF8HZQ09FxERCRmFnWDzDz1vuFyEanZERERCRWEn2BwNJxVUM5aIiEioKewEW8DQc9+kgmrGEhERCRWFnWBrap4dDT0XEREJGYWdYPOFndpKXJpBWUREJOQUdoLNoRmURUREwklhJ9gaNGOpZkdERCT0FHaCzdlwNJbCjoiISKgp7ASbb54dTw1RNrP5SpMKioiIhI7CTrD55tkBoqkBwF3nxTCMcJVIRETkqNLisFNVVUVlZaX/9ZYtW3jqqad4//33W7VgEcPuAovZfBVlVPt316h2R0REJCRaHHbGjh3Liy++CEBxcTFDhw7l8ccfZ+zYsUyfPr3VC3jEs1j8nZRd3n1hR/12REREQqPFYWflypWcfPLJAPznP/8hIyODLVu28OKLL/L000+3egEjQn3YsXsqsVstgIafi4iIhEqLw05lZSXx8fEAvP/++1xwwQVYrVZOOOEEtmzZ0uoFjAj+9bG0GKiIiEiotTjsdOvWjblz57J161bmz5/PqFGjACgqKiIhIaHVCxgRtGSEiIhI2LQ47Nx9993cdtttdOrUiaFDh5KTkwOYtTwDBw5s9QJGBP+SEQ0nFlQzloiISCjYW/qGCy+8kJNOOomCggIGDBjg3z9ixAjOP//8Vi1cxAio2UkE1IwlIiISKi0OOwCZmZlkZmYCUFpayocffkjPnj3p1atXqxYuYjg0i7KIiEi4tLgZ66KLLuKZZ54BzDl3Bg8ezEUXXUT//v3573//2+oFjAi+WZTdFbjsWgxUREQklFocdj766CP/0PM5c+ZgGAbFxcU8/fTTPPjgg61ewIjgWx+rdt9orBp1UBYREQmJFoedkpISUlJSAHjvvfcYN24cMTExjBkzhg0bNrR6ASOCmrFERETCpsVhJzs7m2XLllFRUcF7773nH3q+d+9eoqKiWr2AEaFBM5Z/6LmasUREREKixR2Ub775ZsaPH09cXBwdO3bk1FNPBczmrX79+rV2+SKDs0HNjl01OyIiIqHU4rBzww03cPzxx7N161bOOOMMrFazpqJLly7qs3Mg/nl2KnE5fX12VLMjIiISCgc19Hzw4MEMHjwYwzAwDAOLxcKYMWNau2yRw+GbZ6ecqFhfM5ZqdkREREKhxX12AF588UX69etHdHQ00dHR9O/fn5deeqm1yxY5/JMKNlwbSzU7IiIiodDimp0nnniCu+66i0mTJjFs2DAAPvnkE66//np27drFLbfc0uqFPOI11WdHQ89FRERCosVh529/+xvTp0/niiuu8O8799xz6du3L/fee6/CTlN8o7FqG47GUtgREREJhRY3YxUUFHDiiSc22n/iiSdSUFDQKoWKOP55dhpMKqhmLBERkZBocdjp1q0br7/+eqP9r732Gt27d2+VQkWcgIVAVbMjIiISSi1uxrrvvvu4+OKL+eijj/x9dpYuXcrChQubDEFCwNDzKLsFUJ8dERGRUGlxzc64ceP4/PPPSU1NZe7cucydO5fU1FS++OILzj///GCU8cjnCzsYxFjcgEZjiYiIhMpBzbMzaNAgXn755YB9RUVFPPzww/zxj39slYJFFHu0/2m0P+yoZkdERCQUDmqenaYUFBRw1113tdblIovV6p9YMMaoBhR2REREQqXVwo78gvq5dqLxhR01Y4mIiISCwk6o1Pfbia6v2alRB2UREZGQUNgJlfpmLFd9zY7m2REREQmNZndQvvXWW3/2+M6dOw+5MBGtvmbH5a0CXBp6LiIiEiLNDjtfffXVL55zyimnHFJhIlp9nx2XtxpwUesx8HgNbFZLeMslIiIS4ZoddhYtWhTMckS++vWxHJ4qIBEwR2TFug5q9L+IiIg0k/rshEr9+lh2T6V/l4afi4iIBJ/CTqjU99mx1lbitNevj1WnTsoiIiLBprATKv71sSqIsmsxUBERkVBR2AmV+mYs3JVEOWyAwo6IiEgoKOyEiq9mx13RIOyoGUtERCTYmh12Hn30Uaqqqvyvly5dSk1Njf91WVkZN9xwQ+uWLpI0aMZy1Tdj1ahmR0REJOiaHXYmT55MWVmZ//Xo0aPZtm2b/3VlZSV///vfW7d0kaSpmh1NLCgiIhJ0zQ47hmH87Gv5BQF9dnwdlNWMJSIiEmzqsxMq9ZMK4i5XB2UREZEQUtgJlfrlIqitxGU3w06N5tkREREJuhatVfDPf/6TuDizhqKuro6ZM2eSmpoKENCfR5oQ0GdH8+yIiIiESrPDTocOHXjuuef8rzMzM3nppZcanSMH4PCFnUoNPRcREQmhZoedH3/8MYjFOAr4a3bKiamv2amoqQtjgURERI4O6rMTKr4+O4aHtvFmzU5haXUYCyQiInJ0aHbYWbZsGfPmzQvY9+KLL9K5c2fS09O57rrrAiYZlP34mrGADnFm89X24qoDnS0iIiKtpNlh5/777+fbb7/1v16zZg3XXHMNI0eO5I477uCtt95iypQpQSlkRLDZweYCICtaYUdERCRUmh12Vq1axYgRI/yvZ82axdChQ3nuuee49dZbefrpp3n99deDUsiIUd9vp22MOQpre3E1Xq8mZxQREQmmZoedvXv3kpGR4X+9ZMkSRo8e7X89ZMgQtm7d2rqlizT1YSfV5cFqAbfHy64KNf2JiIgEU7PDTkZGBps3bwbA7XazcuVKTjjhBP/xsrIyHA5H65cwktSHHXtdJRkJUYBZuyMiIiLB0+ywc9ZZZ3HHHXfw8ccfM3nyZGJiYjj55JP9x1evXk3Xrl2DUsiI0WB9rKykaED9dkRERIKt2WHngQcewG63M3z4cJ577jmee+45nE6n//gLL7zAqFGjglLIiOGba6e2gnb1YWfbXoUdERGRYGr2pIKpqal89NFHlJSUEBcXh81mCzg+e/Zs/1IScgANlozw1exsU82OiIhIULV4UsHExMRGQQcgJSUloKanOaZMmcKQIUOIj48nPT2d8847j/Xr1wecU11dTV5eHm3atCEuLo5x48axY8eOgHPy8/MZM2YMMTExpKen8/vf/566usNwdmLnviUj2iX5+uwo7IiIiARTs2t2rr766mad98ILLzT7w5csWUJeXh5Dhgyhrq6OP/7xj4waNYq1a9cSG2sGg1tuuYW3336b2bNnk5iYyKRJk7jgggtYunQpAB6PhzFjxpCZmcmnn35KQUEBV1xxBQ6Hg4cffrjZZQkJf5+dcrLSVLMjIiISCs0OOzNnzqRjx44MHDgQw2iduWHee++9Rp+Rnp7OihUrOOWUUygpKeH555/nlVde4fTTTwdgxowZ9O7dm88++4wTTjiB999/n7Vr1/LBBx+QkZHBscceywMPPMDtt9/Ovffe2+LapqDy99mppF2yOiiLiIiEQrPDzsSJE3n11VfZvHkzV111FZdddhkpKSmtWpiSkhIA/3VXrFhBbW0tI0eO9J/Tq1cvOnTowLJlyzjhhBNYtmwZ/fr1C5gDKDc3l4kTJ/Ltt98ycODARp9TU1MTsLRFaWlpq97HATXRZ2dvZS2V7jpinM3+UYiIiEgLNLvPztSpUykoKOAPf/gDb731FtnZ2Vx00UXMnz+/VWp6vF4vN998M8OGDeOYY44BoLCwEKfTSVJSUsC5GRkZFBYW+s9pGHR8x33HmjJlyhQSExP9W3Z29iGXv1n8zVgVJEQ5iHeZAUdz7YiIiARPizoou1wuLr30UhYsWMDatWvp27cvN9xwA506daK8vPyQCpKXl8c333zDrFmzDuk6zTF58mRKSkr8W8hmfnbWj1ZzVwBoRJaIiEgItHg0lv+NVisWiwXDMPB4PIdUiEmTJjFv3jwWLVpE+/bt/fszMzNxu90UFxcHnL9jxw4yMzP95+w/Osv32nfO/lwuFwkJCQFbSDjra3ZqKwHUb0dERCQEWhR2ampqePXVVznjjDPo0aMHa9as4ZlnniE/P/+g5tgxDINJkyYxZ84cPvzwQzp37hxwfNCgQTgcDhYuXOjft379evLz88nJyQEgJyeHNWvWUFRU5D9nwYIFJCQk0KdPnxaXKaga9NkByNLwcxERkaBrdq/YG264gVmzZpGdnc3VV1/Nq6++Smpq6iF9eF5eHq+88gpvvPEG8fHx/j42iYmJREdHk5iYyDXXXMOtt95KSkoKCQkJ3HjjjeTk5PjX5Ro1ahR9+vTh8ssv59FHH6WwsJA777yTvLw8XC7XIZWv1Tn2DztqxhIREQm2ZoedZ599lg4dOtClSxeWLFnCkiVLmjzvf//7X7M/fPr06QCceuqpAftnzJjBlVdeCcCTTz6J1Wpl3Lhx1NTUkJuby7Rp0/zn2mw25s2bx8SJE8nJySE2NpYJEyZw//33N7scIbNfzY6WjBAREQm+ZoedK664AovF0qof3pxRXFFRUUydOpWpU6ce8JyOHTvyzjvvtGbRgmP/Pju+xUBLFHZERESCpUWTCsoh8o/GMkeu+ZqxCkuq8XgNbNbWDZMiIiJyCKOx5CD459kxa3bS413YrBZqPQY7y2p+5o0iIiJysBR2QsnXZ8dTA5467DYrmQnmiCx1UhYREQkOhZ1Q8oUdgNrATsoafi4iIhIcCjuhZHOCtb6bVH1TlubaERERCS6FnVCyWDTXjoiISIgp7ISarymrNjDsqGZHREQkOBR2Qs25b+Vz2Lc+1jatfC4iIhIUCjuh5p9Feb+JBVWzIyIiEhQKO6Hm77NjTizYNtHsoFxSVUt5TV24SiUiIhKxFHZCzd9nx6zZiY9ykBBljtBS7Y6IiEjrU9gJtf367AC0Szb3aUSWiIhI61PYCTX/+lgNwk79XDta/VxERKT1KeyEmqNxzY6Gn4uIiASPwk6o+Zqx6vvsgMKOiIhIMCnshJq/Gavcv2vf8HPNtSMiItLaFHZCzd+M1bhmRx2URUREWp/CTqg5A9fGgn01O4Wl1dR5vOEolYiISMRS2Am1/dbGAkiLd2G3WvB4DYrKasJUMBERkciksBNqTdTs2KwW2tYPP1cnZRERkdalsBNqTfTZAchKVL8dERGRYFDYCTXfaKwGzViwr9+Owo6IiEjrUtgJtSaWiwDNtSMiIhIsCjuh5u+zE9iM1S65vmZHS0aIiIi0KoWdUHM0GI3l3TfMPEsTC4qIiASFwk6o+Wp2AOr21eK002gsERGRoFDYCTVHNGAxnzexGGhZTR2l1bVhKJiIiEhkUtgJNYulybl2Ypx2kmMcgPrtiIiItCaFnXBwaESWiIhIqCjshIN/yYj9JhZU2BEREWl1Cjvh4G/GKg/YvW9iQY3IEhERaS0KO+FwoLl2NIuyiIhIq1PYCYcD9NlpXz+x4HcFpaEukYiISMRS2AkHZ4OJBRs4sWsqDpuFDUXlfFeowCMiItIaFHbCoYmh5wCJMQ5O7ZkOwBurtoe6VCIiIhFJYSccfCuflxc1OnTese0AeHPVdrxeI5SlEhERiUgKO+HQ8UTz8ds5AetjAYzonU6cy8624ipW5O8NQ+FEREQii8JOOPQaA1GJULIVNi8JOBTlsJHbNxOAuV9tC0fpREREIorCTjg4oqHfr8znX73c6PB5A7MAeHtNAe46b6PjIiIi0nwKO+Ey8DLzcd1bUBXYXJXTpQ2pcS6KK2v5eMPOMBROREQkcijshEvbYyHjGPDUwDf/DThkt1k5Z0BbAOZqVJaIiMghUdgJF4tlX+1OU01Z9aOyFqwtpKKmLpQlExERiSgKO+HU7yKwOmD7V1D4TcCh/u0T6dQmhupaL++vLQxTAUVERI58CjvhFNsGeo42n6/6d8Ahi8XC2PranblfqSlLRETkYCnshNvAy83Hr2dBnTvg0NhjzVFZn2zcxa7ymlCXTEREJCIo7IRb19Mhvi1U7YHv3w041CUtjv7tE/F4Dd5eXRCmAoqIiBzZFHbCzWaHAZeaz5voqOxrynpjlSYYFBERORgKO4cD36isjR9AaWD/nHP6t8VqgZX5xeTvrgxD4URERI5sCjuHgzZdocOJYHjh61cDDqUnRHFi11RAtTsiIiIHQ2HncNFwzh0jcLVzX0fluau2YRhaCV1ERKQlFHYOF33GgiMW9myC/GUBh3KPySTaYeOHnRV8tGFXmAooIiJyZFLYOVy44uCY883nK2YGHEqIcvDroR0A+NvCDardERERaQGFncPJ4KvNx9Wvw7aVAYeuO6ULTpuVL7fs5fPNe8JQOBERkSOTws7hpN0gcwkJDHjn9+D1+g9lJERx0ZD2ADzz4cYwFVBEROTIo7BzuBn1ADjjYduXsCpw3p3rh3fFbrXwycZdrMzfG6YCioiIHFkUdg438Zlw6h3m8w/uhcp9TVbtk2O44DhzkkHV7oiIiDSPws7haOhvIK0XVO6GRQ8FHJp4ajesFvjwuyK+2VYSpgKKiIgcORR2Dkc2B5z1mPn8yxeg4Gv/oc6psZwzwJx3Z+oi1e6IiIj8EoWdw1XnU6DvBeasym/fFtBZOe+0bgC8+00h3+8oC1cJRUREjggKO4ezUQ+aEw3+9AWsnuXf3SMjnjP7ZgIwTbU7IiIiP0th53CW2A6G/958vuBuqCr2H5p0ulm78+bX2/lxV0UYCiciInJkUNg53J2QB226Q8VOWDzFv/uYdomc3isdrwHTF/8QxgKKiIgc3hR2Dnd2J5z1qPn8i3/Aj5/4D/n67vx35U/k764MR+lEREQOewo7R4Kup8Oxl5mdlf97LVTsBmBQx2RO7p5KndfgD//9Gq9Xa2aJiIjsT2HnSHHWo2ZzVlkBzJ0I9YuBPnjeMUQ7bHy2aQ8vLvsxvGUUERE5DCnsHCmcsfCrGWBzwYb58Nl0ADq2ieWPZ/UC4JH3vmPTzvJwllJEROSwo7BzJMnsB7n1MyovuBu2fwXA+KEdGdatDdW1Xm6b/TUeNWeJiIj4KewcaYZcC73OBm8tzL4KqkuxWi08euEA4lx2VuYX88+PN4W7lCIiIocNhZ0jjcUCY5+BxGzYuxnevhUMg3ZJ0dx9dh8AHn//e82sLCIiUi+sYeejjz7inHPOISsrC4vFwty5cwOOG4bB3XffTdu2bYmOjmbkyJFs2LAh4Jw9e/Ywfvx4EhISSEpK4pprrqG8PML7rUQnw7jnwWKDNbNh1SsA/Gpwe07rmYbb4+V3r39Nrcf7CxcSERGJfGENOxUVFQwYMICpU6c2efzRRx/l6aef5tlnn+Xzzz8nNjaW3Nxcqqur/eeMHz+eb7/9lgULFjBv3jw++ugjrrvuulDdQvh0GAqn/8l8/s5tsP0rLBYLj4zrT2K0gzXbSpi2SJMNioiIWAzDOCx6s1osFubMmcN5550HmLU6WVlZ/O53v+O2224DoKSkhIyMDGbOnMkll1zCunXr6NOnD8uXL2fw4MEAvPfee5x11ln89NNPZGVlNeuzS0tLSUxMpKSkhISEhKDcX1B4vfDvcfDDh2Ztz5XvQEYf3li1jZtmrcJutfDfiScyIDsp3CUVERFpdc39+33Y9tnZvHkzhYWFjBw50r8vMTGRoUOHsmzZMgCWLVtGUlKSP+gAjBw5EqvVyueff37Aa9fU1FBaWhqwHZGsVvjVv6DdIKjaCy+OhV0bOXdAFqOPyaTOazBhxhd8s60k3CUVEREJm8M27BQWFgKQkZERsD8jI8N/rLCwkPT09IDjdrudlJQU/zlNmTJlComJif4tOzu7lUsfQlEJcNl/zWHpFUXw4rlYirfw5wv7c2x2EsWVtfz6uc9YtbU43CUVEREJi8M27ATT5MmTKSkp8W9bt24Nd5EOTXQyXD4XUntC6Tb417kk1BTx0jXHM7hjMqXVdVz2z89ZsWVPuEsqIiIScodt2MnMzARgx44dAft37NjhP5aZmUlRUVHA8bq6Ovbs2eM/pykul4uEhISA7YgXmwoT3oSULlC8BV48l/i6vfzr6uMZ2jmF8po6rnj+Cz7ftDvcJRUREQmpwzbsdO7cmczMTBYuXOjfV1payueff05OTg4AOTk5FBcXs2LFCv85H374IV6vl6FDh4a8zGEXnwlXvGnOwbN7I7w4lti6EmZedTwndUulwu3hyhnL+XTjrnCXVEREJGTCGnbKy8tZtWoVq1atAsxOyatWrSI/Px+LxcLNN9/Mgw8+yJtvvsmaNWu44ooryMrK8o/Y6t27N2eeeSb/93//xxdffMHSpUuZNGkSl1xySbNHYkWcpGyzhicuE4rWwswxRFcX8c8JgxneI42qWg9XzVzOovVFv3wtERGRCBDWoeeLFy/mtNNOa7R/woQJzJw5E8MwuOeee/jHP/5BcXExJ510EtOmTaNHjx7+c/fs2cOkSZN46623sFqtjBs3jqeffpq4uLhml+OIHXr+c3Z+b47OKtsOSR3g8rnUJHYi798r+WBdEVYLTB7dm2tP7ozFYgl3aUVERFqsuX+/D5t5dsIpIsMOwN4t8NJ5sGcTxKbD5XNwp/bhT3PWMHvFTwCcOyCLP4/rT7TTFt6yioiItNARP8+OtILkjnD1fMioH5Y+8yyc25fz6IX9uX9sX+xWC29+vZ0Lpn/K1j2V4S6tiIhIUCjsRLq4dLhyHmSfANUl8NJ5WH5YyBU5nfj3tUNJjXOyrqCUc575hE82qOOyiIhEHoWdo0F0Elw+B7qdAbWV8MolsHo2Q7u04c1JJ9G/fSLFlbVc8cLnTF20kepaT7hLLCIi0moUdo4Wzhi45BXoewF4a+F/18JbN5EVC6//Jodxx7XHa8Bj89dz6mOLeXHZjwo9IiISEdRBmQjuoNwUrwcWPQQfPwEYkN4HLpyBkdaT2V/+xBMLvqew1FxVPjMhihtO68pFg7OJcqgDs4iIHF40GqsFjqqw4/PDIvjfdWbHZXs0nPUoDLycGo+X15dvZeqiHxqFnkuGdMBpV2WgiIgcHhR2WuCoDDsA5UUw5zfww4fm62MuhLOfhKgEauo8jUJPl9RY7jq7D6f1Sv+Zi4qIiISGwk4LHLVhB8DrhU//CgsfAMNjTkB41uPQYxQANXUeXlu+lacXbmBXuRuA4T3SuOvs3nRLjw9nyUVE5CinsNMCR3XY8dn6BfznGijJN1/3OhvOfMRcfgIora7lmQ83MmPpZmo9BjarhctP6MgtI3uQGOMIY8FFRORopbDTAgo79WrKYcmf4bNp4K0DRwyc8nvImQR2JwCbd1Xw0Nvr+GCduRp9UoyDicO7Mv6EjsS57OEsvYiIHGUUdlpAYWc/Revg7d/BlqXm69QeMOZx6HyK/5SPN+zkgXlr+X5HOWCGnquHdWbCiZ1IjFZNj4iIBJ/CTgso7DTBMGD1a/D+nVCx09zX62wYeR+kdgOgzuPlf19tY/riH9i8qwKAeJedK07syNXDOtMmzhWu0ouIyFFAYacFFHZ+RlUxfPggfPk8GF6w2mHwNTD8dohtA4DHa/D2mgKmfriR9TvKAIh22LjixI7ccGo31fSIiEhQKOy0gMJOMxR9Bwvuhg3zzdeuBDj5dzD0enBEAeD1GixYt4NnPtzImm0lACTHOLhpRHfGn9ARh01z9IiISOtR2GkBhZ0W2LTYbNoqXGO+TuwAI++BY8aBxQKAYRh8+F0RU979jo1FZp+ezqmx3DG6F6P6ZGCpP09ERORQKOy0gMJOC3m9sHqWOTdP2XZzX/shkPswZB/vP63O4+W1L7fy5ILv/XP0HN85hcmjezGwQ3I4Si4iIhFEYacFFHYOkrsSlj0DnzwFtWYHZfqeDyPvheRO/tPKqmv5+5JNPPfxJmrqvAAc1yGJK3I6MbpfJi671t0SEZGWU9hpAYWdQ1RWaHZi/uplwACbE06YCMNuhpgU/2nbi6t4YsH3zP1qG3Ve859dapyTS4Z04NdDO5CVFB2e8ouIyBFJYacFFHZaSeEamP8n2LzEfG2xQccToceZ0HM0tOkKQFFZNbO+2Morn+f7192yWS2M6JVObt9MTumRRlq8hq2LiMjPU9hpAYWdVmQYsOF9+PCBfZ2Yfdp0hx65ZlNX+8HUerwsWLuDF5f9yGeb9gSc2q9dIqf2TOPUnukcm52EzapOzSIiEkhhpwUUdoJkz2b4fj58/y78uBS8tfuO9RwDZ9zvn6Dw+x1lvLlqO4u/L+KbbaUBl0mKcTC8Rxqn90rn1B7pWotLREQAhZ0WUdgJgepS+OFD+G4efPM/c4V13wSFp94R0LenqKyaJet3svj7nXz8/U5Kq+v8x2xWC4M6JjOiVzojeqfTNS1OQ9lFRI5SCjstoLATYjvXw/t37ZugMCrRXHD0+OvAHthXp87j5autxXz4XREfrivyz9Ds07FNDGf2zWRU30wGZidhVXOXiMhRQ2GnBRR2wuSHReYEhTu+MV8ndYC+F0D3UZA9FGyNV1HfuqeSReuLWLiuiGU/7Mbt8fqPpce7OKNPBmcek8kJXdpoxmYRkQinsNMCCjth5PXAqlfMoevlhfv2uxKh2+lm8Ol2BsSlNXprRU0dS77fyXvfFLLouyLKavY1dyVGOzirX1vGHpvF8Z1SVOMjIhKBFHZaQGHnMOCugO/eMUdybfwAqgJHZ5HUEdJ7m1ta/WNqD/+6XO46L5/+sIv53+5gwdod7Cqv8b+1bWIU5w7IYuyx7ejdNl59fEREIoTCTgso7BxmvB7YttLs07PhfSj4uunzLFZzmYqBl5nD2V3xgLkK++ebdvPGqu28800BZQ06OHdJi2VA+yS6pcfRPT2O7hnxdEiJ0dB2EZEjkMJOCyjsHOYq90DRWihat2/buQ6q9u47xxFrBp7jLjf7+9TX3lTXeli8vog3Vm1n4XdFuOu8jS7vtFvpkhpLdkoM7ZKiaZ8cTbukaLKSommXHE2bWKdqg0REDkMKOy2gsHMEMgwo+Qm++S989RLs3rjvWGoPcxX2zH6Q1stcp8tqo6Sqls827WZjUTkbdpSxcWc5G4vKqa5tHIAaahPrZHCnZIZ0SmFwpxT6ZiWo87OIyGFAYacFFHaOcIYB+Z+ZoefbOVBbGXjcHmUGIF+fn7bHQtZAiE7C6zXYVlzFxqJyftpbyU/FVWzbW8W2+seisppGHxftsDGwQxL92iWSHOskKdpBUoyDxGgnSTEOUuNcWu5CRCQEFHZaQGEngtSUmYHnx0/M5q5d30NdddPnpnQ1Q0+74yDrOPNxv3l+qms9fLu9hOU/7uXLH/ew/Me9lFTVNn29Bk7unsqk07pxfOcUNYGJiASJwk4LKOxEMK8H9v64r59P4Tew/Sso3tL4XEcMdD4Fuo00t5TOjS/nNdi4s5wvNu/hh53llFTVUlJZS3FVLcWVbkqq6thdUYPvv6rBHZPJO60bp/ZMU+gREWllCjstoLBzFKrcA9tXmsFn21fw03KoKAo8p003c46fDkMhpQskd4aoX/73sXVPJX//6Ade//Inf4foPm0TuOG0rvTMiKe8ps7cqusoq39MinEwqGMyHVJiFIpERJpJYacFFHYEwzBnct6wwJznZ+vn4K1rfF5Mqlnjk9LF7P/T+1xo07XJSxaVVvPPTzbz8mdbqHR7mlWM1DgXgzomMahjMoM6JtM3K5Eoh+1Q7kxEJGIp7LSAwo40Ul0Cm5bADwvNJrA9m6BiZ9PnZg2EYy40h74ntmt0eG+Fm5mf/sirX+Tj9niJc9mJc9mJjzIfY112thdX8c220oDlLwAcNgvd0uPp3TaePm0T6JOVQJ+2CSTFOP3nGIZBda2XSncdVbUekmOcxLoaL7UhIhJpFHZaQGFHmqW61Oz/s2cT7PnB7AS9aYm5gjsAFuh4IvQ+B6KSgPr/tHz/iVmskH38AWuCfJ2hV2zZy5c/7mVl/l52lbubPDc1zgUYVLo9VNV62P+/4tQ4Jx1SYujYJrb+MYaemfH0zkzQ0hkiEjEUdlpAYUcOWvlOWDvXnO8nf1nz3tN1BBz/f+a6X9YDN1EZhjksfl1BGWu3l7K2oIR1BWXk76k84Htcdis1TUyc6JMa5+Tk7mmc3D2Vk7unaYi8iBzRFHZaQGFHWkXJT/DN/8waH2+dfxZnsJjPa8rM+YB8NT6JHWDwVXDcFRCbau6rq4HyHVBWaG4eN2T2NztLW82JDEura9myqxKH3UKMw06000aM00a0w4bVaqG0upb83ZVs2V3Jlj0V5O+u5MfdFaz+qaRR36E+bRMY0ikZp92KxWLB4isuFmxWiI9ykBzjICnGnE8oObZ+LqFYl2qIRCTsFHZaQGFHQmbPZvjyefjq5X3LXdhcZtNWWWHjBVB9nHHQdsC+CRGzjjUDUAtGbrnrvKzYspePNuzko+938u320oO+jcRoB4M7JjO4UwpDOiXTr30iLnvzOlLXebxU1nqorPFQXeuhbVJUs98rItKQwk4LKOxIyNVWmbVAy58zh783ZHNCXCbEZwKGOTdQXVXja0QnQ/vjzX5AHU4wJ0Z0xjS7CDvLavhk407WF5ZjGAYG5jxCBmY3I69hUFpVy95Kd/08Qr65hGrx7vdbw2m3cmz7JDq0iaGq1kOV22N2mHZ7qPRvdVS4PY3WJ7NbLXRNi6N323h6t02gd31HbLNfkojIgSnstIDCjoRVwWpzpFd8WzPgRCcH1th46syZoAtWmcFo+yooXN14Zmir3VwPLOs487Ftf0jvC46oVi1urcfLuoJSvti8hy9/3MuXW/YcsCP1z7FZLThslgOuTZYe76Jfu0SOqd/6tUskI8GleYhExE9hpwUUduSIU+eGHWtg6xfmnED5n0PZ9sbnWWzmumBt+5sLotqc5pIYNhfYneZjVKIZjhLbt6hZzMcwDH7cXcnyH/ews6yGGF8fIqedGIfvuY04l50YV/0+lw1n/WKqBSXVrCsoZV1BKWsLSllXUMaPuysajTADcxRapzYx2KwWrBYLVivmo8WC3WohLsoc0p8Q5SA+ykFCtDm8v7S6jl1lNeyuqGF3uZtd5eaj024NWOE+K8lc8T4zMYrkGAfRDpvClchhTGGnBRR2JCIUb4WfvoCCr83aosLVULm7+e+PSW2wVthAyDjG7CBduQcqd5nXqtxtvo5KMMNTcidzZun9a6MOUUVNHesKSlmzrYRvtpXyzbYSNhSVNWo+Czan3Wp2zI4xO2anxDpJj3eRkRhFZoK5+Z7HOH8+GNV6vOypcLOzrIad5TXU1HrJSHCRlRRNapwLmzp8i7SYwk4LKOxIRDIMKCvYF3zKd5ijvTzuwMeKInPixKZmjG4uVwIkd4SkjpCQVb+13/c8vu0hN6dVuT2sLShlR2k1hgEew8AwDLyGgdcLdV4vZdV1lFbXUVZdS2mV+VheU0ecy05qvIvUWCep8S7axLpIiXVSU+dhe3E124orzcf6Fe+Lyqqp9bTsV6PNaiHaYSPKYSXKYY6Oi3baqK71sKvczZ6KAzf12awWMuJdtK2vVeqQEkPHlBg6tImhQ0oMbROjFYZEmqCw0wIKO3LUq602l8vY/lX9emErYdd6c3HUmBSIaVO/pZq1ONXF5gSLe380A1Vz2Jzgim+wJZiPidnm8hu+Lbljo9XnQ80wzAkb91a66ztmmx2191S42VFaTWFptflYUs2O0hrKa5oXFG1WC21inaTGuXDarRSVVrOjrAbPL1RZOW1W2idHkxzr9DcTxjjt/udeA0qraimtD3kl9c9r6rwkRNn9UwckRjtIjDEffbVV/mkFYpwkxjiwWqDWY1Dr8eKu81Lr8VLrMXDYLGQkRGl2bjmsKOy0gMKOSBO8Xv/cPj+rtgqK881h9SVboXQ7lG4LfNy/M/XPspgBKLmj2Y8oMRuSsusfO0BcBjiif3ZCxlArr6mjssZcrsM3Gq2q1hxa77TZSIt3kRrnJDnG2Wh+ojqPl13lbraXVFFYYtYubd1rzpOUv6eSn/ZWtriWKZjiXHYyElxkJESRkRBFeryL1DgXqfFmiGsTaz5PiXFit/38v59aj5e9FW72VLrZU+6mus6Dy27DZbeajw4rTpsVh91KXX3oqvN6qa0zqPV68XgNbFYLTpsVl92Kw2bFaTe3OJe92evK1Xm81NR5FeSOQAo7LaCwIxJEhmGuNVZTtt9WatYQFefXL8GxyQxM7vLmXdfmMkOPI8Z8dMZAdMNaqPrn0clmGeqqzWa7ho/OWDNAJXUww1RcxoEDnqfWbOpzRLfaV9McHq9BQUkV+XsqKa2qDRjK73tuARKiHSTU194kRNlJiHbgtFkpra6lpLKWkipz800jUFLlrq+xqqWk0s3eylqqavdNOukbLeesDxBVbg8VzVzQ1scMLVZcDpv/udNuo8pdx54KN6XVh9B02gxxLjtt4pykxDppE+ukTawLh93Cngo3u8vd7K5ws7u8huKqWgwDslOiOa5DMgOzkziuYzK92ybg+IXABuZEnwXF1WwvrmJ7SRU1tV46p8XSLS2OdknRrToBZ0VNHT/sLGdPhZu2iWbH+rijOKQp7LSAwo7IYcIwzGH4ezaZIag436wtKvnJ7IBdshVqD7xcxiGzOc3apOhkcFeCu8IMX+5ys48TQFpv6DQMOp0EHYdBXHrwyhNi1fVhx2GzNtlHqLymjh31TXhFpTX1z81RbrvKa9hV5mZ3RQ17KtzN7kxusUByjBlIoh023HVeauo89Y/m5vZ4cVgt2G1m7Y3DZsFus2C3Wqnzms1t/q2+Bqg1uOxW+rVLJCHagdcw8Hj39RHzGAbFlW4Kiqsp+5lmzGiHja7pZvDplh5Hj4x4embGk50cc8AQZBgGRWU1bNpZwQ87y9lYVM4PO8v5oaic7SWNa0mTYhy0qx9J2D45hs5psXStD1tp8ZE9XYPCTgso7IgcIQzDDDu11fWPVeZjXTXUlJszUDccNVa529xnsYE9yuwL5H90mTVOvhBVug2MA68rdkCpPc0FYGPTfIXc7wSLuQis1WY+NnzuW0rEf57FLGtsG7OWKS7TDFOu+FYd7RZsHq8ZBKrrvFTXeqipNQOML7xEO2ykxDpIiXWRGO1o9c7XXq9BWXWdP3j5OojvqajBXec1a3riXGZtT5yLNnFOHFYrq7cVs3JLMV9t3ctX+cWUVNU2+zOTYhxmTUtSFA6blR92lrN5V8UBg1eUw0r39Hh6ZMTTLT2O8ppaftxVyeZdFfy4u6LR0i4NpcaZtVQFJVW/WDsWH2Wna1ocXdPiSE9wER9lN6dl8E/RYKfOa4arotJqdpbVmM/LqimpqvVP7WCzWrDVT/dgs1qwWa31AdQMoXarGT6TYhzmiMX6Js70hCgyElzEuexBCV0KOy2gsCMieGrN/kUlW80V7p2x5jIdzth9m7fOXN9sy1JzDbQd34SmbI4YM/TEZUJ8RuPHhPZmH6cQN7FFMq/XYNOuCtZsK8Zd5/X/wTfndzL/8MdH2clKiiYrKYoYZ+OmpDqPl/w9lWwsKmdDkVlD8/2OMjYUlTeaSXx/Vgu0T46hS9q+WiHflhTj9J9XVl3LtuIq/0jCrXsq/TVC+XsqQz5dw4FEO2z8d+KJ9Mlq3b+xCjstoLAjIgelcg9s+dSc2DGgE3bD/4M1wOsBw2PWHHm9+577f/0a+557a6Fil7lWWnkRuMuaX564THPuo5TO5mNchvlZnlqzGc5Ta26GB6KSzNqo2NT6xzSzj5Pd+QsfUs9TC1XF5n3HtGnRUiVHO4/XIH9PJesLy/h+Rxkbi8qJj7LTOTWWTm1i6ZwWS3ZyDE57MwYI/IyaOg8/7qrkh53lbNpZzu4KN2UNp2aoMR9tVgtp8S6zJiY+ivQE83lyjBMDA48XfxOex2tudV7D7DTuNfB4vNR5DWo9Zo3ejtJqispq/M2dvma+zyaPIDOxdWd0V9hpAYUdETlsuSvMOZLKdkB5YdOPJVvNDt+twR5dX5MVA47Yfc8Nwww31cXmIrb7dyR3xEJcfWiKTTefJ2bvC177Tz7pqYPiLWandF8H9boqc64m/4SVncyO5nJEq3TXUVRaQ3ZKTKs3WTb37/fR24VbRORI4IzdNwfRgRiGGUB8cx/t3Ww+Vuwy10yzOcHmqN+cZn+hqr1mZ/CKXfseDY8ZOOqqoLn9wG1Os9aotgL2VpifeyCuBDPM1FaYnc+bM5FlVCLEZ9XXitWaIcnjNp97PeaklW267dtSu5uPhgGlP5md20u27XteXWL2i7LWbxab+R3ZXebkl4ntzJCW2N7cXPH7yuL1mP3E6ur7jNXV1E/QWWMu4eJ7dERD+8FqVqwX47TTKTW8cUM1O6hmR0QEr9estakprR+FVmmGEnflvhFw0Ulm7UyU7zHRDE7ucrPJrWKnuZUXmVvxlp+ffNIeFTihpD0q8D3lO0J08z/DlQBYzADoOfAs2I3YnJA9FDoPhy7DzQV6bfv9wa+tNmcwr9hpPvc3Nbr3PcfY17G9Yed2m6t+eoX6KRZcCUdUJ/bWomasFlDYEREJMt/kk3t/NGs8UrqaNSk/N3Glu74GqKxgXw2V1bGvlgqLWVuzewPs3mhuuzaatThgNqcltoMEX21NO3MuJsNr1ioZHjPkeevMMFNaUF8T9FN9R/XiA5fN5jKXQLHVj+zzL7LrNIPe/gvzOuPNded8S7SU72xZf6xfYrXvm2PKV0OV4NuyzMe66vom0cLAR3dFfZiyNBglaN23z18DZmu6Vsxq37fPu38fsQbB7ewnzQ71rUhhpwUUdkREIkhtFWA55PXYqCk3R+hZLGZAs0fXP0b9fEgzDNj9A2xeDJuWwI8fm82GTbE5zX5OztgGzY3O+mBnNwNHQOf2+g7uddVQudecXqG24tDuM1QmfWk2M7Yi9dkREZGjU2v1lXHFQVqPlr/PYoHUbuY25FozpBSuhoKvISqhvgN3uhlyohIPvfmptmrfvFIVO82asJJt9cu1NFi6xR5ljtCLzwx8dCVgjgisHyFoeNk3irA+YPlGFDZ89HrMWjFfLZmnzgyBvrDWMLjZHObIvzBR2BEREQkmqxWyjjW3YHBE13esbhec60eAQxvELyIiInKYU9gRERGRiKawIyIiIhFNYUdEREQimsKOiIiIRDSFHREREYloCjsiIiIS0RR2REREJKIp7IiIiEhEU9gRERGRiKawIyIiIhFNYUdEREQimsKOiIiIRDSFHREREYlo9nAX4HBgGAYApaWlYS6JiIiINJfv77bv7/iBKOwAZWVlAGRnZ4e5JCIiItJSZWVlJCYmHvC4xfilOHQU8Hq9bN++nfj4eCwWS4vfX1paSnZ2Nlu3biUhISEIJTx8Ha33frTeNxy993603jfo3o/Gez9S7tswDMrKysjKysJqPXDPHNXsAFarlfbt2x/ydRISEg7rfxTBdLTe+9F633D03vvRet+gez8a7/1IuO+fq9HxUQdlERERiWgKOyIiIhLRFHZagcvl4p577sHlcoW7KCF3tN770XrfcPTe+9F636B7PxrvPdLuWx2URUREJKKpZkdEREQimsKOiIiIRDSFHREREYloCjsiIiIS0RR2DtHUqVPp1KkTUVFRDB06lC+++CLcRWp1H330Eeeccw5ZWVlYLBbmzp0bcNwwDO6++27atm1LdHQ0I0eOZMOGDeEpbCuaMmUKQ4YMIT4+nvT0dM477zzWr18fcE51dTV5eXm0adOGuLg4xo0bx44dO8JU4tYzffp0+vfv759QLCcnh3fffdd/PFLve3+PPPIIFouFm2++2b8vUu/93nvvxWKxBGy9evXyH4/U+/bZtm0bl112GW3atCE6Opp+/frx5Zdf+o9H6u+5Tp06Nfq5WywW8vLygMj5uSvsHILXXnuNW2+9lXvuuYeVK1cyYMAAcnNzKSoqCnfRWlVFRQUDBgxg6tSpTR5/9NFHefrpp3n22Wf5/PPPiY2NJTc3l+rq6hCXtHUtWbKEvLw8PvvsMxYsWEBtbS2jRo2ioqLCf84tt9zCW2+9xezZs1myZAnbt2/nggsuCGOpW0f79u155JFHWLFiBV9++SWnn346Y8eO5dtvvwUi974bWr58OX//+9/p379/wP5Ivve+fftSUFDg3z755BP/sUi+77179zJs2DAcDgfvvvsua9eu5fHHHyc5Odl/TqT+nlu+fHnAz3zBggUA/OpXvwIi6OduyEE7/vjjjby8PP9rj8djZGVlGVOmTAljqYILMObMmeN/7fV6jczMTOOxxx7z7ysuLjZcLpfx6quvhqGEwVNUVGQAxpIlSwzDMO/T4XAYs2fP9p+zbt06AzCWLVsWrmIGTXJysvHPf/7zqLjvsrIyo3v37saCBQuM4cOHGzfddJNhGJH9M7/nnnuMAQMGNHksku/bMAzj9ttvN0466aQDHj+afs/ddNNNRteuXQ2v1xtRP3fV7Bwkt9vNihUrGDlypH+f1Wpl5MiRLFu2LIwlC63NmzdTWFgY8D0kJiYydOjQiPseSkpKAEhJSQFgxYoV1NbWBtx7r1696NChQ0Tdu8fjYdasWVRUVJCTk3NU3HdeXh5jxowJuEeI/J/5hg0byMrKokuXLowfP578/Hwg8u/7zTffZPDgwfzqV78iPT2dgQMH8txzz/mPHy2/59xuNy+//DJXX301Foslon7uCjsHadeuXXg8HjIyMgL2Z2RkUFhYGKZShZ7vXiP9e/B6vdx8880MGzaMY445BjDv3el0kpSUFHBupNz7mjVriIuLw+Vycf311zNnzhz69OkT8fc9a9YsVq5cyZQpUxodi+R7Hzp0KDNnzuS9995j+vTpbN68mZNPPpmysrKIvm+ATZs2MX36dLp37878+fOZOHEiv/3tb/nXv/4FHD2/5+bOnUtxcTFXXnklEFn/3rXquUgz5OXl8c033wT0YYh0PXv2ZNWqVZSUlPCf//yHCRMmsGTJknAXK6i2bt3KTTfdxIIFC4iKigp3cUJq9OjR/uf9+/dn6NChdOzYkddff53o6Ogwliz4vF4vgwcP5uGHHwZg4MCBfPPNNzz77LNMmDAhzKULneeff57Ro0eTlZUV7qK0OtXsHKTU1FRsNlujXuk7duwgMzMzTKUKPd+9RvL3MGnSJObNm8eiRYto3769f39mZiZut5vi4uKA8yPl3p1OJ926dWPQoEFMmTKFAQMG8Ne//jWi73vFihUUFRVx3HHHYbfbsdvtLFmyhKeffhq73U5GRkbE3vv+kpKS6NGjBxs3bozonzlA27Zt6dOnT8C+3r17+5vxjobfc1u2bOGDDz7g2muv9e+LpJ+7ws5BcjqdDBo0iIULF/r3eb1eFi5cSE5OThhLFlqdO3cmMzMz4HsoLS3l888/P+K/B8MwmDRpEnPmzOHDDz+kc+fOAccHDRqEw+EIuPf169eTn59/xN97U7xeLzU1NRF93yNGjGDNmjWsWrXKvw0ePJjx48f7n0fqve+vvLycH374gbZt20b0zxxg2LBhjaaV+P777+nYsSMQ2b/nfGbMmEF6ejpjxozx74uon3u4e0gfyWbNmmW4XC5j5syZxtq1a43rrrvOSEpKMgoLC8NdtFZVVlZmfPXVV8ZXX31lAMYTTzxhfPXVV8aWLVsMwzCMRx55xEhKSjLeeOMNY/Xq1cbYsWONzp07G1VVVWEu+aGZOHGikZiYaCxevNgoKCjwb5WVlf5zrr/+eqNDhw7Ghx9+aHz55ZdGTk6OkZOTE8ZSt4477rjDWLJkibF582Zj9erVxh133GFYLBbj/fffNwwjcu+7KQ1HYxlG5N777373O2Px4sXG5s2bjaVLlxojR440UlNTjaKiIsMwIve+DcMwvvjiC8NutxsPPfSQsWHDBuPf//63ERMTY7z88sv+cyL195xhmCOJO3ToYNx+++2NjkXKz11h5xD97W9/Mzp06GA4nU7j+OOPNz777LNwF6nVLVq0yAAabRMmTDAMwxyWeddddxkZGRmGy+UyRowYYaxfvz68hW4FTd0zYMyYMcN/TlVVlXHDDTcYycnJRkxMjHH++ecbBQUF4St0K7n66quNjh07Gk6n00hLSzNGjBjhDzqGEbn33ZT9w06k3vvFF19stG3b1nA6nUa7du2Miy++2Ni4caP/eKTet89bb71lHHPMMYbL5TJ69epl/OMf/wg4Hqm/5wzDMObPn28ATd5PpPzcLYZhGGGpUhIREREJAfXZERERkYimsCMiIiIRTWFHREREIprCjoiIiEQ0hR0RERGJaAo7IiIiEtEUdkRERCSiKeyIiDTBYrEwd+7ccBdDRFqBwo6IHHauvPJKLBZLo+3MM88Md9FE5AhkD3cBRESacuaZZzJjxoyAfS6XK0ylEZEjmWp2ROSw5HK5yMzMDNiSk5MBs4lp+vTpjB49mujoaLp06cJ//vOfgPevWbOG008/nejoaNq0acN1111HeXl5wDkvvPACffv2xeVy0bZtWyZNmhRwfNeuXZx//vnExMTQvXt33nzzzeDetIgEhcKOiByR7rrrLsaNG8fXX3/N+PHjueSSS1i3bh0AFRUV5ObmkpyczPLly5k9ezYffPBBQJiZPn06eXl5XHfddaxZs4Y333yTbt26BXzGfffdx0UXXcTq1as566yzGD9+PHv27AnpfYpIKwj3SqQiIvubMGGCYbPZjNjY2IDtoYceMgzDXJH++uuvD3jP0KFDjYkTJxqGYRj/+Mc/jOTkZKO8vNx//O233zasVqtRWFhoGIZhZGVlGX/6058OWAbAuPPOO/2vy8vLDcB49913W+0+RSQ01GdHRA5Lp512GtOnTw/Yl5KS4n+ek5MTcCwnJ4dVq1YBsG7dOgYMGEBsbKz/+LBhw/B6vaxfvx6LxcL27dsZMWLEz5ahf//+/uexsbEkJCRQVFR0sLckImGisCMih6XY2NhGzUqtJTo6ulnnORyOgNcWiwWv1xuMIolIEKnPjogckT777LNGr3v37g1A7969+frrr6moqPAfX7p0KVarlZ49exIfH0+nTp1YuHBhSMssIuGhmh0ROSzV1NRQWFgYsM9ut5OamgrA7NmzGTx4MCeddBL//ve/+eKLL3j++ecBGD9+PPfccw8TJkzg3nvvZefOndx4441cfvnlZGRkAHDvvfdy/fXXk56ezujRoykrK2Pp0qXceOONob1REQk6hR0ROSy99957tG3bNmBfz549+e677wBzpNSsWbO44YYbaNu2La+++ip9+vQBICYmhvnz53PTTTcxZMgQYmJiGDduHE888YT/WhMmTKC6uponn3yS2267jdTUVC688MLQ3aCIhIzFMAwj3IUQEWkJi8XCnDlzOO+888JdFBE5AqjPjoiIiEQ0hR0RERGJaOqzIyJHHLW+i0hLqGZHREREIprCjoiIiEQ0hR0RERGJaAo7IiIiEtEUdkRERCSiKeyIiIhIRFPYERERkYimsCMiIiIRTWFHREREItr/Aymw5YZa1G0wAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m620.4384\u001b[0m      \u001b[32m382.0728\u001b[0m  0.0010  0.0926\n",
      "      2      \u001b[36m270.6949\u001b[0m      \u001b[32m160.4806\u001b[0m  0.0010  0.0883\n",
      "      3      \u001b[36m159.3039\u001b[0m      \u001b[32m141.4028\u001b[0m  0.0010  0.1016\n",
      "      4      \u001b[36m147.8866\u001b[0m      \u001b[32m135.0690\u001b[0m  0.0010  0.0867\n",
      "      5      \u001b[36m141.7053\u001b[0m      \u001b[32m130.9750\u001b[0m  0.0010  0.0879\n",
      "      6      \u001b[36m137.1370\u001b[0m      \u001b[32m128.6216\u001b[0m  0.0010  0.0977\n",
      "      7      \u001b[36m132.9875\u001b[0m      \u001b[32m127.1554\u001b[0m  0.0010  0.0947\n",
      "      8      \u001b[36m129.0842\u001b[0m      \u001b[32m126.0535\u001b[0m  0.0010  0.0989\n",
      "      9      \u001b[36m125.7572\u001b[0m      \u001b[32m125.0518\u001b[0m  0.0010  0.1004\n",
      "     10      \u001b[36m122.5381\u001b[0m      \u001b[32m122.8381\u001b[0m  0.0010  0.1030\n",
      "     11      \u001b[36m121.2748\u001b[0m      \u001b[32m122.6606\u001b[0m  0.0010  0.1070\n",
      "     12      \u001b[36m118.2585\u001b[0m      \u001b[32m122.0661\u001b[0m  0.0010  0.1103\n",
      "     13      \u001b[36m116.8067\u001b[0m      \u001b[32m121.1448\u001b[0m  0.0010  0.1003\n",
      "     14      \u001b[36m114.8546\u001b[0m      \u001b[32m120.8670\u001b[0m  0.0010  0.1014\n",
      "     15      \u001b[36m113.6295\u001b[0m      \u001b[32m119.8187\u001b[0m  0.0010  0.0901\n",
      "     16      \u001b[36m111.9878\u001b[0m      \u001b[32m119.1762\u001b[0m  0.0010  0.0873\n",
      "     17      \u001b[36m109.7922\u001b[0m      \u001b[32m118.9213\u001b[0m  0.0010  0.0981\n",
      "     18      109.9275      \u001b[32m118.4307\u001b[0m  0.0010  0.0854\n",
      "     19      \u001b[36m108.4722\u001b[0m      \u001b[32m117.5087\u001b[0m  0.0010  0.0900\n",
      "     20      \u001b[36m107.7092\u001b[0m      \u001b[32m117.4994\u001b[0m  0.0010  0.1031\n",
      "     21      \u001b[36m106.3049\u001b[0m      \u001b[32m117.0531\u001b[0m  0.0010  0.0932\n",
      "     22      108.6672      117.7611  0.0010  0.1018\n",
      "     23      \u001b[36m105.2838\u001b[0m      \u001b[32m116.6600\u001b[0m  0.0010  0.1009\n",
      "     24      \u001b[36m104.5858\u001b[0m      117.1554  0.0010  0.1027\n",
      "     25      106.6438      116.7557  0.0010  0.1053\n",
      "     26      \u001b[36m104.0233\u001b[0m      \u001b[32m115.8803\u001b[0m  0.0010  0.1014\n",
      "     27      \u001b[36m103.1336\u001b[0m      116.0546  0.0010  0.0949\n",
      "     28      \u001b[36m102.3531\u001b[0m      116.4495  0.0010  0.0860\n",
      "     29      102.4619      \u001b[32m115.8286\u001b[0m  0.0010  0.1118\n",
      "     30      \u001b[36m102.2263\u001b[0m      \u001b[32m115.0719\u001b[0m  0.0010  0.1142\n",
      "     31      \u001b[36m102.1617\u001b[0m      115.3190  0.0010  0.0891\n",
      "     32      102.6799      116.2675  0.0010  0.0936\n",
      "     33       \u001b[36m99.8395\u001b[0m      115.2359  0.0010  0.0920\n",
      "     34      101.9521      \u001b[32m114.5382\u001b[0m  0.0010  0.0949\n",
      "     35       99.9146      115.4401  0.0010  0.1567\n",
      "     36       \u001b[36m98.8096\u001b[0m      \u001b[32m114.1255\u001b[0m  0.0010  0.0973\n",
      "     37       99.3415      114.9211  0.0010  0.0935\n",
      "     38      100.5616      114.7043  0.0010  0.1000\n",
      "     39       99.7880      114.8439  0.0010  0.0898\n",
      "     40       99.1731      114.7869  0.0010  0.0985\n",
      "     41       \u001b[36m98.0872\u001b[0m      \u001b[32m113.3648\u001b[0m  0.0005  0.0941\n",
      "     42       98.1712      \u001b[32m113.0123\u001b[0m  0.0005  0.0915\n",
      "     43       \u001b[36m96.8556\u001b[0m      113.2048  0.0005  0.0943\n",
      "     44       \u001b[36m96.6298\u001b[0m      113.2834  0.0005  0.1115\n",
      "     45       \u001b[36m96.3159\u001b[0m      \u001b[32m112.9071\u001b[0m  0.0005  0.1091\n",
      "     46       \u001b[36m95.8443\u001b[0m      \u001b[32m112.5336\u001b[0m  0.0005  0.0942\n",
      "     47       \u001b[36m95.3122\u001b[0m      112.6587  0.0005  0.0954\n",
      "     48       96.1766      \u001b[32m112.4534\u001b[0m  0.0005  0.0971\n",
      "     49       96.3519      112.4995  0.0005  0.0987\n",
      "     50       95.9150      \u001b[32m112.4493\u001b[0m  0.0005  0.0956\n",
      "     51       96.2884      112.6814  0.0005  0.0949\n",
      "     52       96.4631      \u001b[32m112.4267\u001b[0m  0.0005  0.0970\n",
      "     53       96.3025      \u001b[32m112.3215\u001b[0m  0.0005  0.0963\n",
      "     54       95.9275      \u001b[32m111.9275\u001b[0m  0.0005  0.0964\n",
      "     55       \u001b[36m95.2876\u001b[0m      112.1129  0.0005  0.0959\n",
      "     56       95.6318      111.9564  0.0005  0.0918\n",
      "     57       95.8401      112.2952  0.0005  0.0988\n",
      "     58       96.6051      112.0851  0.0005  0.0981\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def plot(pipeline):\n",
    "    history = pipeline.named_steps['nn'].history\n",
    "    \n",
    "    epochs = history[:, 'epoch']\n",
    "    train_loss = history[:, 'train_loss']\n",
    "    valid_loss = history[:, 'valid_loss']\n",
    "    \n",
    "    plt.plot(epochs, train_loss, label='Train loss')\n",
    "    plt.plot(epochs, valid_loss, label='Valid loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.title('Train/Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "pipe_fusion.fit(x_cov_nn, y)\n",
    "plot(pipe_fusion)\n",
    "pipe_fusion.fit(x_cov_nn_f, y_f)\n",
    "plot(pipe_fusion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ad26a5e-f3e6-4a07-aae2-23535b5a84a0",
   "metadata": {},
   "source": [
    "| Method                                   | RMSE Guided | RMSE  Free |\n",
    "|------------------------------------------|-------------------|------------------:|\n",
    "| Basic Lasso                              |      18.73 | 15.78 |\n",
    "| Baseline approach                        |        –   | |\n",
    "| Covariance matrices + Lasso              |       7.36 | 10.45 |\n",
    "| Covariance matrices + Neural Network     |       5.02 | 10.61|\n",
    "| Convolutional Neural Network             |       4.54 | 11.51|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "When we compare the progression of our models, we see that incorporating covariance matrices into a simple Lasso regressor brings the rmse down about 62 % over the basic Lasso. \n",
    "\n",
    "...\n",
    "\n",
    "Moving on to a small neural network trained on those 36 tangent‐space features further reduces the error to 5.02, which represents a 39 % drop. Finally, the cnn achieves the lowest rmse of 4.87, a modest 3 % gain over the covariance nn hybrid.\n",
    "\n",
    "That last 3 % improvement, however, comes at a cost, the cnn is significantly more complex to train and deploy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416e1878-049b-4342-ad98-3c20afa1110e",
   "metadata": {},
   "source": [
    "#### Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ca1952-a0f3-45b6-8c78-0fae7b055672",
   "metadata": {},
   "source": [
    "For this question, we have ... regression models trained on different feature representations:\n",
    "\n",
    "- ...\n",
    "- Tangentes spaces of the signal using the NN + Covariance Matrice\n",
    "- Filtered raw signal data using a CNN\n",
    "\n",
    "\n",
    "To obtain each models predictions, we use cross_val_predict, which is essentially the same as cross_val_score but returns the predictions of each fold instead of the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4538322-bae9-4ba8-9e60-3a0001b3f189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m190.0825\u001b[0m      \u001b[32m114.3920\u001b[0m  0.0010  12.3754\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m188.6428\u001b[0m      \u001b[32m121.6362\u001b[0m  0.0010  12.4253\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m197.9401\u001b[0m      \u001b[32m127.3454\u001b[0m  0.0010  12.5659\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m185.1063\u001b[0m      \u001b[32m107.2864\u001b[0m  0.0010  12.5718\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m191.3966\u001b[0m      \u001b[32m118.1544\u001b[0m  0.0010  12.5802\n",
      "      2       \u001b[36m84.8360\u001b[0m       \u001b[32m77.6361\u001b[0m  0.0010  12.2554\n",
      "      2       \u001b[36m88.9043\u001b[0m       \u001b[32m71.5704\u001b[0m  0.0010  12.2696\n",
      "      2       \u001b[36m91.6968\u001b[0m       \u001b[32m71.5878\u001b[0m  0.0010  12.2443\n",
      "      2       \u001b[36m90.0341\u001b[0m      165.0861  0.0010  12.3428\n",
      "      2       \u001b[36m84.2259\u001b[0m       \u001b[32m88.9315\u001b[0m  0.0010  12.3595\n",
      "      3       \u001b[36m72.7609\u001b[0m       \u001b[32m69.5489\u001b[0m  0.0010  12.1344\n",
      "      3       \u001b[36m73.5579\u001b[0m       \u001b[32m64.5679\u001b[0m  0.0010  12.3983\n",
      "      3       \u001b[36m71.7214\u001b[0m       \u001b[32m80.8750\u001b[0m  0.0010  12.2971\n",
      "      3       \u001b[36m72.4569\u001b[0m       \u001b[32m65.3832\u001b[0m  0.0010  12.4904\n",
      "      3       \u001b[36m73.8503\u001b[0m       \u001b[32m98.7922\u001b[0m  0.0010  12.5368\n",
      "      4       \u001b[36m67.0553\u001b[0m       \u001b[32m66.1675\u001b[0m  0.0010  12.6378\n",
      "      4       \u001b[36m65.5816\u001b[0m       \u001b[32m62.5006\u001b[0m  0.0010  12.6623\n",
      "      4       \u001b[36m66.2510\u001b[0m       98.9855  0.0010  12.5675\n",
      "      4       \u001b[36m66.5644\u001b[0m      106.5398  0.0010  12.5078\n",
      "      4       \u001b[36m64.8413\u001b[0m       70.0399  0.0010  12.7557\n",
      "      5       \u001b[36m62.2907\u001b[0m       \u001b[32m66.0042\u001b[0m  0.0010  12.5541\n",
      "      5       \u001b[36m59.9663\u001b[0m       63.1728  0.0010  12.6123\n",
      "      5       \u001b[36m60.8183\u001b[0m      138.2341  0.0010  12.6155\n",
      "      5       \u001b[36m61.3641\u001b[0m      127.2214  0.0010  12.6178\n",
      "      5       \u001b[36m57.4214\u001b[0m       68.6906  0.0010  12.8419\n",
      "      6       \u001b[36m57.1342\u001b[0m       67.9706  0.0010  12.7265\n",
      "      6       \u001b[36m53.6472\u001b[0m       \u001b[32m61.4325\u001b[0m  0.0010  12.7609\n",
      "      6       \u001b[36m53.3603\u001b[0m       93.6041  0.0010  12.8637\n",
      "      6       \u001b[36m56.3369\u001b[0m      121.9392  0.0010  12.7630\n",
      "      6       \u001b[36m48.8350\u001b[0m       67.3860  0.0010  12.9891\n",
      "      7       \u001b[36m49.8409\u001b[0m       \u001b[32m57.3096\u001b[0m  0.0010  13.2797\n",
      "      7       \u001b[36m48.1331\u001b[0m       62.0468  0.0010  13.4034\n",
      "      7       \u001b[36m43.8421\u001b[0m      101.0936  0.0010  13.2654\n",
      "      7       \u001b[36m50.2678\u001b[0m       \u001b[32m80.6953\u001b[0m  0.0010  13.2344\n",
      "      7       \u001b[36m43.5505\u001b[0m      111.1726  0.0010  13.2668\n",
      "      8       \u001b[36m43.8807\u001b[0m       \u001b[32m49.5378\u001b[0m  0.0010  13.1209\n",
      "      8       \u001b[36m45.0045\u001b[0m       \u001b[32m52.9035\u001b[0m  0.0010  13.1719\n",
      "      8       \u001b[36m44.4684\u001b[0m       \u001b[32m57.5442\u001b[0m  0.0010  13.1652\n",
      "      8       \u001b[36m37.3113\u001b[0m       \u001b[32m59.0522\u001b[0m  0.0005  13.3013\n",
      "      8       \u001b[36m40.4727\u001b[0m       \u001b[32m56.5519\u001b[0m  0.0005  13.1937\n",
      "      9       \u001b[36m38.4908\u001b[0m       \u001b[32m39.7157\u001b[0m  0.0010  13.1873\n",
      "      9       \u001b[36m40.8115\u001b[0m       55.4179  0.0010  13.2686\n",
      "      9       \u001b[36m39.6259\u001b[0m       82.3554  0.0010  13.2872\n",
      "      9       \u001b[36m33.2571\u001b[0m       \u001b[32m42.0786\u001b[0m  0.0005  13.3621\n",
      "      9       \u001b[36m37.0385\u001b[0m       \u001b[32m55.9168\u001b[0m  0.0005  13.4449\n",
      "     10       \u001b[36m33.4492\u001b[0m       42.6033  0.0010  13.4331\n",
      "     10       \u001b[36m35.2438\u001b[0m       68.7822  0.0010  13.5616\n",
      "     10       \u001b[36m33.0612\u001b[0m      103.4771  0.0010  13.7587\n",
      "     10       \u001b[36m29.3952\u001b[0m       43.1617  0.0005  13.8406\n",
      "     10       \u001b[36m33.4317\u001b[0m       62.4451  0.0005  13.5524\n",
      "     11       \u001b[36m25.6024\u001b[0m       50.3882  0.0010  13.5190\n",
      "     11       \u001b[36m26.6500\u001b[0m       73.1232  0.0010  13.5506\n",
      "     11       \u001b[36m27.3974\u001b[0m       65.4512  0.0010  13.6597\n",
      "     11       \u001b[36m26.0373\u001b[0m       45.0058  0.0005  13.6423\n",
      "     11       \u001b[36m29.4789\u001b[0m       65.0548  0.0005  13.8889\n",
      "     12       \u001b[36m20.0346\u001b[0m       \u001b[32m38.4378\u001b[0m  0.0010  13.4895\n",
      "     12       \u001b[36m21.1439\u001b[0m       55.4959  0.0010  13.6606\n",
      "     12       \u001b[36m18.3582\u001b[0m       64.5474  0.0010  13.7614\n",
      "     12       \u001b[36m22.0282\u001b[0m       45.8279  0.0005  13.7323\n",
      "     12       \u001b[36m25.8444\u001b[0m       64.5458  0.0005  13.6929\n",
      "     13       \u001b[36m15.7189\u001b[0m       \u001b[32m33.6527\u001b[0m  0.0010  13.2472\n",
      "     13       \u001b[36m18.4862\u001b[0m       \u001b[32m45.0504\u001b[0m  0.0005  13.3714\n",
      "     13       \u001b[36m13.9089\u001b[0m       \u001b[32m31.4227\u001b[0m  0.0005  13.4599\n",
      "     13       \u001b[36m17.8763\u001b[0m       45.1676  0.0005  13.4323\n",
      "     13       \u001b[36m22.3821\u001b[0m       63.1151  0.0005  13.3860\n",
      "     14       \u001b[36m13.7164\u001b[0m       38.5740  0.0010  13.4010\n",
      "     14       \u001b[36m15.5567\u001b[0m       \u001b[32m33.3449\u001b[0m  0.0005  13.7934\n",
      "     14       13.9114       \u001b[32m31.2839\u001b[0m  0.0005  13.6351\n",
      "     14       \u001b[36m16.0822\u001b[0m       60.4768  0.0003  13.6278\n",
      "     14       \u001b[36m21.5930\u001b[0m       \u001b[32m43.5858\u001b[0m  0.0003  13.8991\n",
      "     15       \u001b[36m12.3370\u001b[0m       36.7504  0.0010  14.2614\n",
      "     15       \u001b[36m13.1210\u001b[0m       \u001b[32m30.0099\u001b[0m  0.0005  14.1164\n",
      "     15       \u001b[36m13.0543\u001b[0m       32.3618  0.0005  14.4157\n",
      "     15       \u001b[36m13.8885\u001b[0m       59.4961  0.0003  14.5483\n",
      "     15       \u001b[36m21.4931\u001b[0m       \u001b[32m36.9801\u001b[0m  0.0003  14.4586\n",
      "     16       \u001b[36m12.0499\u001b[0m       35.8740  0.0010  14.7097\n",
      "     16       \u001b[36m12.0895\u001b[0m       \u001b[32m27.2194\u001b[0m  0.0005  14.7340\n",
      "     16       \u001b[36m12.2440\u001b[0m       32.1846  0.0005  14.6321\n",
      "     16       \u001b[36m12.4871\u001b[0m       56.4220  0.0003  14.8427\n",
      "     16       \u001b[36m19.3145\u001b[0m       \u001b[32m31.5767\u001b[0m  0.0003  15.0356\n",
      "     17       13.1285       35.9383  0.0010  15.9074\n",
      "     17       \u001b[36m11.4651\u001b[0m       \u001b[32m25.4813\u001b[0m  0.0005  15.7735\n",
      "     17       \u001b[36m11.4059\u001b[0m       32.2354  0.0005  15.7487\n",
      "     17       \u001b[36m11.6980\u001b[0m       51.8588  0.0003  15.7270\n",
      "     17       \u001b[36m16.8523\u001b[0m       \u001b[32m29.7404\u001b[0m  0.0003  15.7500\n",
      "     18       14.9682       49.1283  0.0005  15.8404\n",
      "     18       \u001b[36m10.9353\u001b[0m       \u001b[32m24.4192\u001b[0m  0.0005  15.7851\n",
      "     18       \u001b[36m10.3624\u001b[0m       32.1085  0.0005  15.9791\n",
      "     18       11.8582       \u001b[32m37.6699\u001b[0m  0.0001  15.9001\n",
      "     18       \u001b[36m14.3953\u001b[0m       30.5577  0.0003  16.0405\n",
      "     19       13.3499       40.1932  0.0005  17.2345\n",
      "     19       \u001b[36m10.4939\u001b[0m       \u001b[32m23.3400\u001b[0m  0.0005  17.3387\n",
      "     19        \u001b[36m9.9948\u001b[0m       31.9687  0.0003  17.4672\n",
      "     19       11.9828       \u001b[32m35.7203\u001b[0m  0.0001  17.1951\n",
      "     19       \u001b[36m12.6443\u001b[0m       31.5810  0.0003  17.9213\n",
      "     20       12.3373       \u001b[32m30.6548\u001b[0m  0.0005  16.9105\n",
      "     20       \u001b[36m10.1793\u001b[0m       \u001b[32m22.6882\u001b[0m  0.0005  16.5200\n",
      "     20       \u001b[36m11.2662\u001b[0m       \u001b[32m35.2751\u001b[0m  0.0001  16.2520\n",
      "     20        \u001b[36m9.4309\u001b[0m       35.6076  0.0003  16.4794\n",
      "     20       \u001b[36m11.4694\u001b[0m       32.4534  0.0003  16.5573\n",
      "     21       \u001b[36m11.2784\u001b[0m       \u001b[32m22.4596\u001b[0m  0.0005  15.6552\n",
      "     21        \u001b[36m9.9274\u001b[0m       \u001b[32m22.6115\u001b[0m  0.0005  15.4866\n",
      "     21       \u001b[36m10.7658\u001b[0m       \u001b[32m34.5192\u001b[0m  0.0001  15.4631\n",
      "     21        \u001b[36m8.8517\u001b[0m       36.4549  0.0003  15.6996\n",
      "     21       \u001b[36m10.6562\u001b[0m       33.1133  0.0003  15.4730\n",
      "     22        \u001b[36m9.8380\u001b[0m       \u001b[32m21.4238\u001b[0m  0.0005  16.0315\n",
      "     22        \u001b[36m9.7795\u001b[0m       \u001b[32m22.1526\u001b[0m  0.0005  15.8317\n",
      "     22       \u001b[36m10.3824\u001b[0m       \u001b[32m33.7979\u001b[0m  0.0001  15.8669\n",
      "     22        \u001b[36m8.4302\u001b[0m       37.5358  0.0003  16.0163\n",
      "     22       11.0212       34.8883  0.0001  16.2757\n",
      "     23        \u001b[36m8.4708\u001b[0m       22.8039  0.0005  16.8873\n",
      "     23        \u001b[36m9.7029\u001b[0m       23.6210  0.0005  16.5627\n",
      "     23       \u001b[36m10.0561\u001b[0m       \u001b[32m33.1466\u001b[0m  0.0001  16.9591\n",
      "     23        \u001b[36m8.0283\u001b[0m       32.3951  0.0001  16.7877\n",
      "     23       11.0897       33.7182  0.0001  16.5394\n",
      "     24        \u001b[36m7.6531\u001b[0m       23.6987  0.0005  16.9098\n",
      "     24        \u001b[36m9.6408\u001b[0m       25.7346  0.0005  16.7462\n",
      "     24        \u001b[36m9.7721\u001b[0m       \u001b[32m32.8653\u001b[0m  0.0001  17.0027\n",
      "     24       \u001b[36m10.4709\u001b[0m       33.3919  0.0001  17.2722\n",
      "     25        \u001b[36m7.2170\u001b[0m       23.4183  0.0005  14.6092\n",
      "     25        \u001b[36m9.5206\u001b[0m       30.9973  0.0005  14.5455\n",
      "     25        \u001b[36m9.5244\u001b[0m       \u001b[32m32.1218\u001b[0m  0.0001  14.1798\n",
      "     25        \u001b[36m9.9363\u001b[0m       33.7315  0.0001  13.6916\n",
      "     26        \u001b[36m9.5060\u001b[0m       35.0414  0.0005  13.4525\n",
      "     26        \u001b[36m6.9882\u001b[0m       23.4811  0.0005  13.6306\n",
      "     26        \u001b[36m9.3048\u001b[0m       \u001b[32m31.8005\u001b[0m  0.0001  13.5944\n",
      "     26        \u001b[36m8.9551\u001b[0m       33.2456  0.0001  13.6650\n",
      "     27       11.8122       38.1360  0.0003  13.0804\n",
      "     27        \u001b[36m6.9821\u001b[0m       23.7273  0.0003  13.0883\n",
      "     27        \u001b[36m9.1017\u001b[0m       \u001b[32m31.5355\u001b[0m  0.0001  13.2375\n",
      "     28        \u001b[36m9.3455\u001b[0m       35.0121  0.0003  11.5848\n",
      "     28        6.9836       23.3618  0.0003  11.6815\n",
      "     28        \u001b[36m8.9132\u001b[0m       \u001b[32m31.2664\u001b[0m  0.0001  11.4428\n",
      "     29        \u001b[36m8.5947\u001b[0m       31.2654  0.0003  10.6356\n",
      "     29        \u001b[36m6.7753\u001b[0m       23.0003  0.0003  10.7565\n",
      "     29        \u001b[36m8.7380\u001b[0m       \u001b[32m31.1101\u001b[0m  0.0001  10.6979\n",
      "     30        \u001b[36m8.3827\u001b[0m       29.5935  0.0003  10.7712\n",
      "     30        \u001b[36m6.6214\u001b[0m       22.7407  0.0003  10.7112\n",
      "     30        \u001b[36m8.5772\u001b[0m       \u001b[32m30.8346\u001b[0m  0.0001  10.8656\n",
      "     31        8.7515       23.2205  0.0001  10.8908\n",
      "     31        \u001b[36m6.3634\u001b[0m       22.3596  0.0001  11.0704\n",
      "     31        \u001b[36m8.4248\u001b[0m       \u001b[32m30.5928\u001b[0m  0.0001  10.9475\n",
      "     32        \u001b[36m8.2805\u001b[0m       \u001b[32m30.1998\u001b[0m  0.0001  10.8339\n",
      "     33        \u001b[36m8.1425\u001b[0m       \u001b[32m29.7685\u001b[0m  0.0001  7.4672\n",
      "     34        \u001b[36m8.0103\u001b[0m       \u001b[32m29.4202\u001b[0m  0.0001  7.3047\n",
      "     35        \u001b[36m7.8816\u001b[0m       \u001b[32m29.3017\u001b[0m  0.0001  7.1548\n",
      "     36        \u001b[36m7.7586\u001b[0m       \u001b[32m29.1133\u001b[0m  0.0001  7.2755\n",
      "     37        \u001b[36m7.6424\u001b[0m       \u001b[32m28.8678\u001b[0m  0.0001  7.0796\n",
      "     38        \u001b[36m7.5313\u001b[0m       \u001b[32m28.5873\u001b[0m  0.0001  7.2236\n",
      "     39        \u001b[36m7.4275\u001b[0m       \u001b[32m28.5169\u001b[0m  0.0001  7.0202\n",
      "     40        \u001b[36m7.3315\u001b[0m       \u001b[32m28.4902\u001b[0m  0.0001  6.9742\n",
      "     41        \u001b[36m7.2420\u001b[0m       \u001b[32m28.4680\u001b[0m  0.0001  7.0820\n",
      "     42        \u001b[36m7.1568\u001b[0m       \u001b[32m28.4495\u001b[0m  0.0001  6.9462\n",
      "     43        \u001b[36m7.0773\u001b[0m       \u001b[32m28.3474\u001b[0m  0.0001  7.0137\n",
      "     44        \u001b[36m7.0029\u001b[0m       \u001b[32m28.3239\u001b[0m  0.0001  7.2692\n",
      "     45        \u001b[36m6.9310\u001b[0m       28.4330  0.0001  7.1250\n",
      "     46        \u001b[36m6.8641\u001b[0m       28.5135  0.0001  7.0005\n",
      "     47        \u001b[36m6.8002\u001b[0m       28.5403  0.0001  6.9602\n",
      "     48        \u001b[36m6.7401\u001b[0m       28.6693  0.0001  7.0305\n",
      "     49        6.7620       29.3896  0.0001  7.3658\n",
      "     50        \u001b[36m6.6812\u001b[0m       29.6132  0.0001  7.9520\n",
      "     51        \u001b[36m6.6240\u001b[0m       29.9519  0.0001  7.5421\n",
      "     52        \u001b[36m6.5772\u001b[0m       29.9926  0.0001  8.2411\n",
      "     53        6.6168       30.7258  0.0000  7.7451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m200.6720\u001b[0m      \u001b[32m208.5575\u001b[0m  0.0010  19.4901\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m227.6349\u001b[0m      \u001b[32m205.3068\u001b[0m  0.0010  19.5442\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m217.6904\u001b[0m      \u001b[32m207.1048\u001b[0m  0.0010  20.3991\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m229.4438\u001b[0m      \u001b[32m153.8134\u001b[0m  0.0010  20.5931\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m221.1901\u001b[0m      \u001b[32m282.0213\u001b[0m  0.0010  20.5743\n",
      "      2      \u001b[36m172.1306\u001b[0m      206.6618  0.0010  18.6329\n",
      "      2      \u001b[36m150.0942\u001b[0m      \u001b[32m173.0801\u001b[0m  0.0010  18.8816\n",
      "      2      \u001b[36m162.9990\u001b[0m      \u001b[32m175.2057\u001b[0m  0.0010  18.7387\n",
      "      2      \u001b[36m164.9526\u001b[0m      \u001b[32m145.1790\u001b[0m  0.0010  19.0032\n",
      "      2      \u001b[36m153.4440\u001b[0m      \u001b[32m249.6506\u001b[0m  0.0010  19.3820\n",
      "      3      \u001b[36m170.8317\u001b[0m      \u001b[32m188.9676\u001b[0m  0.0010  18.9416\n",
      "      3      \u001b[36m140.7735\u001b[0m      \u001b[32m140.0258\u001b[0m  0.0010  19.1221\n",
      "      3      \u001b[36m156.7438\u001b[0m      179.9396  0.0010  19.0659\n",
      "      3      \u001b[36m160.8474\u001b[0m      \u001b[32m142.3326\u001b[0m  0.0010  19.2221\n",
      "      3      \u001b[36m145.5230\u001b[0m      \u001b[32m241.8435\u001b[0m  0.0010  19.4475\n",
      "      4      172.0248      \u001b[32m186.5672\u001b[0m  0.0010  16.5719\n",
      "      4      \u001b[36m135.4297\u001b[0m      147.6366  0.0010  16.8443\n",
      "      4      \u001b[36m152.2073\u001b[0m      187.7596  0.0010  16.6581\n",
      "      4      \u001b[36m157.2473\u001b[0m      144.5942  0.0010  17.0755\n",
      "      4      \u001b[36m140.3288\u001b[0m      245.8413  0.0010  17.3240\n",
      "      5      \u001b[36m168.2787\u001b[0m      188.3647  0.0010  17.8848\n",
      "      5      \u001b[36m130.7284\u001b[0m      141.3621  0.0010  17.6663\n",
      "      5      \u001b[36m148.2742\u001b[0m      181.5988  0.0010  17.5401\n",
      "      5      \u001b[36m151.7815\u001b[0m      \u001b[32m139.8296\u001b[0m  0.0010  17.3769\n",
      "      5      \u001b[36m134.8814\u001b[0m      \u001b[32m229.2759\u001b[0m  0.0010  17.3554\n",
      "      6      \u001b[36m162.6226\u001b[0m      \u001b[32m178.8845\u001b[0m  0.0010  16.3182\n",
      "      6      \u001b[36m125.3694\u001b[0m      143.6727  0.0010  16.5507\n",
      "      6      \u001b[36m145.3840\u001b[0m      \u001b[32m173.5294\u001b[0m  0.0010  16.6024\n",
      "      6      \u001b[36m147.7215\u001b[0m      \u001b[32m135.0775\u001b[0m  0.0010  16.5318\n",
      "      6      \u001b[36m128.9767\u001b[0m      231.0504  0.0010  16.4432\n",
      "      7      \u001b[36m157.5559\u001b[0m      \u001b[32m160.9167\u001b[0m  0.0010  18.0364\n",
      "      7      \u001b[36m119.7325\u001b[0m      146.0994  0.0010  18.0922\n",
      "      7      \u001b[36m142.2734\u001b[0m      \u001b[32m166.2260\u001b[0m  0.0010  18.5001\n",
      "      7      \u001b[36m143.2494\u001b[0m      \u001b[32m129.2534\u001b[0m  0.0010  18.3147\n",
      "      7      \u001b[36m122.6291\u001b[0m      248.2671  0.0010  18.4510\n",
      "      8      \u001b[36m154.2972\u001b[0m      165.8294  0.0010  18.0454\n",
      "      8      \u001b[36m108.1620\u001b[0m      \u001b[32m132.2167\u001b[0m  0.0005  18.5976\n",
      "      8      \u001b[36m139.0455\u001b[0m      \u001b[32m163.4413\u001b[0m  0.0010  18.6272\n",
      "      8      \u001b[36m136.2348\u001b[0m      \u001b[32m122.4410\u001b[0m  0.0010  19.0443\n",
      "      8      \u001b[36m117.8415\u001b[0m      245.0268  0.0010  18.8379\n",
      "      9      \u001b[36m152.8336\u001b[0m      161.9947  0.0010  17.8898\n",
      "      9      \u001b[36m105.4326\u001b[0m      135.1284  0.0005  18.0354\n",
      "      9      \u001b[36m133.9261\u001b[0m      166.5808  0.0010  17.8458\n",
      "      9      \u001b[36m130.1438\u001b[0m      \u001b[32m115.3252\u001b[0m  0.0010  17.7739\n",
      "      9      \u001b[36m114.0792\u001b[0m      243.5145  0.0010  17.5913\n",
      "     10      \u001b[36m149.6246\u001b[0m      173.2515  0.0010  16.4764\n",
      "     10      \u001b[36m102.7514\u001b[0m      137.1961  0.0005  16.6831\n",
      "     10      \u001b[36m132.9855\u001b[0m      169.6585  0.0010  16.6290\n",
      "     10      \u001b[36m124.6750\u001b[0m      115.6341  0.0010  16.6502\n",
      "     10      \u001b[36m107.7366\u001b[0m      270.2869  0.0005  16.4690\n",
      "     11      153.3289      162.8020  0.0010  15.8261\n",
      "     11      \u001b[36m100.4780\u001b[0m      137.4520  0.0005  16.2145\n",
      "     11      \u001b[36m129.1967\u001b[0m      180.4324  0.0010  15.9931\n",
      "     11      \u001b[36m119.9650\u001b[0m      118.3899  0.0010  16.1121\n",
      "     11      \u001b[36m104.8891\u001b[0m      283.9305  0.0005  16.2197\n",
      "     12      170.6504      \u001b[32m134.3590\u001b[0m  0.0005  15.8275\n",
      "     12       \u001b[36m98.2658\u001b[0m      135.9631  0.0005  16.1848\n",
      "     12      129.4508      165.6412  0.0010  16.0075\n",
      "     12      \u001b[36m116.4444\u001b[0m      120.5474  0.0010  15.9614\n",
      "     12      \u001b[36m101.8261\u001b[0m      292.3275  0.0005  15.9318\n",
      "     13      \u001b[36m148.5467\u001b[0m      \u001b[32m127.2636\u001b[0m  0.0005  16.6515\n",
      "     13       \u001b[36m92.0873\u001b[0m      \u001b[32m129.0791\u001b[0m  0.0003  17.0200\n",
      "     13      146.9253      \u001b[32m136.3868\u001b[0m  0.0005  16.9482\n",
      "     13      \u001b[36m113.4365\u001b[0m      128.4064  0.0010  16.9057\n",
      "     13       \u001b[36m98.9232\u001b[0m      291.3722  0.0005  16.9718\n",
      "     14      \u001b[36m146.0858\u001b[0m      137.3969  0.0005  17.5298\n",
      "     14       \u001b[36m90.7228\u001b[0m      \u001b[32m127.4807\u001b[0m  0.0003  17.9959\n",
      "     14      138.9142      \u001b[32m133.3578\u001b[0m  0.0005  17.8294\n",
      "     14      126.1005      133.6031  0.0005  17.5488\n",
      "     14       \u001b[36m92.5514\u001b[0m      244.0402  0.0003  17.9061\n",
      "     15      \u001b[36m143.0154\u001b[0m      136.4471  0.0005  18.7039\n",
      "     15      132.3246      143.5645  0.0005  18.6219\n",
      "     15       \u001b[36m89.0639\u001b[0m      127.5023  0.0003  19.0202\n",
      "     15      132.4302      142.3900  0.0005  18.7535\n",
      "     16      \u001b[36m142.1187\u001b[0m      132.7980  0.0005  17.0247\n",
      "     16      130.6022      147.3411  0.0005  16.8498\n",
      "     16       \u001b[36m87.5733\u001b[0m      \u001b[32m126.0335\u001b[0m  0.0003  16.5751\n",
      "     16      125.9196      \u001b[32m112.3240\u001b[0m  0.0005  16.1289\n",
      "     17      \u001b[36m140.2069\u001b[0m      129.4299  0.0005  17.1473\n",
      "     17      \u001b[36m128.9253\u001b[0m      149.5058  0.0005  17.1200\n",
      "     17       \u001b[36m86.0525\u001b[0m      \u001b[32m124.6847\u001b[0m  0.0003  17.1856\n",
      "     17      125.3834      114.9862  0.0005  17.2794\n",
      "     18      144.1694      \u001b[32m115.9467\u001b[0m  0.0003  15.4709\n",
      "     18      \u001b[36m128.2364\u001b[0m      151.3898  0.0005  15.8976\n",
      "     18       \u001b[36m84.4674\u001b[0m      \u001b[32m122.7531\u001b[0m  0.0003  16.1375\n",
      "     18      122.4228      \u001b[32m109.3779\u001b[0m  0.0005  16.1266\n",
      "     19      145.3923      \u001b[32m111.3022\u001b[0m  0.0003  15.9741\n",
      "     19      129.3287      \u001b[32m123.9383\u001b[0m  0.0003  15.7484\n",
      "     19      119.5422      \u001b[32m108.4867\u001b[0m  0.0005  15.6476\n",
      "     19       \u001b[36m82.8610\u001b[0m      \u001b[32m119.4972\u001b[0m  0.0003  15.8972\n",
      "     20      143.8933      \u001b[32m110.6141\u001b[0m  0.0003  16.6434\n",
      "     20      130.8521      \u001b[32m122.7463\u001b[0m  0.0003  16.8032\n",
      "     20      116.1619      \u001b[32m106.9296\u001b[0m  0.0005  16.8358\n",
      "     20       \u001b[36m81.2035\u001b[0m      120.4540  0.0003  16.9993\n",
      "     21      141.2765      \u001b[32m110.3464\u001b[0m  0.0003  15.8004\n",
      "     21      129.6563      122.9091  0.0003  15.4359\n",
      "     21      113.9627      107.3325  0.0005  15.4990\n",
      "     21       \u001b[36m79.4879\u001b[0m      120.6896  0.0003  15.4941\n",
      "     22      \u001b[36m139.1161\u001b[0m      \u001b[32m110.2423\u001b[0m  0.0003  15.2581\n",
      "     22      128.2960      123.4839  0.0003  15.6032\n",
      "     22      \u001b[36m111.2183\u001b[0m      \u001b[32m106.1402\u001b[0m  0.0005  15.4393\n",
      "     22       \u001b[36m77.7102\u001b[0m      123.0812  0.0003  15.4183\n",
      "     23      \u001b[36m137.1526\u001b[0m      110.4686  0.0003  16.2486\n",
      "     23      \u001b[36m126.9638\u001b[0m      124.5010  0.0003  16.3810\n",
      "     23      \u001b[36m109.6339\u001b[0m      \u001b[32m105.4286\u001b[0m  0.0005  16.3509\n",
      "     23       \u001b[36m75.9015\u001b[0m      129.7453  0.0003  16.5235\n",
      "     24      \u001b[36m135.2334\u001b[0m      111.0616  0.0003  16.1440\n",
      "     24      \u001b[36m125.6531\u001b[0m      126.1707  0.0003  16.4668\n",
      "     24      \u001b[36m107.2800\u001b[0m      \u001b[32m105.0877\u001b[0m  0.0005  16.3598\n",
      "     24       \u001b[36m74.2067\u001b[0m      129.8335  0.0001  16.5356\n",
      "     25      \u001b[36m133.4152\u001b[0m      112.0508  0.0003  16.1541\n",
      "     25      127.1185      \u001b[32m122.0150\u001b[0m  0.0001  16.2627\n",
      "     25      \u001b[36m104.8608\u001b[0m      \u001b[32m104.3985\u001b[0m  0.0005  16.1213\n",
      "     25       \u001b[36m73.3403\u001b[0m      139.8061  0.0001  16.4369\n",
      "     26      \u001b[36m131.5946\u001b[0m      113.5828  0.0003  15.6546\n",
      "     26      \u001b[36m103.5337\u001b[0m      105.2907  0.0005  15.4878\n",
      "     26      \u001b[36m117.0456\u001b[0m      \u001b[32m119.4337\u001b[0m  0.0001  15.7512\n",
      "     26       \u001b[36m72.5587\u001b[0m      144.9440  0.0001  15.5702\n",
      "     27      132.3734      119.3730  0.0001  15.4927\n",
      "     27      \u001b[36m100.1957\u001b[0m      105.5141  0.0005  15.3364\n",
      "     27      \u001b[36m115.6485\u001b[0m      \u001b[32m118.2233\u001b[0m  0.0001  15.6787\n",
      "     27       \u001b[36m71.6647\u001b[0m      149.3882  0.0001  15.6298\n",
      "     28      \u001b[36m120.8534\u001b[0m      117.7670  0.0001  15.3743\n",
      "     28       \u001b[36m97.6888\u001b[0m      106.7003  0.0005  15.6050\n",
      "     28      \u001b[36m114.3741\u001b[0m      \u001b[32m117.5619\u001b[0m  0.0001  15.7195\n",
      "     28       73.7242      131.4199  0.0001  15.6468\n",
      "     29      \u001b[36m119.6597\u001b[0m      117.3339  0.0001  15.5917\n",
      "     29       \u001b[36m95.2572\u001b[0m      108.4069  0.0005  15.4717\n",
      "     29      \u001b[36m113.2510\u001b[0m      \u001b[32m117.0421\u001b[0m  0.0001  15.6237\n",
      "     30      \u001b[36m118.4736\u001b[0m      116.9902  0.0001  14.2294\n",
      "     30      107.1608      120.3497  0.0003  13.9499\n",
      "     30      \u001b[36m112.1801\u001b[0m      \u001b[32m116.7584\u001b[0m  0.0001  13.9784\n",
      "     31      \u001b[36m116.2010\u001b[0m      112.7030  0.0001  17.7490\n",
      "     31      105.1215      131.0030  0.0003  20.5145\n",
      "     31      \u001b[36m111.1389\u001b[0m      \u001b[32m116.5819\u001b[0m  0.0001  20.9101\n",
      "     32      102.4248      154.3806  0.0003  15.8957\n",
      "     32      \u001b[36m110.0991\u001b[0m      \u001b[32m116.4859\u001b[0m  0.0001  15.3472\n",
      "     33      100.6158      140.0408  0.0003  11.7277\n",
      "     33      \u001b[36m109.0704\u001b[0m      \u001b[32m116.4290\u001b[0m  0.0001  11.8433\n",
      "     34      105.4950      132.2352  0.0001  11.7119\n",
      "     34      \u001b[36m108.0482\u001b[0m      116.4959  0.0001  11.8827\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "     35      \u001b[36m106.9929\u001b[0m      116.6045  0.0001  11.6423\n",
      "     36      \u001b[36m105.9301\u001b[0m      116.7464  0.0001  9.7699\n",
      "     37      \u001b[36m104.8508\u001b[0m      116.8202  0.0001  8.6144\n",
      "     38      \u001b[36m102.1843\u001b[0m      125.7352  0.0001  8.3012\n",
      "     39       \u001b[36m95.5163\u001b[0m      124.6022  0.0001  8.3617\n",
      "     40       \u001b[36m94.8541\u001b[0m      124.4205  0.0001  8.4311\n",
      "     41       \u001b[36m94.2119\u001b[0m      124.2171  0.0001  8.3709\n",
      "     42       \u001b[36m91.4309\u001b[0m      136.5996  0.0000  8.3336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m464.1970\u001b[0m      \u001b[32m381.3698\u001b[0m  0.0010  0.2314\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m463.5089\u001b[0m      \u001b[32m385.4606\u001b[0m  0.0010  0.2160\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m461.8489\u001b[0m      \u001b[32m391.4241\u001b[0m  0.0010  0.2226\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m467.0748\u001b[0m      \u001b[32m405.2212\u001b[0m  0.0010  0.2039\n",
      "      2      \u001b[36m273.7858\u001b[0m      \u001b[32m174.7452\u001b[0m  0.0010  0.1695\n",
      "      2      \u001b[36m272.5044\u001b[0m      \u001b[32m176.4835\u001b[0m  0.0010  0.1617\n",
      "      2      \u001b[36m277.7730\u001b[0m      \u001b[32m179.0525\u001b[0m  0.0010  0.1677\n",
      "      2      \u001b[36m282.7645\u001b[0m      \u001b[32m196.6446\u001b[0m  0.0010  0.1666\n",
      "      3      \u001b[36m165.3585\u001b[0m      \u001b[32m137.9292\u001b[0m  0.0010  0.1675\n",
      "      3      \u001b[36m169.5985\u001b[0m      \u001b[32m138.1591\u001b[0m  0.0010  0.1551\n",
      "      3      \u001b[36m166.6096\u001b[0m      \u001b[32m138.7383\u001b[0m  0.0010  0.1877\n",
      "      3      \u001b[36m168.6796\u001b[0m      \u001b[32m136.0057\u001b[0m  0.0010  0.1842\n",
      "      4      \u001b[36m142.4671\u001b[0m      \u001b[32m125.0859\u001b[0m  0.0010  0.1545\n",
      "      4      \u001b[36m140.2821\u001b[0m      \u001b[32m128.0249\u001b[0m  0.0010  0.1656\n",
      "      4      \u001b[36m138.0454\u001b[0m      \u001b[32m123.5317\u001b[0m  0.0010  0.1475\n",
      "      4      \u001b[36m138.8406\u001b[0m      \u001b[32m117.5435\u001b[0m  0.0010  0.1450\n",
      "      5      \u001b[36m123.6931\u001b[0m      \u001b[32m121.1065\u001b[0m  0.0010  0.1366\n",
      "      5      \u001b[36m119.4653\u001b[0m      \u001b[32m111.1449\u001b[0m  0.0010  0.1300\n",
      "      5      \u001b[36m127.8345\u001b[0m      \u001b[32m111.2797\u001b[0m  0.0010  0.1524\n",
      "      5      \u001b[36m118.0906\u001b[0m      \u001b[32m106.2044\u001b[0m  0.0010  0.1736\n",
      "      6      \u001b[36m111.0423\u001b[0m      \u001b[32m111.0844\u001b[0m  0.0010  0.1521\n",
      "      6      \u001b[36m111.3194\u001b[0m       \u001b[32m98.9478\u001b[0m  0.0010  0.1497\n",
      "      6      \u001b[36m106.4539\u001b[0m      \u001b[32m102.5377\u001b[0m  0.0010  0.1673\n",
      "      6      \u001b[36m109.3804\u001b[0m       \u001b[32m96.9019\u001b[0m  0.0010  0.1303\n",
      "      7      \u001b[36m100.1869\u001b[0m      \u001b[32m102.0151\u001b[0m  0.0010  0.1339\n",
      "      7       \u001b[36m98.7399\u001b[0m       \u001b[32m88.7367\u001b[0m  0.0010  0.1590\n",
      "      7       \u001b[36m97.3895\u001b[0m       \u001b[32m96.1863\u001b[0m  0.0010  0.1497\n",
      "      7       \u001b[36m99.3219\u001b[0m       \u001b[32m90.0530\u001b[0m  0.0010  0.1548\n",
      "      8       \u001b[36m92.6195\u001b[0m       \u001b[32m82.3889\u001b[0m  0.0010  0.1536\n",
      "      8       \u001b[36m95.8842\u001b[0m       \u001b[32m92.6547\u001b[0m  0.0010  0.1884\n",
      "      8       \u001b[36m91.9630\u001b[0m       \u001b[32m91.2321\u001b[0m  0.0010  0.1543\n",
      "      8       \u001b[36m93.3900\u001b[0m       \u001b[32m84.5478\u001b[0m  0.0010  0.1528\n",
      "      9       \u001b[36m90.1510\u001b[0m       \u001b[32m88.1897\u001b[0m  0.0010  0.1473\n",
      "      9       \u001b[36m86.7129\u001b[0m       \u001b[32m76.2936\u001b[0m  0.0010  0.1653\n",
      "      9       \u001b[36m87.3533\u001b[0m       \u001b[32m85.6833\u001b[0m  0.0010  0.1685\n",
      "      9       \u001b[36m88.6632\u001b[0m       \u001b[32m81.2947\u001b[0m  0.0010  0.1646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     10       \u001b[36m83.1446\u001b[0m       \u001b[32m70.9504\u001b[0m  0.0010  0.1692\n",
      "     10       \u001b[36m86.1202\u001b[0m       \u001b[32m79.9657\u001b[0m  0.0010  0.1886\n",
      "     10       \u001b[36m81.8693\u001b[0m       \u001b[32m80.9562\u001b[0m  0.0010  0.1854\n",
      "     10       \u001b[36m84.9745\u001b[0m       \u001b[32m77.4604\u001b[0m  0.0010  0.1603\n",
      "     11       \u001b[36m84.2302\u001b[0m       \u001b[32m76.7954\u001b[0m  0.0010  0.1462\n",
      "     11       \u001b[36m79.5558\u001b[0m       \u001b[32m77.1963\u001b[0m  0.0010  0.1345\n",
      "     11       \u001b[36m78.3445\u001b[0m       \u001b[32m67.9383\u001b[0m  0.0010  0.1680\n",
      "     11       \u001b[36m80.7559\u001b[0m       \u001b[32m73.6755\u001b[0m  0.0010  0.1601\n",
      "     12       \u001b[36m79.8397\u001b[0m       \u001b[32m73.4788\u001b[0m  0.0010  0.1842\n",
      "     12       \u001b[36m74.6633\u001b[0m       \u001b[32m64.6596\u001b[0m  0.0010  0.1715\n",
      "     12       \u001b[36m76.7333\u001b[0m       \u001b[32m72.2969\u001b[0m  0.0010  0.1789\n",
      "     12       \u001b[36m77.7994\u001b[0m       \u001b[32m70.3879\u001b[0m  0.0010  0.2028\n",
      "     13       \u001b[36m72.2764\u001b[0m       \u001b[32m62.2234\u001b[0m  0.0010  0.1677\n",
      "     13       \u001b[36m72.8892\u001b[0m       \u001b[32m69.4100\u001b[0m  0.0010  0.1708\n",
      "     13       \u001b[36m76.7493\u001b[0m       \u001b[32m71.9598\u001b[0m  0.0010  0.1772\n",
      "     13       \u001b[36m74.1922\u001b[0m       \u001b[32m67.3183\u001b[0m  0.0010  0.1588\n",
      "     14       \u001b[36m69.6874\u001b[0m       \u001b[32m60.1043\u001b[0m  0.0010  0.1651\n",
      "     14       \u001b[36m69.3007\u001b[0m       \u001b[32m65.4223\u001b[0m  0.0010  0.1612\n",
      "     14       \u001b[36m74.6653\u001b[0m       \u001b[32m69.4274\u001b[0m  0.0010  0.1618\n",
      "     14       \u001b[36m72.4078\u001b[0m       \u001b[32m63.8304\u001b[0m  0.0010  0.1551\n",
      "     15       \u001b[36m66.5826\u001b[0m       \u001b[32m57.4126\u001b[0m  0.0010  0.1200\n",
      "     15       \u001b[36m67.5070\u001b[0m       \u001b[32m62.4794\u001b[0m  0.0010  0.1285\n",
      "     15       \u001b[36m72.0228\u001b[0m       \u001b[32m66.9028\u001b[0m  0.0010  0.1592\n",
      "     15       \u001b[36m69.7215\u001b[0m       \u001b[32m61.9037\u001b[0m  0.0010  0.1447\n",
      "     16       \u001b[36m64.2331\u001b[0m       \u001b[32m55.3160\u001b[0m  0.0010  0.1423\n",
      "     16       \u001b[36m64.5565\u001b[0m       \u001b[32m60.8786\u001b[0m  0.0010  0.1680\n",
      "     16       \u001b[36m68.8065\u001b[0m       \u001b[32m63.7904\u001b[0m  0.0010  0.1378\n",
      "     16       \u001b[36m66.0965\u001b[0m       \u001b[32m59.0542\u001b[0m  0.0010  0.1699\n",
      "     17       64.8894       55.4171  0.0010  0.1456\n",
      "     17       \u001b[36m61.1106\u001b[0m       \u001b[32m59.0317\u001b[0m  0.0010  0.1406\n",
      "     17       \u001b[36m68.0656\u001b[0m       \u001b[32m60.0466\u001b[0m  0.0010  0.1685\n",
      "     17       \u001b[36m64.8978\u001b[0m       \u001b[32m55.7290\u001b[0m  0.0010  0.1654\n",
      "     18       \u001b[36m61.2800\u001b[0m       \u001b[32m51.6599\u001b[0m  0.0010  0.1759\n",
      "     18       61.5922       \u001b[32m57.5515\u001b[0m  0.0010  0.1792\n",
      "     18       \u001b[36m65.0035\u001b[0m       \u001b[32m58.7471\u001b[0m  0.0010  0.1687\n",
      "     18       \u001b[36m60.6812\u001b[0m       \u001b[32m53.4444\u001b[0m  0.0010  0.1718\n",
      "     19       \u001b[36m58.9783\u001b[0m       \u001b[32m50.0618\u001b[0m  0.0010  0.1614\n",
      "     19       \u001b[36m59.6443\u001b[0m       \u001b[32m55.0339\u001b[0m  0.0010  0.1354\n",
      "     19       \u001b[36m64.4732\u001b[0m       \u001b[32m57.4600\u001b[0m  0.0010  0.1518\n",
      "     19       \u001b[36m58.8007\u001b[0m       \u001b[32m51.3481\u001b[0m  0.0010  0.1658\n",
      "     20       \u001b[36m58.2737\u001b[0m       55.6439  0.0010  0.1654\n",
      "     20       \u001b[36m57.4673\u001b[0m       \u001b[32m49.3130\u001b[0m  0.0010  0.2012\n",
      "     20       \u001b[36m62.2254\u001b[0m       \u001b[32m56.9603\u001b[0m  0.0010  0.1796\n",
      "     20       \u001b[36m55.5702\u001b[0m       \u001b[32m50.6247\u001b[0m  0.0010  0.1767\n",
      "     21       \u001b[36m57.7469\u001b[0m       \u001b[32m53.6002\u001b[0m  0.0010  0.1502\n",
      "     21       58.2953       49.7806  0.0010  0.1533\n",
      "     21       \u001b[36m60.1741\u001b[0m       \u001b[32m56.8194\u001b[0m  0.0010  0.1802\n",
      "     21       55.6374       \u001b[32m48.8391\u001b[0m  0.0010  0.1562\n",
      "     22       \u001b[36m56.0754\u001b[0m       \u001b[32m53.5020\u001b[0m  0.0010  0.1660\n",
      "     22       \u001b[36m55.7363\u001b[0m       \u001b[32m46.6943\u001b[0m  0.0010  0.1623\n",
      "     22       \u001b[36m59.0961\u001b[0m       \u001b[32m56.6719\u001b[0m  0.0010  0.1666\n",
      "     22       \u001b[36m52.9393\u001b[0m       \u001b[32m47.0057\u001b[0m  0.0010  0.1611\n",
      "     23       \u001b[36m55.5957\u001b[0m       \u001b[32m52.9544\u001b[0m  0.0010  0.1837\n",
      "     23       \u001b[36m54.1602\u001b[0m       47.2085  0.0010  0.1753\n",
      "     23       \u001b[36m57.6843\u001b[0m       \u001b[32m54.0940\u001b[0m  0.0010  0.1724\n",
      "     23       \u001b[36m51.6670\u001b[0m       \u001b[32m45.4037\u001b[0m  0.0010  0.1782\n",
      "     24       \u001b[36m54.4859\u001b[0m       \u001b[32m50.7943\u001b[0m  0.0010  0.1870\n",
      "     24       54.4071       \u001b[32m45.8714\u001b[0m  0.0010  0.1932\n",
      "     24       \u001b[36m57.0095\u001b[0m       \u001b[32m51.5999\u001b[0m  0.0010  0.1897\n",
      "     24       51.8415       \u001b[32m44.7329\u001b[0m  0.0010  0.1778\n",
      "     25       \u001b[36m52.9936\u001b[0m       \u001b[32m50.4975\u001b[0m  0.0010  0.1891\n",
      "     25       \u001b[36m50.5200\u001b[0m       \u001b[32m45.2596\u001b[0m  0.0010  0.1714\n",
      "     25       \u001b[36m56.9661\u001b[0m       \u001b[32m50.4425\u001b[0m  0.0010  0.1705\n",
      "     25       \u001b[36m49.1308\u001b[0m       \u001b[32m43.1045\u001b[0m  0.0010  0.1960\n",
      "     26       \u001b[36m52.5116\u001b[0m       \u001b[32m50.0661\u001b[0m  0.0010  0.1829\n",
      "     26       51.7186       \u001b[32m43.9685\u001b[0m  0.0010  0.1880\n",
      "     26       \u001b[36m55.7256\u001b[0m       \u001b[32m48.4717\u001b[0m  0.0010  0.1488\n",
      "     26       \u001b[36m49.0400\u001b[0m       \u001b[32m42.2864\u001b[0m  0.0010  0.1524\n",
      "     27       \u001b[36m50.0747\u001b[0m       \u001b[32m47.7447\u001b[0m  0.0010  0.1549\n",
      "     27       50.7924       \u001b[32m41.2047\u001b[0m  0.0010  0.1632\n",
      "     27       \u001b[36m53.2934\u001b[0m       \u001b[32m46.5343\u001b[0m  0.0010  0.1708\n",
      "     27       \u001b[36m47.5508\u001b[0m       \u001b[32m40.6617\u001b[0m  0.0010  0.1592\n",
      "     28       50.9051       \u001b[32m46.7475\u001b[0m  0.0010  0.1876\n",
      "     28       \u001b[36m49.9299\u001b[0m       \u001b[32m39.7776\u001b[0m  0.0010  0.2174\n",
      "     28       54.3907       47.8208  0.0010  0.2089\n",
      "     28       \u001b[36m46.5357\u001b[0m       \u001b[32m40.5105\u001b[0m  0.0010  0.2396\n",
      "     29       \u001b[36m48.6945\u001b[0m       \u001b[32m46.2578\u001b[0m  0.0010  0.2866\n",
      "     29       \u001b[36m52.1907\u001b[0m       \u001b[32m45.3671\u001b[0m  0.0010  0.2741\n",
      "     29       \u001b[36m47.8997\u001b[0m       40.3416  0.0010  0.3258\n",
      "     29       \u001b[36m46.1008\u001b[0m       \u001b[32m39.6233\u001b[0m  0.0010  0.3147\n",
      "     30       48.9567       46.2679  0.0010  0.2290\n",
      "     30       \u001b[36m51.2723\u001b[0m       47.7124  0.0010  0.2146\n",
      "     30       48.5009       40.1805  0.0010  0.1895\n",
      "     30       \u001b[36m44.7596\u001b[0m       \u001b[32m39.3643\u001b[0m  0.0010  0.1612\n",
      "     31       \u001b[36m48.5779\u001b[0m       \u001b[32m44.9361\u001b[0m  0.0010  0.1351\n",
      "     31       \u001b[36m44.6895\u001b[0m       \u001b[32m36.4491\u001b[0m  0.0010  0.1332\n",
      "     31       \u001b[36m50.8956\u001b[0m       \u001b[32m44.3516\u001b[0m  0.0010  0.1563\n",
      "     31       45.7418       \u001b[32m38.2823\u001b[0m  0.0010  0.1293\n",
      "     32       \u001b[36m46.2285\u001b[0m       \u001b[32m43.7669\u001b[0m  0.0010  0.1510\n",
      "     32       45.4956       36.6812  0.0010  0.1493\n",
      "     32       \u001b[36m49.8453\u001b[0m       \u001b[32m42.0592\u001b[0m  0.0010  0.1513\n",
      "     32       \u001b[36m44.2544\u001b[0m       38.2964  0.0010  0.1626\n",
      "     33       \u001b[36m46.0019\u001b[0m       43.7700  0.0010  0.1465\n",
      "     33       \u001b[36m48.9086\u001b[0m       \u001b[32m40.5137\u001b[0m  0.0010  0.1491\n",
      "     33       \u001b[36m44.0003\u001b[0m       \u001b[32m35.1665\u001b[0m  0.0010  0.1648\n",
      "     33       \u001b[36m43.5442\u001b[0m       \u001b[32m38.0120\u001b[0m  0.0010  0.1357\n",
      "     34       \u001b[36m45.6334\u001b[0m       \u001b[32m42.7344\u001b[0m  0.0010  0.1599\n",
      "     34       \u001b[36m47.1429\u001b[0m       \u001b[32m39.9466\u001b[0m  0.0010  0.1359\n",
      "     34       44.6753       36.4461  0.0010  0.1537\n",
      "     34       \u001b[36m42.3387\u001b[0m       38.2294  0.0010  0.1532\n",
      "     35       \u001b[36m44.4813\u001b[0m       \u001b[32m42.5161\u001b[0m  0.0010  0.1531\n",
      "     35       47.5853       40.8610  0.0010  0.1736\n",
      "     35       \u001b[36m42.8216\u001b[0m       35.3986  0.0010  0.1805\n",
      "     35       \u001b[36m41.4564\u001b[0m       \u001b[32m36.0448\u001b[0m  0.0010  0.1922\n",
      "     36       \u001b[36m43.3776\u001b[0m       \u001b[32m41.1870\u001b[0m  0.0010  0.2250\n",
      "     36       \u001b[36m46.3680\u001b[0m       \u001b[32m38.4598\u001b[0m  0.0010  0.2170\n",
      "     36       43.1977       \u001b[32m33.1969\u001b[0m  0.0010  0.2136\n",
      "     36       42.4439       \u001b[32m35.7417\u001b[0m  0.0010  0.1932\n",
      "     37       43.6871       41.4539  0.0010  0.1878\n",
      "     37       \u001b[36m45.5339\u001b[0m       \u001b[32m37.3425\u001b[0m  0.0010  0.1967\n",
      "     37       43.1418       33.8351  0.0010  0.1950\n",
      "     37       42.5568       36.5283  0.0010  0.2013\n",
      "     38       \u001b[36m43.0966\u001b[0m       \u001b[32m39.5280\u001b[0m  0.0010  0.2500\n",
      "     38       46.0504       38.0131  0.0010  0.2835\n",
      "     38       \u001b[36m40.4556\u001b[0m       \u001b[32m35.6393\u001b[0m  0.0010  0.2736\n",
      "     38       \u001b[36m42.3406\u001b[0m       \u001b[32m33.1029\u001b[0m  0.0010  0.3081\n",
      "     39       \u001b[36m41.7230\u001b[0m       \u001b[32m39.2053\u001b[0m  0.0010  0.3210\n",
      "     39       45.7100       \u001b[32m37.2117\u001b[0m  0.0010  0.3014\n",
      "     39       41.4228       \u001b[32m34.0947\u001b[0m  0.0010  0.3065\n",
      "     39       \u001b[36m41.7878\u001b[0m       \u001b[32m32.4954\u001b[0m  0.0010  0.2966\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m468.5498\u001b[0m      \u001b[32m408.4369\u001b[0m  0.0010  0.2681\n",
      "     40       41.9426       39.6841  0.0010  0.2566\n",
      "     40       \u001b[36m44.7945\u001b[0m       \u001b[32m35.7422\u001b[0m  0.0010  0.2142\n",
      "     40       40.4836       34.7981  0.0010  0.2180\n",
      "     40       \u001b[36m41.0639\u001b[0m       \u001b[32m31.4211\u001b[0m  0.0010  0.2346\n",
      "      2      \u001b[36m292.3131\u001b[0m      \u001b[32m186.9168\u001b[0m  0.0010  0.2017\n",
      "     41       \u001b[36m41.1121\u001b[0m       39.2117  0.0010  0.1961\n",
      "     41       \u001b[36m43.4952\u001b[0m       36.4933  0.0010  0.1930\n",
      "     41       \u001b[36m39.2597\u001b[0m       31.6786  0.0010  0.1678\n",
      "     41       \u001b[36m39.7383\u001b[0m       \u001b[32m33.5893\u001b[0m  0.0010  0.1922\n",
      "      3      \u001b[36m168.0908\u001b[0m      \u001b[32m135.6262\u001b[0m  0.0010  0.1754\n",
      "     42       \u001b[36m40.1559\u001b[0m       \u001b[32m39.1401\u001b[0m  0.0010  0.1743\n",
      "     42       \u001b[36m42.9940\u001b[0m       \u001b[32m35.3370\u001b[0m  0.0010  0.1699\n",
      "     42       \u001b[36m39.3806\u001b[0m       33.6830  0.0010  0.1528\n",
      "     42       39.5733       \u001b[32m30.1385\u001b[0m  0.0010  0.1625\n",
      "      4      \u001b[36m143.0567\u001b[0m      \u001b[32m124.0944\u001b[0m  0.0010  0.1596\n",
      "     43       40.2162       \u001b[32m37.0910\u001b[0m  0.0010  0.1673\n",
      "     43       43.4305       \u001b[32m33.6697\u001b[0m  0.0010  0.1526\n",
      "     43       39.7689       30.8852  0.0010  0.1641\n",
      "     43       39.3896       \u001b[32m32.7314\u001b[0m  0.0010  0.1781\n",
      "      5      \u001b[36m127.1353\u001b[0m      \u001b[32m110.6417\u001b[0m  0.0010  0.1780\n",
      "     44       \u001b[36m42.2623\u001b[0m       \u001b[32m31.9504\u001b[0m  0.0010  0.1721\n",
      "     44       40.7295       37.7701  0.0010  0.1825\n",
      "     44       \u001b[36m38.8494\u001b[0m       \u001b[32m29.7545\u001b[0m  0.0010  0.1798\n",
      "     44       39.5030       33.3782  0.0010  0.1852\n",
      "      6      \u001b[36m111.9398\u001b[0m       \u001b[32m98.2427\u001b[0m  0.0010  0.1850\n",
      "     45       \u001b[36m40.9462\u001b[0m       32.7902  0.0010  0.1773\n",
      "     45       \u001b[36m38.7155\u001b[0m       37.6105  0.0010  0.1944\n",
      "     45       \u001b[36m37.4526\u001b[0m       29.8463  0.0010  0.1769\n",
      "     45       \u001b[36m38.6797\u001b[0m       \u001b[32m32.6751\u001b[0m  0.0010  0.1807\n",
      "      7      \u001b[36m101.9974\u001b[0m       \u001b[32m89.9708\u001b[0m  0.0010  0.1521\n",
      "     46       42.1643       33.4944  0.0010  0.1682\n",
      "     46       \u001b[36m38.1791\u001b[0m       \u001b[32m35.8377\u001b[0m  0.0010  0.1697\n",
      "     46       38.0287       \u001b[32m29.6451\u001b[0m  0.0010  0.1817\n",
      "     46       39.0324       \u001b[32m32.3307\u001b[0m  0.0010  0.1690\n",
      "      8       \u001b[36m94.4617\u001b[0m       \u001b[32m83.4089\u001b[0m  0.0010  0.1673\n",
      "     47       41.9493       34.0714  0.0010  0.1712\n",
      "     47       \u001b[36m36.8327\u001b[0m       \u001b[32m34.8359\u001b[0m  0.0010  0.1617\n",
      "     47       39.5106       32.4867  0.0010  0.1651\n",
      "     47       37.9470       \u001b[32m29.2444\u001b[0m  0.0010  0.1813\n",
      "      9       \u001b[36m89.8315\u001b[0m       \u001b[32m77.7684\u001b[0m  0.0010  0.1681\n",
      "     48       41.3461       33.0542  0.0010  0.1627\n",
      "     48       37.7638       35.9045  0.0010  0.1884\n",
      "     48       \u001b[36m37.9303\u001b[0m       32.4991  0.0010  0.1743\n",
      "     48       37.6996       \u001b[32m27.9845\u001b[0m  0.0010  0.1864\n",
      "     10       \u001b[36m84.3509\u001b[0m       \u001b[32m74.1062\u001b[0m  0.0010  0.1677\n",
      "     49       37.7109       \u001b[32m34.2528\u001b[0m  0.0010  0.1753\n",
      "     49       \u001b[36m37.4916\u001b[0m       \u001b[32m31.8437\u001b[0m  0.0010  0.1906\n",
      "     49       \u001b[36m37.1612\u001b[0m       28.5849  0.0010  0.1970\n",
      "     11       \u001b[36m81.1607\u001b[0m       \u001b[32m70.3452\u001b[0m  0.0010  0.1943\n",
      "     50       37.2929       36.0082  0.0010  0.2056\n",
      "     50       \u001b[36m36.7671\u001b[0m       \u001b[32m31.2185\u001b[0m  0.0010  0.1935\n",
      "     50       \u001b[36m36.0243\u001b[0m       \u001b[32m27.3074\u001b[0m  0.0010  0.2088\n",
      "     12       \u001b[36m78.7741\u001b[0m       \u001b[32m66.5586\u001b[0m  0.0010  0.2099\n",
      "     51       37.4081       34.4531  0.0010  0.2039\n",
      "     51       38.6344       31.6138  0.0010  0.2076\n",
      "     51       \u001b[36m35.3947\u001b[0m       27.8954  0.0010  0.1963\n",
      "     13       \u001b[36m74.9458\u001b[0m       \u001b[32m62.7063\u001b[0m  0.0010  0.1958\n",
      "     52       \u001b[36m36.5783\u001b[0m       34.6092  0.0010  0.1691\n",
      "     52       36.9921       \u001b[32m30.9252\u001b[0m  0.0010  0.1640\n",
      "     52       35.9655       27.8424  0.0010  0.2338\n",
      "     14       \u001b[36m72.2015\u001b[0m       \u001b[32m60.8781\u001b[0m  0.0010  0.2059\n",
      "     53       37.0610       \u001b[32m34.0888\u001b[0m  0.0010  0.2353\n",
      "     53       36.7729       \u001b[32m30.6695\u001b[0m  0.0010  0.2334\n",
      "     15       \u001b[36m69.9915\u001b[0m       \u001b[32m58.3484\u001b[0m  0.0010  0.1458\n",
      "     53       36.5959       \u001b[32m26.0225\u001b[0m  0.0010  0.1674\n",
      "     54       36.6518       \u001b[32m33.5871\u001b[0m  0.0010  0.1523\n",
      "     54       \u001b[36m35.3248\u001b[0m       \u001b[32m29.7642\u001b[0m  0.0010  0.1766\n",
      "     16       \u001b[36m66.6953\u001b[0m       \u001b[32m56.6953\u001b[0m  0.0010  0.1541\n",
      "     54       \u001b[36m34.2798\u001b[0m       26.2864  0.0010  0.1547\n",
      "     55       \u001b[36m34.3061\u001b[0m       34.1641  0.0010  0.1695\n",
      "     17       \u001b[36m66.2284\u001b[0m       \u001b[32m53.5257\u001b[0m  0.0010  0.1622\n",
      "     55       \u001b[36m35.3102\u001b[0m       \u001b[32m29.4112\u001b[0m  0.0010  0.1725\n",
      "     55       34.7522       26.1541  0.0010  0.1695\n",
      "     56       35.6681       \u001b[32m33.0526\u001b[0m  0.0010  0.1666\n",
      "     18       \u001b[36m63.4804\u001b[0m       \u001b[32m52.0656\u001b[0m  0.0010  0.1524\n",
      "     56       36.0452       \u001b[32m29.2031\u001b[0m  0.0010  0.1682\n",
      "     56       \u001b[36m34.1345\u001b[0m       \u001b[32m25.8957\u001b[0m  0.0010  0.1560\n",
      "     57       35.7473       \u001b[32m32.7505\u001b[0m  0.0010  0.1441\n",
      "     19       \u001b[36m62.4855\u001b[0m       \u001b[32m50.9968\u001b[0m  0.0010  0.1235\n",
      "     57       36.2102       \u001b[32m28.3692\u001b[0m  0.0010  0.1250\n",
      "     57       \u001b[36m33.9796\u001b[0m       \u001b[32m25.4485\u001b[0m  0.0010  0.1472\n",
      "     58       34.5764       32.8832  0.0010  0.1549\n",
      "     20       \u001b[36m61.0571\u001b[0m       \u001b[32m50.1309\u001b[0m  0.0010  0.1496\n",
      "     58       35.9540       \u001b[32m28.0019\u001b[0m  0.0010  0.1310\n",
      "     58       \u001b[36m33.7719\u001b[0m       26.0832  0.0010  0.1274\n",
      "     21       \u001b[36m59.5836\u001b[0m       \u001b[32m48.8635\u001b[0m  0.0010  0.1193\n",
      "     59       \u001b[36m33.5222\u001b[0m       \u001b[32m30.8807\u001b[0m  0.0010  0.1386\n",
      "     59       36.0521       28.4036  0.0010  0.1335\n",
      "     59       34.3869       26.0392  0.0010  0.1666\n",
      "     60       \u001b[36m33.2212\u001b[0m       32.1273  0.0010  0.1263\n",
      "     60       \u001b[36m34.6720\u001b[0m       28.4315  0.0010  0.1202\n",
      "     22       \u001b[36m58.2133\u001b[0m       \u001b[32m47.0014\u001b[0m  0.0010  0.1392\n",
      "     60       33.9896       26.0826  0.0010  0.1398\n",
      "     61       34.0690       32.6049  0.0010  0.1126\n",
      "     23       \u001b[36m56.8220\u001b[0m       \u001b[32m46.7045\u001b[0m  0.0010  0.1286\n",
      "     61       \u001b[36m32.7611\u001b[0m       \u001b[32m27.5128\u001b[0m  0.0010  0.1300\n",
      "     61       \u001b[36m32.1151\u001b[0m       25.8601  0.0010  0.1118\n",
      "     62       34.7562       30.9751  0.0010  0.1175\n",
      "     62       33.7660       \u001b[32m26.9689\u001b[0m  0.0010  0.1190\n",
      "     24       \u001b[36m55.3955\u001b[0m       \u001b[32m44.5348\u001b[0m  0.0010  0.1507\n",
      "     62       32.5640       \u001b[32m25.0064\u001b[0m  0.0005  0.1306\n",
      "     63       \u001b[36m32.5602\u001b[0m       31.7092  0.0010  0.1189\n",
      "     63       33.8221       27.6770  0.0010  0.1032\n",
      "     25       \u001b[36m54.8782\u001b[0m       \u001b[32m42.8023\u001b[0m  0.0010  0.1201\n",
      "     63       32.2043       \u001b[32m24.6053\u001b[0m  0.0005  0.1211\n",
      "     64       32.9555       \u001b[32m30.5930\u001b[0m  0.0005  0.0968\n",
      "     64       32.8750       27.4281  0.0010  0.0967\n",
      "     26       \u001b[36m54.2205\u001b[0m       \u001b[32m42.5550\u001b[0m  0.0010  0.1342\n",
      "     64       \u001b[36m31.9101\u001b[0m       \u001b[32m24.0354\u001b[0m  0.0005  0.1022\n",
      "     65       33.9511       \u001b[32m26.4844\u001b[0m  0.0010  0.1207\n",
      "     65       32.7308       30.8765  0.0005  0.1342\n",
      "     65       32.6066       24.4954  0.0005  0.1182\n",
      "     27       \u001b[36m53.1317\u001b[0m       \u001b[32m40.9866\u001b[0m  0.0010  0.1360\n",
      "     66       32.8706       \u001b[32m26.3237\u001b[0m  0.0010  0.1154\n",
      "     66       33.2966       \u001b[32m30.2121\u001b[0m  0.0005  0.1473\n",
      "     66       \u001b[36m31.9023\u001b[0m       \u001b[32m24.0202\u001b[0m  0.0005  0.1312\n",
      "     28       \u001b[36m51.0169\u001b[0m       \u001b[32m39.6819\u001b[0m  0.0010  0.1448\n",
      "     67       33.8781       \u001b[32m26.2974\u001b[0m  0.0010  0.1364\n",
      "     67       \u001b[36m31.7151\u001b[0m       30.7529  0.0005  0.1199\n",
      "     67       33.0802       \u001b[32m23.9462\u001b[0m  0.0005  0.1357\n",
      "     29       \u001b[36m50.4898\u001b[0m       \u001b[32m37.8108\u001b[0m  0.0010  0.1184\n",
      "     68       33.6160       \u001b[32m26.1972\u001b[0m  0.0010  0.1467\n",
      "     68       32.4299       30.3436  0.0005  0.1304\n",
      "     68       \u001b[36m31.6497\u001b[0m       \u001b[32m23.7116\u001b[0m  0.0005  0.1178\n",
      "     30       \u001b[36m48.7612\u001b[0m       \u001b[32m35.9028\u001b[0m  0.0010  0.1260\n",
      "     69       31.8410       \u001b[32m29.7039\u001b[0m  0.0005  0.1235\n",
      "     69       32.8346       26.7523  0.0010  0.1374\n",
      "     69       32.0440       \u001b[32m23.1259\u001b[0m  0.0005  0.1381\n",
      "     31       \u001b[36m47.8151\u001b[0m       \u001b[32m35.2670\u001b[0m  0.0010  0.1314\n",
      "     70       31.9921       \u001b[32m29.6186\u001b[0m  0.0005  0.1310\n",
      "     70       33.3432       \u001b[32m26.1685\u001b[0m  0.0010  0.1611\n",
      "     32       \u001b[36m47.3360\u001b[0m       \u001b[32m34.8027\u001b[0m  0.0010  0.1715\n",
      "     70       32.0918       \u001b[32m23.0856\u001b[0m  0.0005  0.1829\n",
      "     71       32.6863       29.7575  0.0005  0.1631\n",
      "     71       \u001b[36m31.4100\u001b[0m       \u001b[32m25.7570\u001b[0m  0.0010  0.1588\n",
      "     33       \u001b[36m46.1480\u001b[0m       \u001b[32m33.6731\u001b[0m  0.0010  0.1726\n",
      "     71       \u001b[36m31.2376\u001b[0m       23.5218  0.0005  0.1771\n",
      "     72       32.2658       30.0593  0.0005  0.1619\n",
      "     72       32.6527       \u001b[32m25.6916\u001b[0m  0.0010  0.1701\n",
      "     34       \u001b[36m45.7984\u001b[0m       \u001b[32m33.0370\u001b[0m  0.0010  0.1283\n",
      "     72       \u001b[36m31.0903\u001b[0m       23.0882  0.0005  0.1564\n",
      "     73       \u001b[36m31.2664\u001b[0m       29.9645  0.0005  0.1487\n",
      "     73       32.2102       \u001b[32m25.4449\u001b[0m  0.0010  0.1410\n",
      "     35       \u001b[36m44.2988\u001b[0m       \u001b[32m31.8244\u001b[0m  0.0010  0.1430\n",
      "     73       32.0186       23.2683  0.0005  0.1353\n",
      "     74       31.6156       29.8978  0.0005  0.1393\n",
      "     74       \u001b[36m31.0459\u001b[0m       \u001b[32m24.8436\u001b[0m  0.0010  0.1413\n",
      "     36       \u001b[36m43.8839\u001b[0m       \u001b[32m30.5939\u001b[0m  0.0010  0.1517\n",
      "     74       31.9759       \u001b[32m22.6051\u001b[0m  0.0005  0.1634\n",
      "     75       32.8688       25.4499  0.0010  0.1532\n",
      "     37       \u001b[36m42.7906\u001b[0m       \u001b[32m30.2889\u001b[0m  0.0010  0.1587\n",
      "     75       \u001b[36m30.7880\u001b[0m       \u001b[32m22.5096\u001b[0m  0.0005  0.1370\n",
      "     76       \u001b[36m30.7781\u001b[0m       25.3481  0.0010  0.1613\n",
      "     38       \u001b[36m42.3121\u001b[0m       \u001b[32m29.2392\u001b[0m  0.0010  0.1617\n",
      "     76       31.8749       22.5159  0.0005  0.1589\n",
      "     77       31.6226       24.9569  0.0010  0.1589\n",
      "     39       \u001b[36m40.8964\u001b[0m       \u001b[32m28.2940\u001b[0m  0.0010  0.1409\n",
      "     77       \u001b[36m30.5912\u001b[0m       23.1369  0.0005  0.1465\n",
      "     78       30.9283       \u001b[32m24.6840\u001b[0m  0.0010  0.1618\n",
      "     40       \u001b[36m40.1297\u001b[0m       \u001b[32m27.7841\u001b[0m  0.0010  0.1273\n",
      "     78       31.3561       22.6603  0.0005  0.1412\n",
      "     79       30.9160       24.7499  0.0010  0.1416\n",
      "     41       \u001b[36m40.0745\u001b[0m       \u001b[32m27.7768\u001b[0m  0.0010  0.1419\n",
      "     79       31.8028       \u001b[32m22.2651\u001b[0m  0.0005  0.1451\n",
      "     80       31.6606       24.8215  0.0010  0.1351\n",
      "     42       \u001b[36m38.9649\u001b[0m       \u001b[32m27.1286\u001b[0m  0.0010  0.1345\n",
      "     80       \u001b[36m30.5386\u001b[0m       \u001b[32m21.9660\u001b[0m  0.0005  0.1250\n",
      "     81       \u001b[36m29.9573\u001b[0m       \u001b[32m24.4490\u001b[0m  0.0010  0.1299\n",
      "     43       39.5009       27.3992  0.0010  0.1318\n",
      "     81       \u001b[36m30.3032\u001b[0m       22.3678  0.0005  0.1357\n",
      "     82       \u001b[36m29.5350\u001b[0m       \u001b[32m24.2200\u001b[0m  0.0010  0.1303\n",
      "     44       \u001b[36m38.1278\u001b[0m       \u001b[32m26.7457\u001b[0m  0.0010  0.1447\n",
      "     82       \u001b[36m30.2413\u001b[0m       \u001b[32m21.9494\u001b[0m  0.0005  0.1497\n",
      "     83       30.4352       \u001b[32m24.0665\u001b[0m  0.0010  0.1434\n",
      "     45       \u001b[36m37.8998\u001b[0m       \u001b[32m25.8687\u001b[0m  0.0010  0.1170\n",
      "     83       30.6393       22.0529  0.0005  0.1360\n",
      "     84       30.7032       \u001b[32m23.7332\u001b[0m  0.0010  0.1189\n",
      "     46       \u001b[36m37.4011\u001b[0m       25.8802  0.0010  0.1337\n",
      "     84       \u001b[36m30.1006\u001b[0m       \u001b[32m21.4636\u001b[0m  0.0005  0.1324\n",
      "     85       \u001b[36m29.3689\u001b[0m       \u001b[32m23.7040\u001b[0m  0.0010  0.1228\n",
      "     47       \u001b[36m37.0053\u001b[0m       \u001b[32m24.8935\u001b[0m  0.0010  0.1268\n",
      "     85       \u001b[36m29.1722\u001b[0m       21.9943  0.0005  0.1195\n",
      "     86       30.7833       \u001b[32m23.4855\u001b[0m  0.0010  0.1263\n",
      "     48       38.0750       \u001b[32m24.5770\u001b[0m  0.0010  0.1194\n",
      "     86       \u001b[36m29.0466\u001b[0m       21.9298  0.0005  0.1191\n",
      "     87       30.7768       23.7116  0.0010  0.1127\n",
      "     49       \u001b[36m36.6983\u001b[0m       24.6014  0.0010  0.0972\n",
      "     87       30.6968       \u001b[32m21.2803\u001b[0m  0.0005  0.0991\n",
      "     88       29.9687       \u001b[32m22.6453\u001b[0m  0.0010  0.1035\n",
      "     50       36.7768       \u001b[32m23.9993\u001b[0m  0.0010  0.1086\n",
      "     88       29.7476       \u001b[32m21.2392\u001b[0m  0.0005  0.1049\n",
      "     89       29.6642       22.8184  0.0010  0.1055\n",
      "     51       \u001b[36m35.3731\u001b[0m       \u001b[32m23.4815\u001b[0m  0.0010  0.0997\n",
      "     89       29.7587       21.2987  0.0005  0.1081\n",
      "     90       \u001b[36m29.0187\u001b[0m       \u001b[32m22.5777\u001b[0m  0.0010  0.1235\n",
      "     52       36.0079       23.6850  0.0010  0.1383\n",
      "     90       29.7213       \u001b[32m20.9785\u001b[0m  0.0005  0.1468\n",
      "     91       30.0223       22.7483  0.0010  0.1482\n",
      "     53       35.8974       23.7970  0.0010  0.1346\n",
      "     91       29.6394       20.9791  0.0005  0.2060\n",
      "     92       29.3696       \u001b[32m22.1870\u001b[0m  0.0010  0.2111\n",
      "     54       \u001b[36m35.2981\u001b[0m       \u001b[32m23.3252\u001b[0m  0.0010  0.2041\n",
      "     92       31.1579       \u001b[32m20.7615\u001b[0m  0.0005  0.1290\n",
      "     93       \u001b[36m28.4008\u001b[0m       22.2255  0.0010  0.1099\n",
      "     55       35.7439       \u001b[32m23.1446\u001b[0m  0.0010  0.1214\n",
      "     93       30.2580       20.9685  0.0005  0.1218\n",
      "     94       29.6287       \u001b[32m22.1699\u001b[0m  0.0010  0.1111\n",
      "     56       \u001b[36m34.1678\u001b[0m       23.1617  0.0010  0.1119\n",
      "     95       28.5251       \u001b[32m21.8679\u001b[0m  0.0010  0.0931\n",
      "     94       \u001b[36m28.8882\u001b[0m       21.2759  0.0005  0.1267\n",
      "     57       34.7451       \u001b[32m22.8079\u001b[0m  0.0010  0.1338\n",
      "     96       \u001b[36m27.9411\u001b[0m       21.9433  0.0010  0.1087\n",
      "     95       29.4501       20.8794  0.0005  0.1188\n",
      "     58       \u001b[36m34.0622\u001b[0m       23.0477  0.0010  0.1065\n",
      "     97       28.6717       \u001b[32m21.4359\u001b[0m  0.0010  0.0955\n",
      "     96       29.3370       20.8500  0.0005  0.1112\n",
      "     59       \u001b[36m33.7563\u001b[0m       \u001b[32m22.3174\u001b[0m  0.0010  0.0983\n",
      "     98       28.2903       21.8436  0.0010  0.0984\n",
      "     60       \u001b[36m33.3450\u001b[0m       22.7316  0.0010  0.0935\n",
      "     99       28.9704       \u001b[32m21.3044\u001b[0m  0.0010  0.1017\n",
      "     61       33.5895       \u001b[32m22.0375\u001b[0m  0.0010  0.1092\n",
      "    100       28.8191       21.5585  0.0010  0.1027\n",
      "     62       33.3794       \u001b[32m21.9093\u001b[0m  0.0010  0.0910\n",
      "    101       28.3994       21.5453  0.0010  0.0977\n",
      "     63       34.0394       22.4327  0.0010  0.1062\n",
      "    102       \u001b[36m27.3919\u001b[0m       21.3220  0.0010  0.1078\n",
      "     64       34.7207       21.9303  0.0010  0.0967\n",
      "    103       27.7580       \u001b[32m21.1726\u001b[0m  0.0010  0.0923\n",
      "     65       33.3464       \u001b[32m21.5805\u001b[0m  0.0010  0.0814\n",
      "    104       28.0716       21.1904  0.0010  0.0917\n",
      "     66       \u001b[36m32.9345\u001b[0m       21.7071  0.0010  0.1152\n",
      "    105       27.8945       21.4826  0.0010  0.1257\n",
      "     67       \u001b[36m32.4982\u001b[0m       \u001b[32m21.1198\u001b[0m  0.0010  0.1067\n",
      "    106       27.7973       \u001b[32m20.8163\u001b[0m  0.0010  0.0888\n",
      "     68       \u001b[36m31.9368\u001b[0m       21.4481  0.0010  0.0960\n",
      "    107       \u001b[36m26.4170\u001b[0m       20.8965  0.0010  0.0969\n",
      "     69       31.9617       21.2290  0.0010  0.0950\n",
      "    108       27.4254       \u001b[32m20.7361\u001b[0m  0.0010  0.0930\n",
      "     70       \u001b[36m31.8770\u001b[0m       21.2223  0.0010  0.1051\n",
      "    109       27.8306       \u001b[32m20.3444\u001b[0m  0.0010  0.1049\n",
      "     71       \u001b[36m31.2866\u001b[0m       \u001b[32m20.6760\u001b[0m  0.0010  0.0951\n",
      "    110       27.8150       21.0276  0.0010  0.1088\n",
      "     72       32.6026       \u001b[32m20.5260\u001b[0m  0.0010  0.1126\n",
      "    111       27.7234       20.8830  0.0010  0.0966\n",
      "     73       \u001b[36m31.2478\u001b[0m       \u001b[32m20.3234\u001b[0m  0.0010  0.0878\n",
      "    112       27.7264       20.5332  0.0010  0.0883\n",
      "     74       31.9939       20.5805  0.0010  0.0857\n",
      "    113       27.9530       21.2237  0.0010  0.0838\n",
      "     75       \u001b[36m30.7430\u001b[0m       \u001b[32m19.5972\u001b[0m  0.0010  0.0857\n",
      "    114       26.8273       \u001b[32m20.1767\u001b[0m  0.0005  0.0915\n",
      "     76       \u001b[36m29.9084\u001b[0m       20.1258  0.0010  0.0941\n",
      "    115       \u001b[36m26.2975\u001b[0m       \u001b[32m20.1012\u001b[0m  0.0005  0.0846\n",
      "     77       30.3414       19.6831  0.0010  0.0853\n",
      "    116       \u001b[36m25.4929\u001b[0m       \u001b[32m19.8470\u001b[0m  0.0005  0.0897\n",
      "     78       31.3213       20.1895  0.0010  0.0887\n",
      "    117       26.5072       19.9789  0.0005  0.0920\n",
      "     79       30.7362       19.6198  0.0010  0.0894\n",
      "    118       26.0192       20.2128  0.0005  0.0883\n",
      "    119       \u001b[36m25.3389\u001b[0m       19.9641  0.0005  0.0980\n",
      "    120       26.4684       \u001b[32m19.8331\u001b[0m  0.0005  0.0898\n",
      "    121       26.1366       20.2293  0.0005  0.0937\n",
      "    122       25.5860       \u001b[32m19.8039\u001b[0m  0.0005  0.1053\n",
      "    123       26.7713       20.1155  0.0005  0.0858\n",
      "    124       25.8696       \u001b[32m19.7042\u001b[0m  0.0005  0.0871\n",
      "    125       25.5229       \u001b[32m19.6411\u001b[0m  0.0005  0.0697\n",
      "    126       25.7674       19.7442  0.0005  0.0675\n",
      "    127       25.3711       20.0991  0.0005  0.0658\n",
      "    128       25.8015       19.8576  0.0005  0.0643\n",
      "    129       25.7778       19.9897  0.0005  0.0653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m684.3059\u001b[0m      \u001b[32m466.5058\u001b[0m  0.0010  0.2138\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m699.2437\u001b[0m      \u001b[32m462.1062\u001b[0m  0.0010  0.1723\n",
      "      2      \u001b[36m386.1279\u001b[0m      \u001b[32m191.8075\u001b[0m  0.0010  0.1850\n",
      "      2      \u001b[36m378.1645\u001b[0m      \u001b[32m188.5837\u001b[0m  0.0010  0.1814\n",
      "      3      \u001b[36m177.8043\u001b[0m      \u001b[32m149.2203\u001b[0m  0.0010  0.1792\n",
      "      3      \u001b[36m184.3788\u001b[0m      \u001b[32m143.7077\u001b[0m  0.0010  0.1873\n",
      "      4      \u001b[36m158.2183\u001b[0m      \u001b[32m139.9855\u001b[0m  0.0010  0.1983\n",
      "      4      \u001b[36m162.6339\u001b[0m      \u001b[32m133.4469\u001b[0m  0.0010  0.1978\n",
      "      5      \u001b[36m149.8836\u001b[0m      \u001b[32m136.0705\u001b[0m  0.0010  0.1979\n",
      "      5      \u001b[36m157.2006\u001b[0m      \u001b[32m129.4709\u001b[0m  0.0010  0.1763\n",
      "      6      \u001b[36m143.8470\u001b[0m      \u001b[32m133.8415\u001b[0m  0.0010  0.2108\n",
      "      6      \u001b[36m148.4792\u001b[0m      \u001b[32m126.6627\u001b[0m  0.0010  0.2103\n",
      "      7      \u001b[36m141.4801\u001b[0m      \u001b[32m130.9163\u001b[0m  0.0010  0.1870\n",
      "      7      \u001b[36m144.3090\u001b[0m      \u001b[32m124.4513\u001b[0m  0.0010  0.2032\n",
      "      8      \u001b[36m140.3667\u001b[0m      \u001b[32m122.8313\u001b[0m  0.0010  0.1474\n",
      "      8      \u001b[36m135.4536\u001b[0m      \u001b[32m129.0300\u001b[0m  0.0010  0.1804\n",
      "      9      \u001b[36m137.5414\u001b[0m      \u001b[32m121.6991\u001b[0m  0.0010  0.1539\n",
      "      9      \u001b[36m132.1869\u001b[0m      \u001b[32m127.7392\u001b[0m  0.0010  0.1684\n",
      "     10      \u001b[36m135.5995\u001b[0m      \u001b[32m119.9732\u001b[0m  0.0010  0.1973\n",
      "     10      \u001b[36m128.7560\u001b[0m      \u001b[32m126.3056\u001b[0m  0.0010  0.2093\n",
      "     11      \u001b[36m132.0240\u001b[0m      \u001b[32m118.5750\u001b[0m  0.0010  0.2446\n",
      "     11      \u001b[36m127.0758\u001b[0m      \u001b[32m125.8815\u001b[0m  0.0010  0.2488\n",
      "     12      \u001b[36m129.1733\u001b[0m      \u001b[32m117.9486\u001b[0m  0.0010  0.2289\n",
      "     12      \u001b[36m124.1649\u001b[0m      \u001b[32m124.9580\u001b[0m  0.0010  0.2052\n",
      "     13      \u001b[36m121.6534\u001b[0m      \u001b[32m123.9189\u001b[0m  0.0010  0.1477\n",
      "     13      \u001b[36m129.0379\u001b[0m      \u001b[32m116.9465\u001b[0m  0.0010  0.1893\n",
      "     14      \u001b[36m120.4453\u001b[0m      \u001b[32m123.4982\u001b[0m  0.0010  0.2081\n",
      "     14      \u001b[36m125.2217\u001b[0m      \u001b[32m116.4205\u001b[0m  0.0010  0.2274\n",
      "     15      \u001b[36m119.4745\u001b[0m      \u001b[32m122.5436\u001b[0m  0.0010  0.2083\n",
      "     15      \u001b[36m123.3409\u001b[0m      \u001b[32m115.7558\u001b[0m  0.0010  0.1976\n",
      "     16      \u001b[36m116.8963\u001b[0m      \u001b[32m121.7517\u001b[0m  0.0010  0.1809\n",
      "     16      124.9727      \u001b[32m115.6425\u001b[0m  0.0010  0.1876\n",
      "     17      \u001b[36m114.9617\u001b[0m      \u001b[32m121.0647\u001b[0m  0.0010  0.1790\n",
      "     17      \u001b[36m122.8733\u001b[0m      \u001b[32m114.6125\u001b[0m  0.0010  0.1841\n",
      "     18      \u001b[36m114.5255\u001b[0m      \u001b[32m120.7048\u001b[0m  0.0010  0.1907\n",
      "     18      \u001b[36m121.5373\u001b[0m      114.7531  0.0010  0.1590\n",
      "     19      \u001b[36m121.1401\u001b[0m      114.6770  0.0010  0.1481\n",
      "     19      \u001b[36m112.8675\u001b[0m      121.2004  0.0010  0.1705\n",
      "     20      \u001b[36m119.2732\u001b[0m      114.7348  0.0010  0.2024\n",
      "     20      \u001b[36m112.3220\u001b[0m      \u001b[32m120.5638\u001b[0m  0.0010  0.2302\n",
      "     21      \u001b[36m118.5852\u001b[0m      114.8807  0.0010  0.2249\n",
      "     21      \u001b[36m111.6854\u001b[0m      \u001b[32m119.5876\u001b[0m  0.0010  0.2313\n",
      "     22      \u001b[36m117.7939\u001b[0m      \u001b[32m114.1667\u001b[0m  0.0005  0.1969\n",
      "     22      \u001b[36m109.4732\u001b[0m      \u001b[32m119.1210\u001b[0m  0.0010  0.2066\n",
      "     23      \u001b[36m117.1497\u001b[0m      \u001b[32m113.4376\u001b[0m  0.0005  0.1585\n",
      "     23      \u001b[36m109.1019\u001b[0m      \u001b[32m118.7265\u001b[0m  0.0010  0.1915\n",
      "     24      \u001b[36m116.2979\u001b[0m      \u001b[32m113.3476\u001b[0m  0.0005  0.1602\n",
      "     24      \u001b[36m107.9762\u001b[0m      119.1869  0.0010  0.1783\n",
      "     25      \u001b[36m115.5095\u001b[0m      \u001b[32m113.2913\u001b[0m  0.0005  0.1893\n",
      "     25      \u001b[36m107.3793\u001b[0m      \u001b[32m117.5918\u001b[0m  0.0010  0.1636\n",
      "     26      \u001b[36m114.0723\u001b[0m      \u001b[32m112.9953\u001b[0m  0.0005  0.1986\n",
      "     26      \u001b[36m106.2827\u001b[0m      119.0168  0.0010  0.1881\n",
      "     27      \u001b[36m113.5009\u001b[0m      \u001b[32m112.9109\u001b[0m  0.0005  0.1715\n",
      "     27      107.1680      117.9001  0.0010  0.1638\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m537.2861\u001b[0m      \u001b[32m442.6839\u001b[0m  0.0010  0.1618\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m637.9986\u001b[0m      \u001b[32m462.0808\u001b[0m  0.0010  0.1655\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m554.2759\u001b[0m      \u001b[32m772.9968\u001b[0m  0.0010  0.1732\n",
      "     28      114.6629      \u001b[32m112.8030\u001b[0m  0.0005  0.1630\n",
      "     28      108.7102      117.8566  0.0010  0.1841\n",
      "      2      \u001b[36m282.4923\u001b[0m      \u001b[32m268.8925\u001b[0m  0.0010  0.1582\n",
      "      2      \u001b[36m270.1103\u001b[0m      \u001b[32m189.8737\u001b[0m  0.0010  0.1818\n",
      "      2      \u001b[36m343.5159\u001b[0m      \u001b[32m194.7192\u001b[0m  0.0010  0.1784\n",
      "     29      113.5127      \u001b[32m112.5205\u001b[0m  0.0005  0.1560\n",
      "      3      \u001b[36m149.9521\u001b[0m      \u001b[32m154.4735\u001b[0m  0.0010  0.1433\n",
      "     29      \u001b[36m105.4791\u001b[0m      \u001b[32m117.3388\u001b[0m  0.0010  0.1984\n",
      "      3      \u001b[36m170.4361\u001b[0m      \u001b[32m150.6686\u001b[0m  0.0010  0.1680\n",
      "      3      \u001b[36m154.4842\u001b[0m      \u001b[32m203.9567\u001b[0m  0.0010  0.1792\n",
      "     30      \u001b[36m112.5500\u001b[0m      112.6327  0.0005  0.1659\n",
      "      4      \u001b[36m137.0560\u001b[0m      \u001b[32m141.9606\u001b[0m  0.0010  0.1595\n",
      "      4      \u001b[36m147.9831\u001b[0m      \u001b[32m140.3492\u001b[0m  0.0010  0.1612\n",
      "      4      \u001b[36m140.2490\u001b[0m      \u001b[32m192.8647\u001b[0m  0.0010  0.1622\n",
      "     30      106.5672      \u001b[32m116.9405\u001b[0m  0.0010  0.1833\n",
      "     31      \u001b[36m111.2611\u001b[0m      \u001b[32m112.3232\u001b[0m  0.0005  0.1785\n",
      "      5      \u001b[36m129.6926\u001b[0m      \u001b[32m136.1068\u001b[0m  0.0010  0.1491\n",
      "      5      \u001b[36m139.1696\u001b[0m      \u001b[32m135.0894\u001b[0m  0.0010  0.1624\n",
      "     31      \u001b[36m104.2667\u001b[0m      \u001b[32m116.5742\u001b[0m  0.0010  0.1672\n",
      "      5      \u001b[36m134.5955\u001b[0m      \u001b[32m185.7962\u001b[0m  0.0010  0.1933\n",
      "     32      112.6586      \u001b[32m111.9899\u001b[0m  0.0005  0.1752\n",
      "      6      \u001b[36m124.4679\u001b[0m      \u001b[32m131.7932\u001b[0m  0.0010  0.1717\n",
      "      6      \u001b[36m135.8092\u001b[0m      \u001b[32m131.8983\u001b[0m  0.0010  0.1789\n",
      "     32      \u001b[36m103.9508\u001b[0m      \u001b[32m116.5134\u001b[0m  0.0010  0.1666\n",
      "     33      112.0515      112.5377  0.0005  0.1469\n",
      "      6      \u001b[36m130.9354\u001b[0m      \u001b[32m178.8727\u001b[0m  0.0010  0.1748\n",
      "      7      \u001b[36m122.3565\u001b[0m      \u001b[32m128.9229\u001b[0m  0.0010  0.1637\n",
      "     33      104.1639      116.6192  0.0010  0.1569\n",
      "      7      \u001b[36m130.2053\u001b[0m      \u001b[32m129.0810\u001b[0m  0.0010  0.1649\n",
      "      7      \u001b[36m126.4601\u001b[0m      \u001b[32m170.4297\u001b[0m  0.0010  0.1482\n",
      "     34      111.3549      112.2557  0.0005  0.1789\n",
      "      8      \u001b[36m118.6932\u001b[0m      \u001b[32m126.9633\u001b[0m  0.0010  0.1783\n",
      "      8      \u001b[36m126.0840\u001b[0m      \u001b[32m127.4904\u001b[0m  0.0010  0.1588\n",
      "      8      \u001b[36m121.5247\u001b[0m      \u001b[32m162.5069\u001b[0m  0.0010  0.1518\n",
      "     34      104.2080      \u001b[32m115.9777\u001b[0m  0.0010  0.1899\n",
      "     35      113.8773      \u001b[32m111.7284\u001b[0m  0.0005  0.1498\n",
      "      9      \u001b[36m115.8047\u001b[0m      \u001b[32m125.0666\u001b[0m  0.0010  0.1386\n",
      "      9      \u001b[36m118.2624\u001b[0m      \u001b[32m157.4440\u001b[0m  0.0010  0.1475\n",
      "      9      \u001b[36m122.3216\u001b[0m      \u001b[32m125.5960\u001b[0m  0.0010  0.1803\n",
      "     35      \u001b[36m103.1037\u001b[0m      116.5025  0.0010  0.1676\n",
      "     36      112.0586      111.8621  0.0005  0.1809\n",
      "     10      \u001b[36m113.1849\u001b[0m      \u001b[32m123.1746\u001b[0m  0.0010  0.1533\n",
      "     10      \u001b[36m115.1075\u001b[0m      \u001b[32m153.8489\u001b[0m  0.0010  0.1696\n",
      "     10      \u001b[36m120.4021\u001b[0m      125.6761  0.0010  0.1807\n",
      "     36      103.8051      116.6557  0.0010  0.1925\n",
      "     37      112.1382      111.7680  0.0005  0.1997\n",
      "     11      \u001b[36m112.9640\u001b[0m      \u001b[32m122.4751\u001b[0m  0.0010  0.1813\n",
      "     11      \u001b[36m112.8387\u001b[0m      \u001b[32m150.3704\u001b[0m  0.0010  0.1996\n",
      "     11      \u001b[36m116.9378\u001b[0m      \u001b[32m124.6711\u001b[0m  0.0010  0.2021\n",
      "     37      \u001b[36m102.3592\u001b[0m      116.4625  0.0010  0.2371\n",
      "     38      111.3981      \u001b[32m111.2950\u001b[0m  0.0005  0.2221\n",
      "     12      \u001b[36m109.6608\u001b[0m      \u001b[32m121.4505\u001b[0m  0.0010  0.2135\n",
      "     12      \u001b[36m110.5942\u001b[0m      \u001b[32m147.4595\u001b[0m  0.0010  0.2374\n",
      "     12      \u001b[36m113.0369\u001b[0m      \u001b[32m123.7817\u001b[0m  0.0010  0.2225\n",
      "     39      112.3678      111.3956  0.0005  0.2016\n",
      "     38      102.4025      116.3310  0.0010  0.2156\n",
      "     13      \u001b[36m108.7070\u001b[0m      \u001b[32m120.3196\u001b[0m  0.0010  0.2134\n",
      "     13      \u001b[36m110.3655\u001b[0m      \u001b[32m145.1403\u001b[0m  0.0010  0.2001\n",
      "     13      \u001b[36m112.7249\u001b[0m      \u001b[32m123.3138\u001b[0m  0.0010  0.1882\n",
      "     40      \u001b[36m110.6304\u001b[0m      \u001b[32m111.1244\u001b[0m  0.0005  0.2081\n",
      "     14      \u001b[36m106.2712\u001b[0m      \u001b[32m118.7471\u001b[0m  0.0010  0.1845\n",
      "     14      \u001b[36m109.9080\u001b[0m      \u001b[32m143.7243\u001b[0m  0.0010  0.1659\n",
      "     14      \u001b[36m110.6872\u001b[0m      \u001b[32m123.0794\u001b[0m  0.0010  0.1953\n",
      "     15      106.4213      \u001b[32m118.1722\u001b[0m  0.0010  0.1929\n",
      "     41      111.9936      \u001b[32m111.0603\u001b[0m  0.0005  0.2059\n",
      "     15      \u001b[36m108.4246\u001b[0m      \u001b[32m143.2503\u001b[0m  0.0010  0.1851\n",
      "     15      \u001b[36m109.3613\u001b[0m      \u001b[32m120.8152\u001b[0m  0.0010  0.1889\n",
      "     42      \u001b[36m106.9898\u001b[0m      111.1495  0.0005  0.1838\n",
      "     16      \u001b[36m105.4381\u001b[0m      \u001b[32m117.0187\u001b[0m  0.0010  0.1950\n",
      "     16      \u001b[36m106.6620\u001b[0m      \u001b[32m139.6134\u001b[0m  0.0010  0.1928\n",
      "     16      \u001b[36m106.9989\u001b[0m      121.7182  0.0010  0.1747\n",
      "     17      \u001b[36m104.8401\u001b[0m      \u001b[32m116.7688\u001b[0m  0.0010  0.2052\n",
      "     43      110.5714      \u001b[32m111.0258\u001b[0m  0.0005  0.2155\n",
      "     17      \u001b[36m105.9052\u001b[0m      \u001b[32m139.0449\u001b[0m  0.0010  0.2023\n",
      "     17      \u001b[36m106.3915\u001b[0m      121.3969  0.0010  0.2116\n",
      "     44      110.4138      111.1366  0.0005  0.1874\n",
      "     18      \u001b[36m103.1772\u001b[0m      \u001b[32m116.2946\u001b[0m  0.0010  0.1913\n",
      "     18      106.0934      139.7563  0.0010  0.1954\n",
      "     18      \u001b[36m105.6315\u001b[0m      121.3595  0.0010  0.1767\n",
      "     45      108.8578      111.0887  0.0005  0.1668\n",
      "     19      \u001b[36m101.1676\u001b[0m      116.4787  0.0010  0.1857\n",
      "     19      \u001b[36m104.3734\u001b[0m      \u001b[32m138.7080\u001b[0m  0.0010  0.1647\n",
      "     19      \u001b[36m104.5247\u001b[0m      \u001b[32m120.1882\u001b[0m  0.0010  0.1546\n",
      "     20      101.1981      \u001b[32m116.2761\u001b[0m  0.0010  0.1455\n",
      "     46      108.5782      \u001b[32m110.7565\u001b[0m  0.0005  0.1690\n",
      "     20      \u001b[36m103.9170\u001b[0m      \u001b[32m138.5604\u001b[0m  0.0010  0.1613\n",
      "     20      \u001b[36m103.8157\u001b[0m      120.7386  0.0010  0.1682\n",
      "     21      \u001b[36m100.4113\u001b[0m      \u001b[32m115.5853\u001b[0m  0.0010  0.1537\n",
      "     47      108.1613      \u001b[32m110.5993\u001b[0m  0.0005  0.1719\n",
      "     21      \u001b[36m102.7735\u001b[0m      \u001b[32m136.4817\u001b[0m  0.0010  0.1561\n",
      "     21      104.2666      121.4525  0.0010  0.1647\n",
      "     22      \u001b[36m100.3793\u001b[0m      \u001b[32m115.2615\u001b[0m  0.0010  0.1487\n",
      "     48      107.8234      \u001b[32m110.5341\u001b[0m  0.0005  0.1721\n",
      "     22      102.7848      \u001b[32m136.2813\u001b[0m  0.0010  0.1566\n",
      "     22      \u001b[36m102.9708\u001b[0m      120.7869  0.0010  0.1441\n",
      "     23       \u001b[36m99.4554\u001b[0m      115.4089  0.0010  0.1555\n",
      "     49      107.1360      110.9741  0.0005  0.1682\n",
      "     23      \u001b[36m101.4144\u001b[0m      \u001b[32m136.0803\u001b[0m  0.0010  0.1709\n",
      "     23      \u001b[36m101.6124\u001b[0m      120.2721  0.0010  0.1709\n",
      "     24       \u001b[36m99.2769\u001b[0m      \u001b[32m114.8232\u001b[0m  0.0010  0.1592\n",
      "     50      108.8691      110.7671  0.0005  0.1416\n",
      "     24      \u001b[36m101.4071\u001b[0m      \u001b[32m119.0394\u001b[0m  0.0005  0.1412\n",
      "     24       \u001b[36m99.4682\u001b[0m      \u001b[32m135.5801\u001b[0m  0.0010  0.1531\n",
      "     25       \u001b[36m98.7003\u001b[0m      \u001b[32m114.3772\u001b[0m  0.0010  0.1640\n",
      "     51      109.6039      \u001b[32m110.2399\u001b[0m  0.0005  0.1648\n",
      "     25      100.2546      \u001b[32m135.2147\u001b[0m  0.0010  0.1460\n",
      "     25       \u001b[36m99.3571\u001b[0m      119.0749  0.0005  0.1585\n",
      "     26       \u001b[36m97.9531\u001b[0m      115.3059  0.0010  0.1415\n",
      "     26       \u001b[36m98.9301\u001b[0m      \u001b[32m133.0832\u001b[0m  0.0010  0.1190\n",
      "     52      109.4953      \u001b[32m110.1110\u001b[0m  0.0005  0.1545\n",
      "     26       \u001b[36m97.4844\u001b[0m      \u001b[32m118.9619\u001b[0m  0.0005  0.1546\n",
      "     27       98.4995      115.1014  0.0010  0.1675\n",
      "     27       99.5015      134.0135  0.0010  0.1549\n",
      "     53      108.2969      \u001b[32m110.0585\u001b[0m  0.0005  0.1432\n",
      "     27       98.3090      \u001b[32m118.2440\u001b[0m  0.0005  0.1294\n",
      "     28       \u001b[36m96.6845\u001b[0m      114.6264  0.0010  0.1626\n",
      "     54      108.1425      \u001b[32m109.9259\u001b[0m  0.0005  0.1391\n",
      "     28      100.0697      135.0345  0.0010  0.1471\n",
      "     28       98.9020      118.2802  0.0005  0.1655\n",
      "     55      \u001b[36m106.0299\u001b[0m      110.1110  0.0005  0.1279\n",
      "     29       \u001b[36m98.2849\u001b[0m      133.3501  0.0010  0.1503\n",
      "     29       96.6872      114.4871  0.0010  0.1638\n",
      "     29       \u001b[36m97.1348\u001b[0m      118.4369  0.0005  0.1462\n",
      "     56      106.9229      110.0131  0.0005  0.1202\n",
      "     30       \u001b[36m95.8064\u001b[0m      \u001b[32m114.1627\u001b[0m  0.0005  0.1254\n",
      "     30       \u001b[36m98.2557\u001b[0m      \u001b[32m132.9646\u001b[0m  0.0010  0.1531\n",
      "     30       98.4019      \u001b[32m117.7844\u001b[0m  0.0005  0.1563\n",
      "     57      106.4228      110.1895  0.0005  0.1304\n",
      "     31       \u001b[36m94.8327\u001b[0m      \u001b[32m114.0175\u001b[0m  0.0005  0.1496\n",
      "     31       \u001b[36m96.4331\u001b[0m      \u001b[32m117.6016\u001b[0m  0.0005  0.1273\n",
      "     31       \u001b[36m97.6832\u001b[0m      \u001b[32m131.9558\u001b[0m  0.0010  0.1806\n",
      "     58      108.0030      110.2976  0.0005  0.1490\n",
      "     32       \u001b[36m94.4066\u001b[0m      \u001b[32m113.9892\u001b[0m  0.0005  0.1344\n",
      "     32       98.3537      \u001b[32m117.2560\u001b[0m  0.0005  0.1374\n",
      "     32       \u001b[36m96.2197\u001b[0m      132.2816  0.0010  0.1469\n",
      "     33       \u001b[36m93.0031\u001b[0m      \u001b[32m113.9703\u001b[0m  0.0005  0.1553\n",
      "     33       97.1758      117.8522  0.0005  0.1326\n",
      "     33       \u001b[36m96.0091\u001b[0m      132.1541  0.0010  0.1465\n",
      "     34       94.4557      \u001b[32m113.8212\u001b[0m  0.0005  0.1366\n",
      "     34       98.6298      118.0585  0.0005  0.1494\n",
      "     34       96.6148      \u001b[32m131.5627\u001b[0m  0.0010  0.1486\n",
      "     35       94.4095      \u001b[32m113.5980\u001b[0m  0.0005  0.1350\n",
      "     35       97.0858      \u001b[32m117.1283\u001b[0m  0.0005  0.1373\n",
      "     35       \u001b[36m95.5665\u001b[0m      133.8117  0.0010  0.1521\n",
      "     36       93.4768      \u001b[32m113.2201\u001b[0m  0.0005  0.1633\n",
      "     36       97.5695      \u001b[32m116.6278\u001b[0m  0.0005  0.1420\n",
      "     36       95.7294      \u001b[32m130.7241\u001b[0m  0.0010  0.1314\n",
      "     37       93.7027      113.6001  0.0005  0.1539\n",
      "     37       96.1610      130.8437  0.0010  0.1102\n",
      "     37       \u001b[36m96.3145\u001b[0m      116.7468  0.0005  0.1555\n",
      "     38       93.3065      113.5682  0.0005  0.1094\n",
      "     38       \u001b[36m95.4474\u001b[0m      116.9093  0.0005  0.1524\n",
      "     38       \u001b[36m95.2494\u001b[0m      131.3882  0.0010  0.1577\n",
      "     39       \u001b[36m92.4909\u001b[0m      \u001b[32m113.2108\u001b[0m  0.0005  0.1334\n",
      "     39       96.4165      116.7347  0.0005  0.1290\n",
      "     39       \u001b[36m94.3282\u001b[0m      132.4851  0.0010  0.1416\n",
      "     40       93.5508      113.2956  0.0005  0.1446\n",
      "     40       95.4574      116.8262  0.0005  0.1312\n",
      "     40       \u001b[36m93.6630\u001b[0m      \u001b[32m129.8145\u001b[0m  0.0010  0.1415\n",
      "     41       \u001b[36m93.7105\u001b[0m      \u001b[32m116.5539\u001b[0m  0.0003  0.1274\n",
      "     41       95.2022      \u001b[32m129.0799\u001b[0m  0.0010  0.1324\n",
      "     42       94.5714      116.7871  0.0003  0.1309\n",
      "     42       94.0403      130.0156  0.0010  0.1484\n",
      "     43       94.0479      \u001b[32m116.1217\u001b[0m  0.0003  0.1639\n",
      "     43       94.4965      130.5274  0.0010  0.1590\n",
      "     44       94.3851      116.7862  0.0003  0.1313\n",
      "     44       93.8090      129.1594  0.0010  0.1627\n",
      "     45       \u001b[36m92.9408\u001b[0m      116.3850  0.0003  0.1392\n",
      "     45       94.0408      \u001b[32m129.0492\u001b[0m  0.0010  0.1314\n",
      "     46       94.3098      116.2043  0.0003  0.1473\n",
      "     46       94.3512      131.2788  0.0010  0.1969\n",
      "     47       94.2492      116.4275  0.0003  0.1544\n",
      "     47       94.0059      130.4797  0.0010  0.1135\n",
      "     48       \u001b[36m92.8227\u001b[0m      129.7645  0.0010  0.1236\n",
      "     49       \u001b[36m92.0836\u001b[0m      \u001b[32m128.7054\u001b[0m  0.0010  0.1153\n",
      "     50       92.3172      \u001b[32m128.5893\u001b[0m  0.0010  0.1080\n",
      "     51       \u001b[36m91.9463\u001b[0m      \u001b[32m127.8471\u001b[0m  0.0010  0.0924\n",
      "     52       92.4075      128.9647  0.0010  0.1134\n",
      "     53       92.5719      129.6835  0.0010  0.0944\n",
      "     54       92.2927      \u001b[32m126.6808\u001b[0m  0.0010  0.0865\n",
      "     55       \u001b[36m90.7879\u001b[0m      127.7832  0.0010  0.0738\n",
      "     56       91.8587      129.1452  0.0010  0.0743\n",
      "     57       \u001b[36m90.3471\u001b[0m      129.4789  0.0010  0.0736\n",
      "     58       90.7144      129.2534  0.0010  0.0737\n",
      "     59       \u001b[36m90.2602\u001b[0m      \u001b[32m123.4397\u001b[0m  0.0005  0.0727\n",
      "     60       \u001b[36m89.9125\u001b[0m      \u001b[32m123.3128\u001b[0m  0.0005  0.0730\n",
      "     61       \u001b[36m89.1154\u001b[0m      \u001b[32m122.8078\u001b[0m  0.0005  0.0769\n",
      "     62       \u001b[36m88.8856\u001b[0m      123.6186  0.0005  0.1090\n",
      "     63       89.7366      \u001b[32m122.6974\u001b[0m  0.0005  0.0993\n",
      "     64       89.7029      124.0385  0.0005  0.0914\n",
      "     65       \u001b[36m88.7076\u001b[0m      123.5452  0.0005  0.0886\n",
      "     66       \u001b[36m88.1618\u001b[0m      123.4870  0.0005  0.0795\n",
      "     67       89.5537      124.2819  0.0005  0.0747\n"
     ]
    }
   ],
   "source": [
    "# Prediction of each model for the guided and free dataset \n",
    "\n",
    "... #Other models \n",
    "\n",
    "y_g_predict_cnn = cross_val_predict(pipe_cnn,x, y, groups=groups, cv=logo, n_jobs=-1)\n",
    "y_f_predict_cnn = cross_val_predict(pipe_cnn,x_f, y_f, groups=groups_f, cv=logo, n_jobs=-1)\n",
    "\n",
    "y_g_predict_nn_covariance = cross_val_predict(pipe_fusion, x_cov_nn, y, groups=groups, cv=logo, n_jobs=-1)\n",
    "y_f_predict_nn_covariance = cross_val_predict(pipe_fusion, x_cov_nn_f, y_f, groups=groups_f, cv=logo, n_jobs=-1)\n",
    "\n",
    "\n",
    "# Rajouter les prédictions de vos modéles \n",
    "y_g_predict_ensemble = (y_g_predict_cnn + y_g_predict_nn_covariance) / 2 \n",
    "y_f_predict_ensemble = (y_f_predict_cnn + y_f_predict_nn_covariance) / 2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "663b6d7d-2915-465a-bbb8-220eac33dfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.6441526009970495 5.256709398294728 11.901573243849358 10.62443379408165\n",
      "0.046434447 0.059491545 0.2448608 0.19512913\n",
      "RMSE scores of the average method guided: 4.53488707749863 and free : 10.668102843239081\n",
      "NMSE scores of the average method guided: 0.044275176 and free : 0.19673648\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(y,y_predict):\n",
    "    mse  = mean_squared_error(y, y_predict)\n",
    "    rmse = np.sqrt(mse)\n",
    "    nmse = mse / np.var(y)  \n",
    "    return rmse, nmse \n",
    "\n",
    "\n",
    "# Metrics Guided\n",
    "rmse_g_cnn,   nmse_g_cnn   = calculate_metrics(y, y_g_predict_cnn)\n",
    "rmse_g_nn,    nmse_g_nn    = calculate_metrics(y, y_g_predict_nn_covariance)\n",
    "rmse_g_ens,   nmse_g_ens   = calculate_metrics(y, y_g_predict_ensemble)\n",
    "\n",
    "\n",
    "# RMSE Free\n",
    "rmse_f_cnn,   nmse_f_cnn   = calculate_metrics(y_f, y_f_predict_cnn)\n",
    "rmse_f_nn,    nmse_f_nn    = calculate_metrics(y_f, y_f_predict_nn_covariance)\n",
    "rmse_f_ens,   nmse_f_ens   = calculate_metrics(y_f, y_f_predict_ensemble)\n",
    "\n",
    "\n",
    "print(rmse_g_cnn,rmse_g_nn,rmse_f_cnn, rmse_f_nn)\n",
    "print(nmse_g_cnn,nmse_g_nn,nmse_f_cnn, nmse_f_nn)\n",
    "\n",
    "print(\"RMSE scores of the average method guided:\", rmse_g_ens, \"and free :\",rmse_f_ens)\n",
    "print(\"NMSE scores of the average method guided:\", nmse_g_ens, \"and free :\",nmse_f_ens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcfe570-c122-49e5-97cd-df0cc4f7733f",
   "metadata": {},
   "source": [
    "At this point we have our ensemble predictions (y_ensemble) and all the metrics we need for the average method. The power of this approach depends not only on each models individual performance but also on the diversity of their errors. Because we weight every model equally, a very weak model can drag the average down.\n",
    "\n",
    "However, when all the models are accurate enough and make different kinds of mistakes, averaging becomes extremely effective. For example, our CNN excels at learning local, spatial patterns in the raw filtered signal, while our nn + covariance network captures global statistical dependencies over time. Their errors rarely coincide, so when the CNN prediction is off for a given sample, the nn model often compensates it, and vice-versa. By averaging their outputs, we reduce the total variance and achieve a lower rmse/nmse than either model on its own, all without any extra hyperparameter tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db5d2bb-967f-4213-b2f5-84c46be9f023",
   "metadata": {},
   "source": [
    "For the second method, we concatenate the predictions of our models into a new dataset. This dataset will have the form (n_windows, targets × M), with M being the number of different models. After that, we train our model on this data and calculate the RMSE compared to the true y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45e80d0e-410f-49a8-8f90-6c86d5af21fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Guided RMSE: 4.5080758849207605 NMSE: 0.04375319\n",
      "Stacking Free RMSE: 12.041801759870632  NMSE: 0.25066486\n"
     ]
    }
   ],
   "source": [
    "# Creation of the new dataset with the prediction of our models \n",
    "x_g_meta = np.concatenate([y_g_predict_cnn, y_g_predict_nn_covariance], axis=1)\n",
    "x_f_meta = np.concatenate([y_f_predict_cnn, y_f_predict_nn_covariance], axis=1) \n",
    "\n",
    "# model \n",
    "lasso_model = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('lasso',  Lasso(alpha=0.1, max_iter=20000))\n",
    "    ])\n",
    "\n",
    "# Calculate the prediction\n",
    "y_g_predict_stack = cross_val_predict(lasso_model,x_g_meta,y,groups=groups,cv=logo,n_jobs=-1)\n",
    "y_f_predict_stack = cross_val_predict(lasso_model,x_f_meta,y_f,groups=groups_f,cv=logo,n_jobs=-1)\n",
    "\n",
    "\n",
    "#Calculate the metrics \n",
    "rmse_g_stack, nmse_g_stack = calculate_metrics(y, y_g_predict_stack)\n",
    "rmse_f_stack, nmse_f_stack = calculate_metrics(y_f, y_f_predict_stack)\n",
    "\n",
    "print(\"Stacking Guided RMSE:\",rmse_g_stack, \"NMSE:\",nmse_g_stack)\n",
    "print(\"Stacking Free RMSE:\",rmse_f_stack,\" NMSE:\", nmse_f_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a8b16d-4605-49a5-b1f5-de60823fa035",
   "metadata": {},
   "source": [
    "| Model / Ensembling       | RMSE Guided       | NMSE Guided       | RMSE Free         | NMSE Free         |\n",
    "| ---------------------- | ----------------- | ----------------- | ----------------- | ----------------- |\n",
    "| ...    |     |     |     |     |\n",
    "| Covariance matrices + Neural Network   | 5.19      | 0.044      | 10.70      | 0.24      |\n",
    "| CNN | 4.53   |  0.05  | 11.82   | 0.19   |\n",
    "| Averaging   | 4.46 | 0.042 | 10.67 | 0.196 |\n",
    "| Stacking    | 4.40    | 0.040    | 11.55    | 0.23    |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcfa3eb-c771-4065-b45a-7d3da757802d",
   "metadata": {},
   "source": [
    "# à completer \n",
    "We can see a small upgrade ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7e4785-3c25-4626-8b9b-d8e24a39591f",
   "metadata": {},
   "source": [
    "While the averaging method assigns equal weight to each base learner, stacking learns an optimal combination of their predictions and assigns weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d014d8bd-e2f8-4633-b2b1-650516a719bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with all the dataset \n",
    "lasso_model.fit(x_g_meta,y)\n",
    "\n",
    "# Have the coefficient for each output compare to each input\n",
    "# the matrix will have the shape (51, 51 * m)\n",
    "coeff_matrice = lasso_model.named_steps['lasso'].coef_\n",
    "\n",
    "def model_contributions(coeff_matrice,model_names):\n",
    "    m = len(model_names) \n",
    "    n_outputs, n_features = coeff_matrice.shape\n",
    "    per_model = n_features // m\n",
    "    \n",
    "    coef_abs = np.abs(coeff_matrice)\n",
    "    \n",
    "    # Calculate the coeef for each model and put them in a dico \n",
    "    sums = {}\n",
    "    for i in range(m):\n",
    "        start = i * per_model\n",
    "        end   = (i + 1) * per_model\n",
    "        model_sum = coef_abs[:, start:end].sum()\n",
    "        sums[model_names[i]] = model_sum\n",
    "    \n",
    "    # Normalize so that the contributions sum to 1\n",
    "    total = sum(sums.values())\n",
    "    for name in sums:\n",
    "        sums[name] /= total\n",
    "        \n",
    "    return sums\n",
    "\n",
    "sums = model_contributions(coeff_matrice,[\"CNN\",\"NN + Covariance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b30465cf-441e-4d2f-8e67-f5394c9fd9a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = list(sums.keys())\n",
    "contribs = list(sums.values())\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(models, contribs)\n",
    "plt.ylabel('Relative Contribution')\n",
    "plt.xlabel('Base Model')\n",
    "plt.title('Model Contributions in Meta-Learner')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b753342-6a6e-4e29-b51d-931728596593",
   "metadata": {},
   "source": [
    "# à completer \n",
    "\"Discuss how the bias-variance tradeoff relates to the observed (or expected)\n",
    "evolution of performance.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7200fc39-0b93-4182-b893-c935d537e03b",
   "metadata": {},
   "source": [
    "#### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0592a278-3414-48f7-bed5-c8f8125312f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x_train, y_train, test_x):\n",
    "    model.fit(x_train, y_train)\n",
    "    y_predict = model.predict(test_x)\n",
    "    return y_predict \n",
    "\n",
    "\n",
    "def build_csv(predict_guided, predict_free, name = \"team2_submission.csv\"):\n",
    "    submission_array = np.vstack((predict_guided, predict_free))\n",
    "    df = pd.DataFrame(submission_array)\n",
    "    df.to_csv(name, index=False, header=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1c8ffe2-24d5-492e-89f8-5c58f2397f1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 4. StandardScaler expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predict_guided_stack \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlasso_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_g_meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_g_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m predict_free_stack \u001b[38;5;241m=\u001b[39m predict(lasso_model,x_f_meta,y_f,X_f_test)\n",
      "Cell \u001b[0;32mIn[26], line 3\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(model, x_train, y_train, test_x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(model, x_train, y_train, test_x):\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(x_train, y_train)\n\u001b[0;32m----> 3\u001b[0m     y_predict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y_predict\n",
      "File \u001b[0;32m~/ls/envs/sfml/lib/python3.12/site-packages/sklearn/pipeline.py:787\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[0;34m(self, X, **params)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 787\u001b[0m         Xt \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    790\u001b[0m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n",
      "File \u001b[0;32m~/ls/envs/sfml/lib/python3.12/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m~/ls/envs/sfml/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:1062\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1059\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1061\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m-> 1062\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[0;32m~/ls/envs/sfml/lib/python3.12/site-packages/sklearn/utils/validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2942\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m   2943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m-> 2944\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m   2946\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/ls/envs/sfml/lib/python3.12/site-packages/sklearn/utils/validation.py:1101\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1096\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1097\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1098\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1099\u001b[0m     )\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m-> 1101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[1;32m   1107\u001b[0m     _assert_all_finite(\n\u001b[1;32m   1108\u001b[0m         array,\n\u001b[1;32m   1109\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m   1110\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m   1111\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mensure_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1112\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 4. StandardScaler expected <= 2."
     ]
    }
   ],
   "source": [
    "predict_guided_stack = predict(lasso_model,x_g_meta,y,X_g_test)\n",
    "predict_free_stack = predict(lasso_model,x_f_meta,y_f,X_f_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf626cc-1709-48df-9532-d01cdb498e00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
