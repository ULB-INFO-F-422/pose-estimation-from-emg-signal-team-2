{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "895a75fb",
   "metadata": {},
   "source": [
    "# INFO-f422: ML Project\n",
    "\n",
    "authors:\n",
    "+ 1 \n",
    "+ 2\n",
    "+ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dcc5fc",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17578f83-f52e-47ee-9d73-b33d910722dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.set_printoptions(threshold=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493fe755",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31ae6158-83fc-4039-ad35-4168b9fed8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data\"\n",
    "\n",
    "X_g_train = np.load(\"../guided/guided_dataset_X.npy\")\n",
    "y_g_train = np.load(\"../guided/guided_dataset_y.npy\")\n",
    "X_g_test = np.load(\"../guided/guided_testset_X.npy\")\n",
    "\n",
    "X_f_train = np.load(\"../freemoves/freemoves_dataset_X.npy\")\n",
    "y_f_train = np.load(\"../freemoves/freemoves_dataset_y.npy\")\n",
    "X_f_test = np.load(\"../freemoves/freemoves_testset_X.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5d74e88-bad4-4a00-9cdf-b5b5b1f6cfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guided:\n",
      "X_g_train (5, 8, 230000) / y_g_train(5, 51, 230000) / X_g_test(5, 332, 8, 500)\n",
      "\n",
      "Free moves:\n",
      "X_f_train(5, 8, 270000) / y_f_train(5, 51, 270000) / X_f_test(5, 308, 8, 500)\n"
     ]
    }
   ],
   "source": [
    "print(\"Guided:\")\n",
    "print(f\"X_g_train {X_g_train.shape} / y_g_train{y_g_train.shape} / X_g_test{X_g_test.shape}\\n\")\n",
    "print(\"Free moves:\")\n",
    "print(f\"X_f_train{X_f_train.shape} / y_f_train{y_f_train.shape} / X_f_test{X_f_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a29437e-9e32-45e1-a305-0bfad39af254",
   "metadata": {},
   "source": [
    "### 1) Signal filtering\n",
    "\n",
    "TODO: data exploration to take informed decision on filter (type of noise,....) to use and on filter parametres (no magic number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4331fabb-bc67-49c5-8d18-9572b5dc3e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import butter, sosfiltfilt, firwin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bdf832f-16c7-4dbf-b0cc-43fc14aa64c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def filtrage(x):\n",
    "    nyq  = 1024 / 2\n",
    "    low  = 20  / nyq\n",
    "    high = 450 / nyq\n",
    "    \n",
    "    sos = butter(4,[low,high], btype='band', output= 'sos')\n",
    "    \n",
    "    for sess in range(x.shape[0]):\n",
    "        for elec in range(x.shape[1]):\n",
    "            # Application of the filtrage for x\n",
    "            x[sess, elec, :] = sosfiltfilt(sos, x[sess, elec, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1dfcb3-48a7-4df1-80ec-79e5fd3bd630",
   "metadata": {},
   "source": [
    "### 2) Dataset preparation\n",
    "\n",
    "At the beginning, we implemented a naive function that loops for each windows needed. \n",
    "\n",
    "This version work but:\n",
    "- Only when the step size can divides the total number of samples.  \n",
    "- Copies every window into a new array, incurring  CPU overhead and unnecessary memory usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24c97d3d-aa09-418c-bafd-3d7be401f11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap(data, overlap=0.5, size=500):\n",
    "    Data = []\n",
    "    step = int(size * (1 - overlap))\n",
    "    n = (data.shape[2] - size) // step + 1\n",
    "    fin = n * step\n",
    "    \n",
    "    for start in range(0, fin, step):\n",
    "        end = start + size\n",
    "        W = data[... , start:end]\n",
    "        Data.append(W)\n",
    "        \n",
    "    Data = np.array(Data)\n",
    "    Data = Data.transpose(1, 0, 2, 3)\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29750bdf-f261-4fd6-84b0-d4dc3bd2aa55",
   "metadata": {},
   "source": [
    "But after some research, we decided to use the sliding_window_view function from the Numpy library for several reasons:\n",
    "\n",
    "+ Fast vectorized numpy operations, compiled c-code (no python overhead, interpreter).\n",
    "\n",
    "+ sliding_window_view function returns a view, no copy.\n",
    "\n",
    "+ The function simplifies the implementation by automating window creation and indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f74676a2-d393-4e44-98ce-48190ed197a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guided windowed:\n",
      "X_g_train_wdw (5, 919, 8, 500) / y_g_train_wdw(5, 919, 51) / X_g_test(5, 332, 8, 500)\n",
      "X_f_train_wdw (5, 1079, 8, 500) / y_f_train_wdw(5, 1079, 51) / X_f_test(5, 308, 8, 500)\n"
     ]
    }
   ],
   "source": [
    "def create_overlap_windows(x, y, window_size, overlap, axis):\n",
    "\n",
    "    step = int(window_size * (1 - overlap))\n",
    "\n",
    "    # sliding_windows_view Generate all possible windows with the corresponding step, that not what we want.\n",
    "    x_w = sliding_window_view(x,window_size,axis)\n",
    "    y_w = sliding_window_view(y,window_size,axis)\n",
    "\n",
    "    # only keep windows where the step is a multiple of our step \n",
    "    x_w = x_w[:,:,::step,:]\n",
    "    y_w = y_w[:,:,::step,:]\n",
    "\n",
    "    # We transpose the axes windows and electrode/signal \n",
    "    x_w = x_w.transpose(0, 2, 1, 3)     #  (session, window, electrode, time) and not  (session, electrode, window, time) TODO??\n",
    "    y_w = y_w.transpose(0, 2, 1, 3)     # (session, window, signals, time)\n",
    "\n",
    "    # Finaly, we keep only the last hand position (targets) for y, because for this project\n",
    "    # we need to predict, for each window in x, the final hand position in the\n",
    "    # same windows in the dataset y\n",
    "    y_w = y_w[..., -1]  # (sessions, windows, targets)\n",
    "\n",
    "    return x_w, y_w\n",
    "\n",
    "\n",
    "X_g_train_wdw, y_g_train_wdw = create_overlap_windows(X_g_train, y_g_train, window_size=500, overlap=0.5, axis=2)\n",
    "X_f_train_wdw, y_f_train_wdw = create_overlap_windows(X_f_train, y_f_train, window_size=500, overlap=0.5, axis=2)\n",
    "# !! windowed data is a view --> share original data memory (modify one, modify both)\n",
    "\n",
    "print(\"Guided windowed:\")\n",
    "print(f\"X_g_train_wdw {X_g_train_wdw.shape} / y_g_train_wdw{y_g_train_wdw.shape} / X_g_test{X_g_test.shape}\")\n",
    "print(f\"X_f_train_wdw {X_f_train_wdw.shape} / y_f_train_wdw{y_f_train_wdw.shape} / X_f_test{X_f_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a5557ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_windows_tests(x, y):\n",
    "    # (maybe automate tests given windowsize and overlap and consider internal frag (shoudl be discarded)\n",
    "    \n",
    "    x_w, y_w = create_overlap_windows(x, y, window_size=500, overlap=0.5, axis=2)    \n",
    "    \n",
    "    assert np.array_equal(x_w[0, 0, 0, :10], x[0, 0, :10]) # (sess 0) first 10 of electrode 0 in window 0\n",
    "    assert np.array_equal(x_w[0, 1, 0, :10], x[0, 0, 250:260]) # (sess 0) first 10 of electrode 0 in window 1\n",
    "    assert np.array_equal(x_w[0, 1, 4, :10], x[0, 4, 250:260]) # (sess 0) first 10 of electrode 4 in window 1\n",
    "    assert np.array_equal(x_w[0, 918, 0, -10:], x[0, 0, 229990:230000]) # (sess 0) last 10 of electrode 0 in last window (918) - (perfect fit!)\n",
    "\n",
    "quick_windows_tests(X_g_train, y_g_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efa6666-9c5a-422c-8c26-79b8f6594316",
   "metadata": {},
   "source": [
    "#### 3) Cross validation strategy\n",
    "\n",
    "For this question, we have thought about various methods of cross validation. First, our data are continous because it's a signal, so preserving temporal structure is important. We can’t use a method of cross validation which randomly shuffles our windows. \n",
    "\n",
    "We also need to prevents data leaking so we can't use a methode who use the windows of one session for training AND validation because we have overlapping data in each session, two windows in the same session can share the same datas, and if these two windows are in train and validation, it will lead to data leakage and overly optimistic performance (data in the train set will also be in the validation set). \n",
    "\n",
    "So it's naturally that we have chosen the \"Leave One Group Out\" method, this method will use each session as the validation set once and the other for training. We completly prevent data leakage because each session is indepandent from the other, and we reduce the bias because each session will be used for validation.\n",
    "\n",
    "In our case, \"LOGO\" and \"GroupKFold(5)\" produce the same splits, but we choose \"LOGO\" because it's more explicit, readers will immediatly see that we use one session for validation each time while \"GroupKFold\" need to have 5 in parameter to do the same thong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d586a98-56d4-4372-a33f-01ba5ec537cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "groups(4595,)\n",
      "\n",
      "groups_f(5395,)\n",
      "\n",
      "Guided windowed flattened:\n",
      "X_g_train_wdw_flat(4595, 4000) / y_g_train_wdw_flat(4595, 51)\n",
      "Free windowed flattened:\n",
      "X_f_train_wdw_flat(5395, 4000) / y_f_train_wdw_flat(5395, 51)\n"
     ]
    }
   ],
   "source": [
    "x_shape = X_g_train_wdw.shape\n",
    "y_shape = y_g_train_wdw.shape\n",
    "\n",
    "x_f_shape = X_f_train_wdw.shape\n",
    "y_f_shape = y_f_train_wdw.shape\n",
    "\n",
    "\n",
    "groups = np.repeat(np.arange(1,x_shape[0]+1),x_shape[1] ) # 111 (919 times), 222 (919 times), ...\n",
    "print(f\"groups{groups.shape}\\n\")\n",
    "\n",
    "groups_f = np.repeat(np.arange(1,x_f_shape[0]+1),x_f_shape[1] ) \n",
    "print(f\"groups_f{groups_f.shape}\\n\")\n",
    "\n",
    "# We need to flatten the dataset x and y because the function logo (and latter \"croos_val_score\")\n",
    "# want all the data in a 2d list, we will know have  the dataset X for exemple.\n",
    "# [4595, 4000] and not [5,919,8,500], 4595 is the multiplication of 5 and 919 (3500 = 8*500), and y \n",
    "# [4595,51] and not [5,919,51].\n",
    "# Now all the windows are store in a list and the \"groups\" list above allow the function \n",
    "# logo to know at wich session each windows belong\n",
    "# The windows 3 for example (x_windows_flat[2]) belong to the sessions groups[2] = 1\n",
    "X_g_train_wdw_flat = X_g_train_wdw.reshape(x_shape[0] * x_shape[1], x_shape[2] * x_shape[3])\n",
    "y_g_train_wdw_flat = y_g_train_wdw.reshape(y_shape[0] * y_shape[1], y_shape[2])\n",
    "\n",
    "X_f_train_wdw_flat = X_f_train_wdw.reshape(x_f_shape[0] * x_f_shape[1], x_f_shape[2] * x_f_shape[3])\n",
    "y_f_train_wdw_flat = y_f_train_wdw.reshape(y_f_shape[0] * y_f_shape[1], y_f_shape[2])\n",
    "\n",
    "print(\"Guided windowed flattened:\")\n",
    "print(f\"X_g_train_wdw_flat{X_g_train_wdw_flat.shape} / y_g_train_wdw_flat{y_g_train_wdw_flat.shape}\")\n",
    "\n",
    "print(\"Free windowed flattened:\")\n",
    "print(f\"X_f_train_wdw_flat{X_f_train_wdw_flat.shape} / y_f_train_wdw_flat{y_f_train_wdw_flat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92449bff-754d-4a5a-b2ca-d88385116328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut, cross_val_score,cross_val_predict\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07e1243c-045c-436b-85b1-725a43ead35f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# np.random.seed(0)\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "lasso_model = Lasso(max_iter=10) # If the iteration is higher, it take to much time, even on collab \n",
    "'''# If you don't want the warning but prepare your afternoon for the runtime \n",
    "lasso_model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lasso',   Lasso(alpha=0.1, max_iter=20000))\n",
    "])'''\n",
    "\n",
    "\n",
    "rmse_scorer = make_scorer(\n",
    "    lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "    greater_is_better=False  # Score near 0 is better \n",
    ")\n",
    "\n",
    "def cross_validation_with_scores(X,Y,groups,model,cv,scoring):\n",
    "    # The cross_val_score function by sklearn will execute our cv and return a tab \n",
    "    neg_rmse_scores = cross_val_score(\n",
    "        model,\n",
    "        X,\n",
    "        Y,\n",
    "        groups=groups,\n",
    "        cv=cv,\n",
    "        scoring=rmse_scorer,\n",
    "        n_jobs=-1 # Use all cores \n",
    "    )\n",
    "    \n",
    "    # Conversion of negatifs scores into positifs (convention of sklearn)\n",
    "    rmse_scores = -neg_rmse_scores  \n",
    "    print(\"RMSE for each folder:\", rmse_scores)\n",
    "    print(\"RMSE mean:\", rmse_scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "366607c5-4aac-4b22-9010-2a1f919dd752",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for each folder: [17.94011704 18.68652126 20.46060078 17.30326328 19.3046778 ]\n",
      "RMSE mean: 18.739036031720634\n",
      "RMSE for each folder: [17.80702643 16.56398358 16.04999994 15.49969335 13.02059498]\n",
      "RMSE mean: 15.788259656326385\n"
     ]
    }
   ],
   "source": [
    "#Guided \n",
    "cross_validation_with_scores(X_g_train_wdw_flat,y_g_train_wdw_flat,groups,lasso_model,logo,rmse_scorer)\n",
    "\n",
    "#Free\n",
    "cross_validation_with_scores(X_f_train_wdw_flat,y_f_train_wdw_flat,groups_f,lasso_model,logo,rmse_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "384848b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session 0 target info:\n",
      "  min = -108.68231864942676\n",
      "  max = 44.76897408739836\n",
      "  mean = -5.73247691191569\n"
     ]
    }
   ],
   "source": [
    "# rmse context\n",
    "sess = 0\n",
    "y_max = np.max(y_g_train_wdw[sess])\n",
    "y_min = np.min(y_g_train_wdw[sess])\n",
    "y_mean = np.mean(y_g_train_wdw[sess])\n",
    "\n",
    "print(f\"Session {sess} target info:\\n  min = {y_min}\\n  max = {y_max}\\n  mean = {y_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecd17ef4-108a-480b-aba8-33de84479a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "   train groups: [2 3 4 5]\n",
      "   test groups: [1]\n",
      "Fold 1\n",
      "   train groups: [1 3 4 5]\n",
      "   test groups: [2]\n",
      "Fold 2\n",
      "   train groups: [1 2 4 5]\n",
      "   test groups: [3]\n",
      "Fold 3\n",
      "   train groups: [1 2 3 5]\n",
      "   test groups: [4]\n",
      "Fold 4\n",
      "   train groups: [1 2 3 4]\n",
      "   test groups: [5]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, (train_index, test_index) in enumerate(logo.split(X_g_train_wdw_flat, y_g_train_wdw_flat, groups)):\n",
    "    print(f\"Fold {i}\")\n",
    "    print(f\"   train groups: {np.unique(groups[train_index])}\")\n",
    "    print(f\"   test groups: {np.unique(groups[test_index])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb3b954-0157-4eae-a02d-355a20329d9a",
   "metadata": {},
   "source": [
    "#### 5) More sophisticated approach\n",
    "\n",
    "For this question, we decided to implement the two approaches in order to have a better understanding and more methods to compare.\n",
    "\n",
    "We started with the covariance approch, following the steps in section 3.2:\n",
    "\n",
    "- We first calculate the covaraince of each windows with the PyRiemmann Covariances class, which expects a 3d array, so we need to reshape it into the form (windows,electrode,time). After using this class, we obtain an array of 8×8 covariance matrices (SPD_tab) for each window.\n",
    "\n",
    "- Next, we map each SPD matrix into their tangent space using the TangentSpace class. This projection transforms our SPD matrices into Euclidean vectors. Thanks to this, our dataset becomes 2D again, and we can directly use a traditional regression algorithms and sklearn function.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ffd5b0f-b9dc-406e-a930-9ec1ba230d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pyriemann\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from sklearn.pipeline     import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4668e27-97cc-4c58-b39f-1d67dcbb7ce4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Covariances method\n",
    "\n",
    "\n",
    "# Reshape the flattten dataset\n",
    "X_g_train_reshape = X_g_train_wdw_flat.reshape(4595,8,500)\n",
    "\n",
    "X_f_train_reshape = X_f_train_wdw_flat.reshape(5395,8,500)\n",
    "\n",
    "'''\n",
    "covariance = Covariances(estimator='oas')\n",
    "SPD_tab = covariance.fit_transform(X_g_train_reshape) \n",
    "# print(SPD_tab.shape) # (4595,8,8)\n",
    "ts = TangentSpace()\n",
    "tangent_tab = ts.fit_transform(SPD_tab)\n",
    "# print(tangent_tab.shape) # (4595,36) Now that we have a 2d tab, we can use traditional regression algorithms\n",
    "'''\n",
    "\n",
    "pipe_cov = Pipeline([\n",
    "    ('cov',    Covariances(estimator='oas')),   # Covariances matrices of each windows\n",
    "    ('ts',     TangentSpace()),                 # This projection will transform our SPD matrices into euclidean vector\n",
    "    ('scale', StandardScaler()),          # Standardize each feature (mean =0, std =1) \n",
    "    ('lasso',  lasso_model)               # Lasso model \n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96428ea2-8afb-40d0-a5d8-5d0a9b1db6d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.225e+01, tolerance: 4.499e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.743e+03, tolerance: 4.204e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.416e+03, tolerance: 5.580e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.093e+04, tolerance: 1.938e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.449e+03, tolerance: 1.159e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.410e+05, tolerance: 4.810e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.876e+04, tolerance: 2.176e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.683e+04, tolerance: 1.336e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.070e+05, tolerance: 5.174e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.469e+04, tolerance: 1.623e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.735e+04, tolerance: 1.958e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.705e+05, tolerance: 6.268e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.288e+04, tolerance: 1.587e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.758e+01, tolerance: 6.573e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.925e+03, tolerance: 2.453e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.118e+04, tolerance: 4.521e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.136e+03, tolerance: 1.364e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.548e+01, tolerance: 4.776e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.908e+03, tolerance: 4.466e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.157e+03, tolerance: 5.686e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.899e+04, tolerance: 1.997e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.580e+03, tolerance: 1.193e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.229e+05, tolerance: 4.854e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.533e+04, tolerance: 2.284e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.970e+04, tolerance: 1.378e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.744e+04, tolerance: 5.241e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.014e+04, tolerance: 1.708e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.434e+04, tolerance: 1.965e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.462e+05, tolerance: 6.333e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.740e+04, tolerance: 1.647e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.337e+01, tolerance: 6.703e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.137e+03, tolerance: 2.427e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.902e+04, tolerance: 4.602e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.032e+03, tolerance: 1.402e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.592e+01, tolerance: 4.492e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.463e+03, tolerance: 4.189e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.281e+03, tolerance: 5.803e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.986e+04, tolerance: 1.931e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.134e+03, tolerance: 1.121e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.104e+04, tolerance: 4.759e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.365e+04, tolerance: 2.145e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.550e+04, tolerance: 1.293e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.049e+04, tolerance: 5.200e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.032e+04, tolerance: 1.644e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.695e+04, tolerance: 1.921e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.037e+05, tolerance: 6.286e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.190e+04, tolerance: 1.582e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.541e+03, tolerance: 2.356e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.740e+04, tolerance: 4.605e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.205e+03, tolerance: 1.377e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.556e+03, tolerance: 4.037e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.106e+03, tolerance: 5.886e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.382e+04, tolerance: 1.966e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.159e+03, tolerance: 1.141e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.511e+05, tolerance: 4.853e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.367e+04, tolerance: 2.249e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.862e+04, tolerance: 1.347e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.128e+05, tolerance: 5.276e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.328e+04, tolerance: 1.661e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.199e+04, tolerance: 1.987e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.314e+05, tolerance: 6.315e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.485e+04, tolerance: 1.616e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.622e+01, tolerance: 4.404e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.353e+01, tolerance: 6.584e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.134e+04, tolerance: 2.563e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.066e+03, tolerance: 4.038e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.517e+04, tolerance: 4.627e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.392e+03, tolerance: 5.997e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.930e+03, tolerance: 1.367e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.082e+04, tolerance: 1.874e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.309e+03, tolerance: 1.188e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.326e+05, tolerance: 4.898e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.703e+04, tolerance: 2.142e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.652e+04, tolerance: 1.390e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.699e+04, tolerance: 5.309e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+04, tolerance: 1.602e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.151e+04, tolerance: 1.980e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.317e+05, tolerance: 6.463e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.229e+03, tolerance: 1.591e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.158e+00, tolerance: 6.764e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.985e+03, tolerance: 2.565e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.796e+04, tolerance: 4.615e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.676e+03, tolerance: 1.351e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for each folder: [8.21531223 7.74146158 7.93251768 6.92650706 7.90240062]\n",
      "RMSE mean: 7.743639831450407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.377e+02, tolerance: 6.182e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.075e+03, tolerance: 9.952e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.569e+04, tolerance: 3.994e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.704e+04, tolerance: 2.363e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.205e+04, tolerance: 1.507e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.653e+04, tolerance: 4.424e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.267e+04, tolerance: 2.648e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.430e+04, tolerance: 2.026e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.364e+04, tolerance: 4.156e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.440e+04, tolerance: 2.303e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.079e+04, tolerance: 2.614e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.776e+02, tolerance: 6.018e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.776e+04, tolerance: 3.917e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.108e+04, tolerance: 2.075e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.274e+03, tolerance: 1.039e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.577e+05, tolerance: 4.083e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.724e+04, tolerance: 2.386e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.745e+04, tolerance: 1.474e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.649e+05, tolerance: 4.471e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.672e+04, tolerance: 2.600e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.445e+04, tolerance: 1.965e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.485e+05, tolerance: 4.330e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.049e+04, tolerance: 2.228e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.188e+04, tolerance: 2.546e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.613e+04, tolerance: 3.964e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+04, tolerance: 2.071e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.529e+01, tolerance: 6.162e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.166e+02, tolerance: 4.879e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.554e+02, tolerance: 6.418e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.781e+03, tolerance: 9.192e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.959e+01, tolerance: 4.744e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.319e+04, tolerance: 3.677e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.440e+04, tolerance: 2.073e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.254e+04, tolerance: 1.035e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.373e+04, tolerance: 1.369e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.732e+04, tolerance: 3.817e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.221e+05, tolerance: 4.212e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.169e+04, tolerance: 2.244e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.706e+04, tolerance: 1.391e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.891e+04, tolerance: 2.490e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.441e+02, tolerance: 5.063e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.269e+05, tolerance: 4.474e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.879e+04, tolerance: 1.816e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.560e+04, tolerance: 2.626e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.226e+05, tolerance: 4.100e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.505e+04, tolerance: 1.946e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.904e+04, tolerance: 2.137e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.347e+05, tolerance: 4.389e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.645e+03, tolerance: 9.219e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.073e+04, tolerance: 2.226e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.251e+04, tolerance: 2.771e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.785e+04, tolerance: 2.296e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.517e+04, tolerance: 2.648e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.742e+02, tolerance: 1.486e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.347e+04, tolerance: 3.799e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.396e+05, tolerance: 4.013e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.640e+03, tolerance: 1.263e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.928e+04, tolerance: 1.959e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.576e+04, tolerance: 2.052e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.251e+04, tolerance: 3.736e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.413e+03, tolerance: 2.084e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.348e+04, tolerance: 1.712e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.038e+04, tolerance: 3.785e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.084e+03, tolerance: 1.842e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.606e+04, tolerance: 2.252e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.067e+04, tolerance: 3.349e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.482e+04, tolerance: 1.702e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for each folder: [10.60965401 12.45418765 11.22372944  9.79798601  9.58054703]\n",
      "RMSE mean: 10.733220829146067\n"
     ]
    }
   ],
   "source": [
    "# Now we juste need to use the cv function we build above \n",
    "#Guided \n",
    "cross_validation_with_scores(X_g_train_reshape,y_g_train_wdw_flat,groups,pipe_cov,logo,rmse_scorer)\n",
    "\n",
    "#Free \n",
    "cross_validation_with_scores(X_f_train_reshape,y_f_train_wdw_flat,groups_f,pipe_cov,logo,rmse_scorer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744f83a5-45be-463c-93c1-254dec71ef8e",
   "metadata": {},
   "source": [
    "For the neural network approch, we have done:\n",
    "\n",
    "##### Simple model\n",
    "We started by a simple model composed of 3 linear layers.\n",
    "\n",
    "To evaluate it, we used Skorch, which lets us plug a PyTorch nn.Module into a sklearn function.\n",
    "Thanks to Skorch, we could reuse our existing cross_validation_with_scores() function.\n",
    "\n",
    "The rmse mean was 17.03, which beat the vanilla lasso model but not the covriance matrices lasso. We analyzed the value of the training loss and validation loss for each epoch and we saw that the model is underfitting.\n",
    "So we decide to complexify it.\n",
    "\n",
    "##### Model complexity\n",
    "\n",
    "We upgraded to a small CNN with three 1D convolutional layers because the dataset has to much datas for an nn classique and to automatically learn local temporal patterns in the EMG signal. \n",
    "\n",
    "After each convolutional layer:\n",
    "\n",
    "We normalize our data to stabilize and speed up training.\n",
    "\n",
    "We introduce a simple Relu activation so the network can learn more complex features.\n",
    "\n",
    "We “cut” the time dimension in half, keeping only the strongest responses and reducing data size.\n",
    "\n",
    "Once the three convolutional blocks are done, we flatten the output tensor and pass it into a two layer head, to convert these extracted features into the 51 joint-angle predictions\n",
    "\n",
    "##### Early stopping and LR scheduling\n",
    "\n",
    "We added two Skorch callbacks:\n",
    "\n",
    "-EarlyStopping to stop the training when no improvement is seen for 10 epochs.\n",
    "\n",
    "-LRScheduler to cut the learning rate by half when the validation loss stalls 3 epochs consecutives.\n",
    "\n",
    "This combination prevents wasted epochs once the model converges and refines the learning rate to squeeze out extra gains.\n",
    "\n",
    "##### Batch size reduction\n",
    "\n",
    "We lowered the batch size from 128 to 64. Using smaller batches adds a bit of randomness to each weight update, which helps the model generalize better without altering its structure.\n",
    "\n",
    "With these three changes, the nn average RMSE dropped to ~5.07, a dramatic improvement over the initial ~17."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8074777f-d8a4-4172-b732-6a425d3f0fbc",
   "metadata": {},
   "source": [
    "| Simple Model | Complex Model | Complex Model + Early Stopping & LR Scheduler | Same as #3 but Batch Size = 64 |\n",
    "|:------------:|:-------------:|:---------------------------------------------:|:-----------------------------:|\n",
    "| ![](./images/1.png) | ![](./images/2.png) | ![](./images/3.png) | ![](./images/4.png) |\n",
    "| **RMSE mean:** 17.03 | **RMSE mean:** 6.06 | **RMSE mean:** 5.89 | **RMSE mean:** 4.87 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8a70a53-6562-4aa8-b197-af063ca6a13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U skorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skorch import NeuralNetRegressor\n",
    "from skorch.callbacks import EarlyStopping\n",
    "from skorch.callbacks import LRScheduler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "42cd60a5-d6f7-4e74-91f0-f26c7c8b7ec8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "## NN method\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.f = nn.Sequential(\n",
    "            nn.Unflatten(1, (8,500)), #unflatten data for  convolution (64 (batch_size), 8,500)\n",
    "\n",
    "            nn.Conv1d(8, 32, kernel_size=11, padding=5), # 8 input channels (electrodes) and 32 is the output, the number of feature he learn.\n",
    "            # Thanks to padding, the output length remains 500 (64,32,500) \n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),  # Halve the temporal dimension  (64,32,250)          \n",
    "            \n",
    "            nn.Conv1d(32, 64, kernel_size=9, padding=4), #(64,64,250)\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),  #(64,64,125)      \n",
    "\n",
    "            nn.Conv1d(64, 128, kernel_size=7, padding=3), # (64,128,125)\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),  # (64,128,62)    \n",
    "            \n",
    "            nn.Flatten(), # Reflatten our data for the next part (64,128*62)\n",
    "        )\n",
    "        self.r = nn.Sequential(\n",
    "            nn.Linear(7936, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 51),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.f(x)\n",
    "        return self.r(x)\n",
    "\n",
    "      \n",
    "# Convert dataset from float64 to float32 because PyTorch layers expect float32\n",
    "# 32-bit precision is sufficient for our signals and speeds up training\n",
    "print(X_g_train_wdw_flat.dtype)   \n",
    "print(y_g_train_wdw_flat.dtype)   \n",
    "\n",
    "x = X_g_train_wdw_flat.astype('float32')\n",
    "y = y_g_train_wdw_flat.astype('float32')\n",
    "\n",
    "x_f = X_f_train_wdw_flat.astype('float32')\n",
    "y_f = y_f_train_wdw_flat.astype('float32')\n",
    "\n",
    "\n",
    "net = NeuralNetRegressor(\n",
    "    module=NeuralNetwork,                 # PyTorch model \n",
    "    max_epochs=100,                 \n",
    "    lr=1e-3,                       \n",
    "    batch_size=64,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    callbacks=[('earlystop', EarlyStopping('valid_loss', patience=10)), # Stop if validation loss doesn't improve for 5 epochs \n",
    "                    ('lr_sched', LRScheduler(\n",
    "           policy=torch.optim.lr_scheduler.ReduceLROnPlateau, # Halve LR if validation loss stalls for 3 epochs\n",
    "           monitor='valid_loss',\n",
    "           patience=3, factor=0.5))]\n",
    ")\n",
    "\n",
    "\n",
    "pipe_cnn = Pipeline([\n",
    "    ('scale', StandardScaler()),  # Standardize inputs for a quick start, after the cnn will then normalize its data at each step\n",
    "    ('net',   net)                \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3771a06a-a336-45a1-af7d-146c519b0c45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m183.7466\u001b[0m      \u001b[32m117.0032\u001b[0m  0.0010  12.0949\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m189.3073\u001b[0m      \u001b[32m126.2191\u001b[0m  0.0010  12.0673\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m180.7975\u001b[0m      \u001b[32m117.4314\u001b[0m  0.0010  12.1996\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m197.3731\u001b[0m      \u001b[32m121.0930\u001b[0m  0.0010  12.2818\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m188.5981\u001b[0m      \u001b[32m127.7126\u001b[0m  0.0010  12.3568\n",
      "      2       \u001b[36m87.7310\u001b[0m      \u001b[32m109.3404\u001b[0m  0.0010  14.1686\n",
      "      2       \u001b[36m83.0573\u001b[0m       \u001b[32m79.4835\u001b[0m  0.0010  14.5236\n",
      "      2       \u001b[36m86.1038\u001b[0m      \u001b[32m101.4511\u001b[0m  0.0010  14.5651\n",
      "      2       \u001b[36m92.7528\u001b[0m       \u001b[32m80.5137\u001b[0m  0.0010  14.4968\n",
      "      2       \u001b[36m88.0846\u001b[0m       \u001b[32m70.8250\u001b[0m  0.0010  14.6335\n",
      "      3       \u001b[36m71.7454\u001b[0m       \u001b[32m82.0072\u001b[0m  0.0010  14.1871\n",
      "      3       \u001b[36m71.3979\u001b[0m       \u001b[32m70.8756\u001b[0m  0.0010  14.4481\n",
      "      3       \u001b[36m73.8106\u001b[0m      104.9197  0.0010  14.5051\n",
      "      3       \u001b[36m73.3394\u001b[0m       \u001b[32m65.9932\u001b[0m  0.0010  14.5265\n",
      "      3       \u001b[36m74.4162\u001b[0m       \u001b[32m63.6646\u001b[0m  0.0010  14.6656\n",
      "      4       \u001b[36m65.3786\u001b[0m      109.8720  0.0010  13.6359\n",
      "      4       \u001b[36m66.1868\u001b[0m       \u001b[32m67.8483\u001b[0m  0.0010  13.9701\n",
      "      4       \u001b[36m67.9883\u001b[0m       \u001b[32m92.9873\u001b[0m  0.0010  13.8523\n",
      "      4       \u001b[36m65.2174\u001b[0m       70.3397  0.0010  14.1126\n",
      "      4       \u001b[36m66.6974\u001b[0m       \u001b[32m61.5003\u001b[0m  0.0010  13.8529\n",
      "      5       \u001b[36m57.1699\u001b[0m      121.2900  0.0010  13.9379\n",
      "      5       \u001b[36m61.9896\u001b[0m       \u001b[32m67.5517\u001b[0m  0.0010  14.2278\n",
      "      5       \u001b[36m63.3696\u001b[0m      112.2947  0.0010  14.1869\n",
      "      5       \u001b[36m57.4883\u001b[0m       \u001b[32m62.4535\u001b[0m  0.0010  14.0096\n",
      "      5       \u001b[36m61.2147\u001b[0m       66.8975  0.0010  14.4048\n",
      "      6       \u001b[36m49.4435\u001b[0m      110.6339  0.0010  14.3343\n",
      "      6       \u001b[36m48.9398\u001b[0m       71.6584  0.0010  14.6942\n",
      "      6       \u001b[36m59.4840\u001b[0m      157.9479  0.0010  14.9415\n",
      "      6       \u001b[36m57.9302\u001b[0m       74.2148  0.0010  15.0156\n",
      "      6       \u001b[36m55.6606\u001b[0m       68.8032  0.0010  14.7876\n",
      "      7       \u001b[36m44.3109\u001b[0m       85.4393  0.0010  14.6077\n",
      "      7       \u001b[36m52.2402\u001b[0m       68.8686  0.0010  15.0368\n",
      "      7       \u001b[36m54.5199\u001b[0m      105.4687  0.0010  15.1324\n",
      "      7       \u001b[36m44.4612\u001b[0m       99.0961  0.0010  15.3240\n",
      "      7       \u001b[36m49.1960\u001b[0m       90.8365  0.0010  15.3686\n",
      "      8       \u001b[36m39.3213\u001b[0m       \u001b[32m44.4001\u001b[0m  0.0005  15.9957\n",
      "      8       \u001b[36m48.1076\u001b[0m       \u001b[32m57.3495\u001b[0m  0.0010  16.1063\n",
      "      8       \u001b[36m40.2043\u001b[0m      103.9971  0.0010  16.1512\n",
      "      8       \u001b[36m47.1071\u001b[0m       \u001b[32m51.6719\u001b[0m  0.0010  16.2713\n",
      "      8       \u001b[36m44.7533\u001b[0m      115.2530  0.0010  16.5479\n",
      "      9       \u001b[36m35.5247\u001b[0m       50.5773  0.0005  15.6664\n",
      "      9       \u001b[36m44.8019\u001b[0m       \u001b[32m49.9908\u001b[0m  0.0010  15.6464\n",
      "      9       \u001b[36m33.9791\u001b[0m       75.1117  0.0010  15.5985\n",
      "      9       \u001b[36m40.4772\u001b[0m       67.6510  0.0010  15.6340\n",
      "      9       \u001b[36m42.6010\u001b[0m       \u001b[32m55.0832\u001b[0m  0.0005  15.5347\n",
      "     10       \u001b[36m31.1481\u001b[0m       50.5102  0.0005  14.4477\n",
      "     10       \u001b[36m39.3330\u001b[0m       57.4269  0.0010  14.6053\n",
      "     10       \u001b[36m27.6671\u001b[0m       \u001b[32m62.0904\u001b[0m  0.0005  14.6465\n",
      "     10       \u001b[36m33.3133\u001b[0m       68.6059  0.0010  14.8592\n",
      "     10       \u001b[36m37.3235\u001b[0m       \u001b[32m42.6188\u001b[0m  0.0005  14.6990\n",
      "     11       \u001b[36m26.4877\u001b[0m       44.7340  0.0005  14.0472\n",
      "     11       \u001b[36m32.6888\u001b[0m       53.7821  0.0010  14.2988\n",
      "     11       \u001b[36m23.5136\u001b[0m       \u001b[32m46.3112\u001b[0m  0.0005  14.3089\n",
      "     11       \u001b[36m24.3804\u001b[0m       70.2326  0.0010  14.2412\n",
      "     11       \u001b[36m32.5826\u001b[0m       \u001b[32m40.0812\u001b[0m  0.0005  14.6744\n",
      "     12       \u001b[36m21.0480\u001b[0m       \u001b[32m36.5697\u001b[0m  0.0005  15.2448\n",
      "     12       \u001b[36m21.7970\u001b[0m       \u001b[32m34.1127\u001b[0m  0.0010  15.7525\n",
      "     12       \u001b[36m17.2035\u001b[0m       56.0573  0.0010  15.4997\n",
      "     12       \u001b[36m19.9292\u001b[0m       46.5264  0.0005  15.8770\n",
      "     12       \u001b[36m28.7695\u001b[0m       \u001b[32m38.7565\u001b[0m  0.0005  15.7497\n",
      "     13       \u001b[36m16.7622\u001b[0m       \u001b[32m32.9763\u001b[0m  0.0005  14.6307\n",
      "     13       \u001b[36m15.7971\u001b[0m       48.2693  0.0010  14.5587\n",
      "     13       \u001b[36m13.9830\u001b[0m       \u001b[32m41.9673\u001b[0m  0.0005  14.6748\n",
      "     13       \u001b[36m16.7260\u001b[0m       47.9487  0.0005  14.7042\n",
      "     13       \u001b[36m23.9879\u001b[0m       \u001b[32m38.1265\u001b[0m  0.0005  14.6256\n",
      "     14       \u001b[36m15.0545\u001b[0m       \u001b[32m26.1093\u001b[0m  0.0005  14.3299\n",
      "     14       \u001b[36m13.3090\u001b[0m       36.3172  0.0010  14.2799\n",
      "     14       \u001b[36m14.4306\u001b[0m       49.8201  0.0005  14.1752\n",
      "     14       14.0210       \u001b[32m41.7501\u001b[0m  0.0005  14.4258\n",
      "     14       \u001b[36m17.6418\u001b[0m       \u001b[32m35.7833\u001b[0m  0.0005  14.5430\n",
      "     15       \u001b[36m14.0350\u001b[0m       \u001b[32m23.4821\u001b[0m  0.0005  14.7648\n",
      "     15       \u001b[36m12.1817\u001b[0m       37.6700  0.0010  15.3711\n",
      "     15       \u001b[36m12.9157\u001b[0m       52.3708  0.0005  15.3298\n",
      "     15       \u001b[36m12.5397\u001b[0m       \u001b[32m41.4435\u001b[0m  0.0005  15.4610\n",
      "     15       \u001b[36m13.9971\u001b[0m       \u001b[32m32.3430\u001b[0m  0.0005  15.4558\n",
      "     16       \u001b[36m12.7113\u001b[0m       \u001b[32m23.4646\u001b[0m  0.0005  16.5370\n",
      "     16       \u001b[36m12.1313\u001b[0m       44.4728  0.0010  16.1694\n",
      "     16       \u001b[36m11.7758\u001b[0m       \u001b[32m39.1254\u001b[0m  0.0005  16.3998\n",
      "     16       12.9642       \u001b[32m28.0579\u001b[0m  0.0003  16.7071\n",
      "     16       \u001b[36m12.4968\u001b[0m       \u001b[32m30.2941\u001b[0m  0.0005  16.4748\n",
      "     17       \u001b[36m11.3795\u001b[0m       25.9693  0.0005  16.4189\n",
      "     17       14.5502       45.2093  0.0005  16.3712\n",
      "     17       \u001b[36m11.1880\u001b[0m       \u001b[32m34.6563\u001b[0m  0.0005  16.2729\n",
      "     17       \u001b[36m12.9074\u001b[0m       31.7588  0.0003  16.5078\n",
      "     17       \u001b[36m11.7994\u001b[0m       30.7997  0.0005  16.2580\n",
      "     18       \u001b[36m10.4949\u001b[0m       30.7538  0.0005  16.5941\n",
      "     18       14.9241       35.9784  0.0005  16.4648\n",
      "     18       \u001b[36m10.6175\u001b[0m       \u001b[32m30.9170\u001b[0m  0.0005  16.5413\n",
      "     18       \u001b[36m11.3824\u001b[0m       31.9787  0.0003  16.6717\n",
      "     18       \u001b[36m11.5623\u001b[0m       \u001b[32m29.8159\u001b[0m  0.0005  16.7126\n",
      "     19        \u001b[36m9.9474\u001b[0m       35.5863  0.0005  16.8174\n",
      "     19       12.4474       \u001b[32m27.7684\u001b[0m  0.0005  16.8604\n",
      "     19       \u001b[36m10.1054\u001b[0m       \u001b[32m28.4498\u001b[0m  0.0005  17.1102\n",
      "     19       \u001b[36m10.4908\u001b[0m       31.1242  0.0003  17.2091\n",
      "     19       12.0168       \u001b[32m26.2790\u001b[0m  0.0005  17.1805\n",
      "     20        \u001b[36m9.4837\u001b[0m       42.8768  0.0005  16.2360\n",
      "     20       \u001b[36m10.3046\u001b[0m       \u001b[32m24.4845\u001b[0m  0.0005  16.5125\n",
      "     20        \u001b[36m9.6377\u001b[0m       \u001b[32m27.2457\u001b[0m  0.0005  16.3904\n",
      "     20       \u001b[36m10.0683\u001b[0m       31.5514  0.0003  16.4078\n",
      "     20       12.9143       \u001b[32m23.9491\u001b[0m  0.0005  16.3858\n",
      "     21       10.3092       47.7319  0.0003  16.1979\n",
      "     21        \u001b[36m8.9405\u001b[0m       \u001b[32m23.0144\u001b[0m  0.0005  16.4071\n",
      "     21        \u001b[36m9.1261\u001b[0m       27.3972  0.0005  16.2243\n",
      "     21       10.5835       34.8217  0.0001  16.5969\n",
      "     21       13.0173       32.9741  0.0005  16.5172\n",
      "     22       10.3175       40.6191  0.0003  16.3815\n",
      "     22        \u001b[36m8.2134\u001b[0m       \u001b[32m22.3835\u001b[0m  0.0005  17.0183\n",
      "     22        \u001b[36m8.5912\u001b[0m       27.3444  0.0005  16.8777\n",
      "     22       10.5427       36.3116  0.0001  17.0553\n",
      "     22       12.9799       40.4781  0.0005  17.0027\n",
      "     23        9.8421       31.5344  0.0003  17.5825\n",
      "     23        \u001b[36m7.7842\u001b[0m       \u001b[32m21.9219\u001b[0m  0.0005  18.0699\n",
      "     23        \u001b[36m8.2544\u001b[0m       27.2781  0.0005  17.8023\n",
      "     23        \u001b[36m9.8678\u001b[0m       37.0696  0.0001  18.1005\n",
      "     23       13.4291       71.6399  0.0005  17.6475\n",
      "     24        9.5244       26.9491  0.0003  16.5383\n",
      "     24        \u001b[36m7.4871\u001b[0m       \u001b[32m21.6995\u001b[0m  0.0005  16.3869\n",
      "     24        \u001b[36m8.0489\u001b[0m       27.5470  0.0005  16.2506\n",
      "     24        \u001b[36m9.4152\u001b[0m       36.8696  0.0001  16.8776\n",
      "     24       13.5813       65.7949  0.0005  16.6669\n",
      "     25       10.3718       24.7460  0.0001  16.2626\n",
      "     25        8.7005       37.4583  0.0003  16.1773\n",
      "     25        \u001b[36m7.2708\u001b[0m       \u001b[32m21.4301\u001b[0m  0.0005  16.3370\n",
      "     25        \u001b[36m8.6015\u001b[0m       30.4528  0.0001  15.9244\n",
      "     25       \u001b[36m10.5195\u001b[0m       33.2960  0.0003  16.1115\n",
      "     26        \u001b[36m7.1014\u001b[0m       \u001b[32m21.2599\u001b[0m  0.0005  15.1655\n",
      "     26        9.1942       34.2053  0.0003  15.4370\n",
      "     26        \u001b[36m9.2970\u001b[0m       32.3858  0.0003  15.0979\n",
      "     27        \u001b[36m6.9604\u001b[0m       \u001b[32m21.0745\u001b[0m  0.0005  12.0926\n",
      "     27        8.8059       36.2714  0.0003  11.9173\n",
      "     27        \u001b[36m8.9488\u001b[0m       30.8550  0.0003  11.6687\n",
      "     28        8.5696       36.9689  0.0003  11.2831\n",
      "     28        \u001b[36m6.8514\u001b[0m       \u001b[32m20.9646\u001b[0m  0.0005  11.5479\n",
      "     28        \u001b[36m8.6654\u001b[0m       29.2258  0.0003  11.2927\n",
      "     29        8.6220       34.5831  0.0001  11.9292\n",
      "     29        \u001b[36m6.7637\u001b[0m       \u001b[32m20.7902\u001b[0m  0.0005  11.9827\n",
      "     29        \u001b[36m8.4623\u001b[0m       \u001b[32m23.1965\u001b[0m  0.0001  12.2200\n",
      "     30        \u001b[36m6.6904\u001b[0m       \u001b[32m20.6686\u001b[0m  0.0005  13.5965\n",
      "     30        \u001b[36m8.3133\u001b[0m       \u001b[32m22.6891\u001b[0m  0.0001  13.2260\n",
      "     31        \u001b[36m6.6310\u001b[0m       20.6694  0.0005  9.8761\n",
      "     31        \u001b[36m8.0974\u001b[0m       \u001b[32m22.4580\u001b[0m  0.0001  9.7120\n",
      "     32        \u001b[36m6.5581\u001b[0m       20.7353  0.0005  10.0860\n",
      "     32        \u001b[36m7.9089\u001b[0m       \u001b[32m22.3074\u001b[0m  0.0001  10.2957\n",
      "     33        \u001b[36m6.5405\u001b[0m       20.8101  0.0005  10.3465\n",
      "     33        \u001b[36m7.7534\u001b[0m       \u001b[32m22.1970\u001b[0m  0.0001  10.2079\n",
      "     34        \u001b[36m6.5155\u001b[0m       21.0443  0.0005  9.8030\n",
      "     34        \u001b[36m7.6215\u001b[0m       \u001b[32m22.1002\u001b[0m  0.0001  9.9232\n",
      "     35        7.0904       25.2996  0.0003  10.2499\n",
      "     35        \u001b[36m7.5074\u001b[0m       22.1028  0.0001  10.2432\n",
      "     36        7.3523       24.4434  0.0003  9.7671\n",
      "     36        \u001b[36m7.4084\u001b[0m       \u001b[32m22.0664\u001b[0m  0.0001  9.7224\n",
      "     37        6.8777       24.6409  0.0003  9.6032\n",
      "     37        \u001b[36m7.3202\u001b[0m       \u001b[32m22.0404\u001b[0m  0.0001  9.6784\n",
      "     38        6.5724       24.3151  0.0003  9.7047\n",
      "     38        \u001b[36m7.2417\u001b[0m       \u001b[32m22.0247\u001b[0m  0.0001  9.6217\n",
      "     39        \u001b[36m6.2294\u001b[0m       22.3135  0.0001  9.4282\n",
      "     39        \u001b[36m7.1695\u001b[0m       \u001b[32m22.0160\u001b[0m  0.0001  9.5417\n",
      "     40        \u001b[36m7.1056\u001b[0m       22.0190  0.0001  9.3791\n",
      "     41        \u001b[36m7.0472\u001b[0m       22.0705  0.0001  7.8706\n",
      "     42        \u001b[36m6.9916\u001b[0m       22.1654  0.0001  7.8522\n",
      "     43        \u001b[36m6.9413\u001b[0m       22.2100  0.0001  7.6442\n",
      "     44        \u001b[36m6.8377\u001b[0m       22.5861  0.0001  7.7697\n",
      "     45        \u001b[36m6.7624\u001b[0m       22.9948  0.0001  7.6128\n",
      "     46        \u001b[36m6.7209\u001b[0m       22.9864  0.0001  7.8058\n",
      "     47        \u001b[36m6.6829\u001b[0m       23.0200  0.0001  7.8871\n",
      "     48        \u001b[36m6.5561\u001b[0m       23.4852  0.0000  7.8046\n",
      "RMSE for each folder: [6.15534556 4.68717426 4.93688072 4.41636697 3.63331335]\n",
      "RMSE mean: 4.765816174271922\n"
     ]
    }
   ],
   "source": [
    "#Guided\n",
    "cross_validation_with_scores(x,y,groups,pipe_cnn,logo,rmse_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "080fae37-8d5c-420c-ba16-00a2e9acaf4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m219.1000\u001b[0m      \u001b[32m293.1583\u001b[0m  0.0010  13.9384\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m224.9589\u001b[0m      \u001b[32m242.9936\u001b[0m  0.0010  14.0721\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m222.1733\u001b[0m      \u001b[32m205.4595\u001b[0m  0.0010  14.1591\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m231.9693\u001b[0m      \u001b[32m148.7291\u001b[0m  0.0010  14.1602\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m199.9677\u001b[0m      \u001b[32m221.3613\u001b[0m  0.0010  14.1809\n",
      "      2      \u001b[36m177.8046\u001b[0m      \u001b[32m210.1426\u001b[0m  0.0010  14.1591\n",
      "      2      \u001b[36m162.1591\u001b[0m      \u001b[32m184.0694\u001b[0m  0.0010  14.1777\n",
      "      2      \u001b[36m156.1242\u001b[0m      \u001b[32m254.2190\u001b[0m  0.0010  14.4730\n",
      "      2      \u001b[36m150.9769\u001b[0m      \u001b[32m141.5371\u001b[0m  0.0010  14.2469\n",
      "      2      \u001b[36m166.1331\u001b[0m      \u001b[32m144.2413\u001b[0m  0.0010  14.3413\n",
      "      3      \u001b[36m176.2048\u001b[0m      \u001b[32m197.3821\u001b[0m  0.0010  14.7587\n",
      "      3      \u001b[36m156.3163\u001b[0m      189.9777  0.0010  14.8073\n",
      "      3      \u001b[36m141.6545\u001b[0m      \u001b[32m135.2890\u001b[0m  0.0010  14.8922\n",
      "      3      \u001b[36m146.0714\u001b[0m      257.4220  0.0010  14.9178\n",
      "      3      \u001b[36m162.7391\u001b[0m      \u001b[32m137.0963\u001b[0m  0.0010  14.9614\n",
      "      4      \u001b[36m173.3958\u001b[0m      \u001b[32m193.4380\u001b[0m  0.0010  14.6346\n",
      "      4      \u001b[36m150.2952\u001b[0m      193.9317  0.0010  14.5813\n",
      "      4      \u001b[36m136.6154\u001b[0m      \u001b[32m134.3099\u001b[0m  0.0010  14.6420\n",
      "      4      \u001b[36m141.1076\u001b[0m      259.0159  0.0010  14.7415\n",
      "      4      \u001b[36m159.7538\u001b[0m      \u001b[32m130.4021\u001b[0m  0.0010  14.7152\n",
      "      5      \u001b[36m166.4826\u001b[0m      194.5589  0.0010  14.2701\n",
      "      5      \u001b[36m145.6193\u001b[0m      \u001b[32m183.0582\u001b[0m  0.0010  14.6145\n",
      "      5      \u001b[36m132.1799\u001b[0m      138.3429  0.0010  14.7039\n",
      "      5      \u001b[36m136.1898\u001b[0m      263.2105  0.0010  14.6232\n",
      "      5      \u001b[36m154.4065\u001b[0m      \u001b[32m126.1199\u001b[0m  0.0010  14.5586\n",
      "      6      \u001b[36m161.0219\u001b[0m      \u001b[32m182.2900\u001b[0m  0.0010  14.1554\n",
      "      6      \u001b[36m139.8228\u001b[0m      \u001b[32m181.8283\u001b[0m  0.0010  14.2213\n",
      "      6      \u001b[36m125.7068\u001b[0m      148.9457  0.0010  14.2847\n",
      "      6      \u001b[36m130.4065\u001b[0m      265.1301  0.0010  14.3160\n",
      "      6      \u001b[36m147.2465\u001b[0m      129.7780  0.0010  14.5071\n",
      "      7      \u001b[36m154.9386\u001b[0m      \u001b[32m170.1131\u001b[0m  0.0010  15.1274\n",
      "      7      \u001b[36m134.1331\u001b[0m      \u001b[32m175.1883\u001b[0m  0.0010  15.3805\n",
      "      7      \u001b[36m120.9409\u001b[0m      314.7051  0.0005  15.1766\n",
      "      7      \u001b[36m119.2627\u001b[0m      166.4424  0.0010  15.3496\n",
      "      7      \u001b[36m142.5872\u001b[0m      131.0715  0.0010  15.2449\n",
      "      8      \u001b[36m150.3713\u001b[0m      \u001b[32m165.5861\u001b[0m  0.0010  14.3191\n",
      "      8      136.4665      209.8483  0.0010  14.2748\n",
      "      8      \u001b[36m113.9828\u001b[0m      187.6476  0.0010  14.1833\n",
      "      8      \u001b[36m119.3766\u001b[0m      321.1656  0.0005  14.6094\n",
      "      8      \u001b[36m137.4574\u001b[0m      \u001b[32m118.1850\u001b[0m  0.0010  14.3662\n",
      "      9      \u001b[36m144.2386\u001b[0m      170.9478  0.0010  13.9271\n",
      "      9      \u001b[36m132.1786\u001b[0m      184.6560  0.0010  14.1166\n",
      "      9      \u001b[36m105.2157\u001b[0m      138.2855  0.0005  14.0553\n",
      "      9      \u001b[36m115.6730\u001b[0m      325.9579  0.0005  14.0569\n",
      "      9      \u001b[36m133.1512\u001b[0m      119.2354  0.0010  14.0263\n",
      "     10      \u001b[36m142.5886\u001b[0m      168.9612  0.0010  14.4149\n",
      "     10      133.5078      197.4873  0.0010  14.3903\n",
      "     10      \u001b[36m102.5010\u001b[0m      156.3772  0.0005  14.3345\n",
      "     10      \u001b[36m128.8063\u001b[0m      123.2942  0.0010  14.4275\n",
      "     10      \u001b[36m112.5890\u001b[0m      321.1211  0.0005  14.4851\n",
      "     11      \u001b[36m139.6621\u001b[0m      172.1907  0.0010  14.7536\n",
      "     11      \u001b[36m129.7231\u001b[0m      215.0835  0.0010  15.0305\n",
      "     11       \u001b[36m99.7653\u001b[0m      162.9699  0.0005  15.0673\n",
      "     11      \u001b[36m106.4404\u001b[0m      271.7501  0.0003  15.1068\n",
      "     11      \u001b[36m122.6393\u001b[0m      \u001b[32m117.5987\u001b[0m  0.0010  15.1684\n",
      "     12      141.1559      172.2909  0.0010  15.1755\n",
      "     12      141.5118      \u001b[32m128.6639\u001b[0m  0.0005  15.1984\n",
      "     12       \u001b[36m96.9903\u001b[0m      168.5721  0.0005  15.2853\n",
      "     12      \u001b[36m117.6419\u001b[0m      123.2884  0.0010  15.0792\n",
      "     13      155.6607      \u001b[32m120.6787\u001b[0m  0.0005  13.3346\n",
      "     13      131.2549      138.5970  0.0005  13.2813\n",
      "     13      \u001b[36m112.9057\u001b[0m      121.8150  0.0010  13.0410\n",
      "     13       \u001b[36m90.6999\u001b[0m      138.5236  0.0003  13.3709\n",
      "     14      141.8527      127.8630  0.0005  13.2585\n",
      "     14      \u001b[36m129.1711\u001b[0m      146.5287  0.0005  13.6274\n",
      "     14      \u001b[36m110.4878\u001b[0m      119.9527  0.0010  13.4522\n",
      "     15      \u001b[36m139.1556\u001b[0m      138.2316  0.0005  12.3570\n",
      "     15      \u001b[36m127.6987\u001b[0m      147.5550  0.0005  12.1750\n",
      "     15      \u001b[36m104.8699\u001b[0m      125.9769  0.0010  12.0128\n",
      "     16      \u001b[36m138.1160\u001b[0m      145.7338  0.0005  11.8671\n",
      "     16      116.3839      127.8737  0.0005  12.1551\n",
      "     16      \u001b[36m125.0564\u001b[0m      150.1513  0.0005  12.2477\n",
      "     17      \u001b[36m136.5468\u001b[0m      156.0105  0.0005  11.4539\n",
      "     17      124.6566      \u001b[32m117.4156\u001b[0m  0.0005  11.4534\n",
      "     17      129.1627      144.2184  0.0003  11.4332\n",
      "     18      142.1830      123.1642  0.0003  11.4286\n",
      "     18      116.1441      \u001b[32m115.7990\u001b[0m  0.0005  11.5841\n",
      "     18      131.4540      150.5934  0.0003  11.6348\n",
      "     19      141.9290      124.1816  0.0003  12.1781\n",
      "     19      113.9773      \u001b[32m113.6878\u001b[0m  0.0005  12.6884\n",
      "     19      130.7673      153.7514  0.0003  12.6719\n",
      "     20      140.3256      123.2150  0.0003  13.1349\n",
      "     20      111.6987      \u001b[32m113.3111\u001b[0m  0.0005  13.0033\n",
      "     20      129.6098      153.4353  0.0003  13.1373\n",
      "     21      138.5458      122.7319  0.0003  12.0674\n",
      "     21      109.2259      \u001b[32m110.7101\u001b[0m  0.0005  12.0660\n",
      "     21      132.9765      \u001b[32m124.8128\u001b[0m  0.0001  12.0171\n",
      "     22      140.2374      125.6407  0.0001  11.9406\n",
      "     22      106.0143      \u001b[32m110.3215\u001b[0m  0.0005  12.0971\n",
      "     22      \u001b[36m118.6468\u001b[0m      \u001b[32m123.0215\u001b[0m  0.0001  12.3353\n",
      "     23      \u001b[36m102.9409\u001b[0m      110.8808  0.0005  11.8255\n",
      "     23      \u001b[36m117.2531\u001b[0m      123.0795  0.0001  11.7085\n",
      "     24       \u001b[36m99.9609\u001b[0m      111.4918  0.0005  10.2844\n",
      "     24      \u001b[36m115.8471\u001b[0m      123.2778  0.0001  10.3897\n",
      "     25       \u001b[36m96.0932\u001b[0m      113.8886  0.0005  9.9214\n",
      "     25      \u001b[36m114.5489\u001b[0m      123.4739  0.0001  10.0378\n",
      "     26       \u001b[36m93.8248\u001b[0m      113.7180  0.0005  9.8933\n",
      "     26      \u001b[36m113.3610\u001b[0m      123.6717  0.0001  10.0478\n",
      "     27      106.2938      132.3175  0.0003  10.1849\n",
      "     27      \u001b[36m111.5903\u001b[0m      147.7345  0.0001  10.2469\n",
      "     28      103.4070      126.2022  0.0003  9.8933\n",
      "     28      \u001b[36m107.7993\u001b[0m      144.8521  0.0001  10.0370\n",
      "     29       99.5720      137.4297  0.0003  10.2635\n",
      "     29      \u001b[36m106.6416\u001b[0m      141.7951  0.0001  10.3277\n",
      "     30       99.3771      153.9985  0.0003  10.1346\n",
      "     30      \u001b[36m105.8769\u001b[0m      139.6274  0.0001  10.1470\n",
      "     31      106.9268      125.6265  0.0001  10.2223\n",
      "     31      \u001b[36m103.4313\u001b[0m      138.5633  0.0000  10.3102\n",
      "RMSE for each folder: [11.84212014 15.36254378 11.58963046  9.84877608  9.72626717]\n",
      "RMSE mean: 11.673867526592275\n"
     ]
    }
   ],
   "source": [
    "#Free\n",
    "cross_validation_with_scores(x_f,y_f,groups_f,pipe_cnn,logo,rmse_scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abdb180-6ce2-42f5-8745-04c47650ba73",
   "metadata": {},
   "source": [
    "The mean rmse for this cnn was better than that of the covariance pipeline, but it takes a really long time to execute compared to that method. If only I could have the benefits of both approaches…\n",
    "\n",
    "It was with this train of thought that I created a hybrid between the cnn and the covariance matrice method. For this hybrid, I didn’t need a CNN because the data after the covariance matrice step was  small, so I just built a two-layer nn and the results are very close to the cnn but it runs much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e5ee8f6-481f-4723-bd8c-96a023576f07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "#Small nn\n",
    "class Cov_nn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.s = nn.Sequential(\n",
    "          nn.Linear(36, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(128, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(64, 51)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.s(x)\n",
    "\n",
    "net_nn2 = NeuralNetRegressor(\n",
    "    module      = Cov_nn,\n",
    "    max_epochs  = 200,\n",
    "    lr          = 1e-3,\n",
    "    batch_size  = 64,\n",
    "    optimizer   = torch.optim.Adam,\n",
    "    callbacks=[('earlystop', EarlyStopping('valid_loss', patience=5)), # Stop if validation loss doesn't improve for 5 epochs \n",
    "                    ('lr_sched', LRScheduler(\n",
    "           policy=torch.optim.lr_scheduler.ReduceLROnPlateau, # Halve LR if validation loss stalls for 3 epochs\n",
    "           monitor='valid_loss',\n",
    "           patience=3, factor=0.5))]\n",
    ")\n",
    "\n",
    "\n",
    "pipe_fusion = Pipeline([\n",
    "    ('cov',   Covariances(estimator='oas')),   \n",
    "    ('ts',    TangentSpace()),                \n",
    "    ('scale', StandardScaler()),               \n",
    "    ('cast',  FunctionTransformer(lambda X: X.astype(np.float32), validate=False)), # cast to float32 for PyTorch compatibility\n",
    "    ('nn',   net_nn2)                          \n",
    "])\n",
    "\n",
    "x_cov_nn = X_g_train_reshape.astype('float32')\n",
    "x_cov_nn_f = X_f_train_reshape.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77ca50b6-460a-4868-a4a4-09c76ec4f6ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m463.9438\u001b[0m      \u001b[32m385.1099\u001b[0m  0.0010  0.1092\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m456.9825\u001b[0m      \u001b[32m375.9054\u001b[0m  0.0010  0.1149\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m463.5073\u001b[0m      \u001b[32m379.5745\u001b[0m  0.0010  0.1192\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m467.8356\u001b[0m      \u001b[32m396.4691\u001b[0m  0.0010  0.1442\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m469.2179\u001b[0m      \u001b[32m393.7036\u001b[0m  0.0010  0.1261\n",
      "      2      \u001b[36m270.9305\u001b[0m      \u001b[32m174.6277\u001b[0m  0.0010  0.1313\n",
      "      2      \u001b[36m259.0113\u001b[0m      \u001b[32m171.8371\u001b[0m  0.0010  0.1355\n",
      "      2      \u001b[36m285.5589\u001b[0m      \u001b[32m173.8935\u001b[0m  0.0010  0.1258\n",
      "      2      \u001b[36m264.0672\u001b[0m      \u001b[32m169.8182\u001b[0m  0.0010  0.1336\n",
      "      2      \u001b[36m286.1873\u001b[0m      \u001b[32m180.9821\u001b[0m  0.0010  0.1335\n",
      "      3      \u001b[36m166.1235\u001b[0m      \u001b[32m136.0250\u001b[0m  0.0010  0.1174\n",
      "      3      \u001b[36m163.6912\u001b[0m      \u001b[32m131.6867\u001b[0m  0.0010  0.1073\n",
      "      3      \u001b[36m165.1028\u001b[0m      \u001b[32m141.6717\u001b[0m  0.0010  0.1279\n",
      "      3      \u001b[36m167.3513\u001b[0m      \u001b[32m134.1842\u001b[0m  0.0010  0.1224\n",
      "      3      \u001b[36m171.3194\u001b[0m      \u001b[32m138.8273\u001b[0m  0.0010  0.1183\n",
      "      4      \u001b[36m140.2595\u001b[0m      \u001b[32m122.5045\u001b[0m  0.0010  0.1180\n",
      "      4      \u001b[36m138.1652\u001b[0m      \u001b[32m118.7344\u001b[0m  0.0010  0.1079\n",
      "      4      \u001b[36m142.5947\u001b[0m      \u001b[32m127.5607\u001b[0m  0.0010  0.1195\n",
      "      4      \u001b[36m142.8328\u001b[0m      \u001b[32m122.4727\u001b[0m  0.0010  0.1243\n",
      "      5      \u001b[36m125.2799\u001b[0m      \u001b[32m110.4142\u001b[0m  0.0010  0.1110\n",
      "      5      \u001b[36m119.5609\u001b[0m      \u001b[32m109.6400\u001b[0m  0.0010  0.1307\n",
      "      5      \u001b[36m123.6726\u001b[0m      \u001b[32m114.9299\u001b[0m  0.0010  0.1384\n",
      "      5      \u001b[36m126.7971\u001b[0m      \u001b[32m108.3515\u001b[0m  0.0010  0.1425\n",
      "      4      \u001b[36m143.3042\u001b[0m      \u001b[32m128.3937\u001b[0m  0.0010  0.2738\n",
      "      6      \u001b[36m111.8304\u001b[0m       \u001b[32m99.8386\u001b[0m  0.0010  0.1273\n",
      "      6      \u001b[36m106.6030\u001b[0m      \u001b[32m101.4475\u001b[0m  0.0010  0.1145\n",
      "      6      \u001b[36m109.8918\u001b[0m      \u001b[32m105.6420\u001b[0m  0.0010  0.1127\n",
      "      6      \u001b[36m113.0091\u001b[0m       \u001b[32m97.1786\u001b[0m  0.0010  0.1291\n",
      "      5      \u001b[36m125.1542\u001b[0m      \u001b[32m115.3559\u001b[0m  0.0010  0.1284\n",
      "      7      \u001b[36m100.0579\u001b[0m       \u001b[32m91.1610\u001b[0m  0.0010  0.1335\n",
      "      7       \u001b[36m99.6292\u001b[0m       \u001b[32m94.5959\u001b[0m  0.0010  0.1327\n",
      "      7       \u001b[36m99.0535\u001b[0m       \u001b[32m99.6196\u001b[0m  0.0010  0.1298\n",
      "      7      \u001b[36m103.4149\u001b[0m       \u001b[32m88.6602\u001b[0m  0.0010  0.1333\n",
      "      6      \u001b[36m110.3926\u001b[0m      \u001b[32m105.2985\u001b[0m  0.0010  0.1335\n",
      "      8       \u001b[36m94.1841\u001b[0m       \u001b[32m82.6670\u001b[0m  0.0010  0.1368\n",
      "      8       \u001b[36m94.6272\u001b[0m       \u001b[32m87.4056\u001b[0m  0.0010  0.1410\n",
      "      8       \u001b[36m94.3311\u001b[0m       \u001b[32m94.0371\u001b[0m  0.0010  0.1283\n",
      "      8       \u001b[36m96.5179\u001b[0m       \u001b[32m83.2363\u001b[0m  0.0010  0.1262\n",
      "      7      \u001b[36m101.1171\u001b[0m       \u001b[32m93.6740\u001b[0m  0.0010  0.1341\n",
      "      9       \u001b[36m89.2020\u001b[0m       \u001b[32m77.7299\u001b[0m  0.0010  0.1398\n",
      "      9       \u001b[36m89.8301\u001b[0m       \u001b[32m83.2402\u001b[0m  0.0010  0.1242\n",
      "      9       \u001b[36m90.1638\u001b[0m       \u001b[32m88.4628\u001b[0m  0.0010  0.1413\n",
      "      9       \u001b[36m92.8119\u001b[0m       \u001b[32m78.9277\u001b[0m  0.0010  0.1500\n",
      "      8       \u001b[36m94.9974\u001b[0m       \u001b[32m89.1371\u001b[0m  0.0010  0.1427\n",
      "     10       \u001b[36m85.3140\u001b[0m       \u001b[32m74.1455\u001b[0m  0.0010  0.1477\n",
      "     10       \u001b[36m84.1353\u001b[0m       \u001b[32m78.8097\u001b[0m  0.0010  0.1362\n",
      "     10       \u001b[36m86.6650\u001b[0m       \u001b[32m85.0508\u001b[0m  0.0010  0.1320\n",
      "     10       \u001b[36m89.2218\u001b[0m       \u001b[32m74.6083\u001b[0m  0.0010  0.1132\n",
      "      9       \u001b[36m90.8097\u001b[0m       \u001b[32m85.1828\u001b[0m  0.0010  0.1194\n",
      "     11       \u001b[36m81.0276\u001b[0m       \u001b[32m71.4853\u001b[0m  0.0010  0.1269\n",
      "     11       \u001b[36m82.8335\u001b[0m       \u001b[32m79.6350\u001b[0m  0.0010  0.1089\n",
      "     11       \u001b[36m81.8467\u001b[0m       \u001b[32m75.2443\u001b[0m  0.0010  0.1310\n",
      "     11       \u001b[36m85.5756\u001b[0m       \u001b[32m72.2337\u001b[0m  0.0010  0.1132\n",
      "     10       \u001b[36m85.3985\u001b[0m       \u001b[32m79.6599\u001b[0m  0.0010  0.1205\n",
      "     12       \u001b[36m77.6221\u001b[0m       \u001b[32m69.5153\u001b[0m  0.0010  0.1221\n",
      "     12       \u001b[36m79.5289\u001b[0m       \u001b[32m76.3514\u001b[0m  0.0010  0.1192\n",
      "     12       \u001b[36m77.8244\u001b[0m       \u001b[32m71.5614\u001b[0m  0.0010  0.1422\n",
      "     12       \u001b[36m82.3153\u001b[0m       \u001b[32m70.2095\u001b[0m  0.0010  0.1241\n",
      "     11       \u001b[36m81.3299\u001b[0m       \u001b[32m74.5811\u001b[0m  0.0010  0.1279\n",
      "     13       \u001b[36m78.1653\u001b[0m       \u001b[32m73.1164\u001b[0m  0.0010  0.1235\n",
      "     13       \u001b[36m74.9504\u001b[0m       \u001b[32m66.7715\u001b[0m  0.0010  0.1407\n",
      "     13       \u001b[36m76.0545\u001b[0m       \u001b[32m68.9715\u001b[0m  0.0010  0.1313\n",
      "     13       \u001b[36m80.0905\u001b[0m       \u001b[32m67.4333\u001b[0m  0.0010  0.1274\n",
      "     12       \u001b[36m79.6137\u001b[0m       \u001b[32m73.4517\u001b[0m  0.0010  0.1127\n",
      "     14       \u001b[36m73.9642\u001b[0m       \u001b[32m69.5005\u001b[0m  0.0010  0.1082\n",
      "     14       \u001b[36m73.9335\u001b[0m       \u001b[32m64.4106\u001b[0m  0.0010  0.1173\n",
      "     14       \u001b[36m77.1414\u001b[0m       \u001b[32m65.1467\u001b[0m  0.0010  0.1165\n",
      "     14       \u001b[36m73.7654\u001b[0m       \u001b[32m66.6701\u001b[0m  0.0010  0.1328\n",
      "     13       \u001b[36m74.9251\u001b[0m       \u001b[32m69.8821\u001b[0m  0.0010  0.1308\n",
      "     15       \u001b[36m71.0588\u001b[0m       \u001b[32m66.5852\u001b[0m  0.0010  0.1197\n",
      "     15       \u001b[36m70.6710\u001b[0m       \u001b[32m61.8941\u001b[0m  0.0010  0.1287\n",
      "     15       \u001b[36m74.6614\u001b[0m       \u001b[32m59.9345\u001b[0m  0.0010  0.1180\n",
      "     15       \u001b[36m72.5163\u001b[0m       \u001b[32m63.4119\u001b[0m  0.0010  0.1298\n",
      "     14       \u001b[36m71.7429\u001b[0m       \u001b[32m63.8319\u001b[0m  0.0010  0.1215\n",
      "     16       \u001b[36m68.0049\u001b[0m       \u001b[32m64.4699\u001b[0m  0.0010  0.1266\n",
      "     16       \u001b[36m67.4456\u001b[0m       \u001b[32m57.7348\u001b[0m  0.0010  0.1076\n",
      "     16       \u001b[36m70.9890\u001b[0m       \u001b[32m57.8264\u001b[0m  0.0010  0.1170\n",
      "     16       \u001b[36m67.8510\u001b[0m       \u001b[32m61.3956\u001b[0m  0.0010  0.1145\n",
      "     15       \u001b[36m70.8301\u001b[0m       \u001b[32m61.7751\u001b[0m  0.0010  0.1250\n",
      "     17       \u001b[36m67.5998\u001b[0m       \u001b[32m61.1247\u001b[0m  0.0010  0.1116\n",
      "     17       \u001b[36m64.3414\u001b[0m       \u001b[32m56.7042\u001b[0m  0.0010  0.1168\n",
      "     17       \u001b[36m67.5964\u001b[0m       \u001b[32m55.9380\u001b[0m  0.0010  0.1250\n",
      "     17       \u001b[36m66.8648\u001b[0m       \u001b[32m58.8834\u001b[0m  0.0010  0.1093\n",
      "     16       \u001b[36m68.5999\u001b[0m       \u001b[32m61.2722\u001b[0m  0.0010  0.1217\n",
      "     18       \u001b[36m63.1052\u001b[0m       \u001b[32m58.8184\u001b[0m  0.0010  0.1049\n",
      "     18       \u001b[36m63.4234\u001b[0m       \u001b[32m54.4227\u001b[0m  0.0010  0.1270\n",
      "     18       \u001b[36m65.8292\u001b[0m       \u001b[32m53.7942\u001b[0m  0.0010  0.1031\n",
      "     18       \u001b[36m63.7325\u001b[0m       \u001b[32m58.0325\u001b[0m  0.0010  0.1197\n",
      "     17       \u001b[36m64.9148\u001b[0m       \u001b[32m58.8358\u001b[0m  0.0010  0.1194\n",
      "     19       \u001b[36m61.7925\u001b[0m       \u001b[32m56.8832\u001b[0m  0.0010  0.1164\n",
      "     19       \u001b[36m61.2392\u001b[0m       \u001b[32m52.1273\u001b[0m  0.0010  0.1224\n",
      "     19       \u001b[36m63.9207\u001b[0m       \u001b[32m51.1198\u001b[0m  0.0010  0.1284\n",
      "     19       \u001b[36m59.9501\u001b[0m       \u001b[32m55.3614\u001b[0m  0.0010  0.1146\n",
      "     18       \u001b[36m63.8569\u001b[0m       \u001b[32m57.3887\u001b[0m  0.0010  0.1051\n",
      "     20       \u001b[36m60.6363\u001b[0m       \u001b[32m54.2146\u001b[0m  0.0010  0.1189\n",
      "     20       \u001b[36m63.0297\u001b[0m       \u001b[32m49.9285\u001b[0m  0.0010  0.0967\n",
      "     20       \u001b[36m59.4284\u001b[0m       53.4289  0.0010  0.1238\n",
      "     20       \u001b[36m59.3723\u001b[0m       \u001b[32m53.4230\u001b[0m  0.0010  0.1225\n",
      "     19       \u001b[36m60.5838\u001b[0m       \u001b[32m57.0539\u001b[0m  0.0010  0.1224\n",
      "     21       \u001b[36m57.7445\u001b[0m       \u001b[32m53.7512\u001b[0m  0.0010  0.1152\n",
      "     21       \u001b[36m61.0135\u001b[0m       \u001b[32m47.6086\u001b[0m  0.0010  0.1095\n",
      "     21       \u001b[36m57.0906\u001b[0m       \u001b[32m51.2025\u001b[0m  0.0010  0.1118\n",
      "     21       \u001b[36m57.2218\u001b[0m       \u001b[32m51.2231\u001b[0m  0.0010  0.1140\n",
      "     20       \u001b[36m59.6507\u001b[0m       \u001b[32m51.4040\u001b[0m  0.0010  0.1291\n",
      "     22       \u001b[36m56.0720\u001b[0m       \u001b[32m52.0633\u001b[0m  0.0010  0.1162\n",
      "     22       \u001b[36m60.0520\u001b[0m       \u001b[32m46.7048\u001b[0m  0.0010  0.0994\n",
      "     22       \u001b[36m56.1438\u001b[0m       \u001b[32m48.3702\u001b[0m  0.0010  0.1158\n",
      "     22       \u001b[36m56.9232\u001b[0m       \u001b[32m49.9504\u001b[0m  0.0010  0.1030\n",
      "     21       59.6753       \u001b[32m51.3463\u001b[0m  0.0010  0.1090\n",
      "     23       \u001b[36m54.2855\u001b[0m       \u001b[32m51.2451\u001b[0m  0.0010  0.1097\n",
      "     23       \u001b[36m57.9858\u001b[0m       \u001b[32m44.2576\u001b[0m  0.0010  0.1209\n",
      "     23       \u001b[36m55.3048\u001b[0m       \u001b[32m47.2276\u001b[0m  0.0010  0.1148\n",
      "     23       \u001b[36m54.8297\u001b[0m       \u001b[32m49.0031\u001b[0m  0.0010  0.1128\n",
      "     24       \u001b[36m52.8358\u001b[0m       \u001b[32m50.1160\u001b[0m  0.0010  0.1072\n",
      "     22       \u001b[36m58.4467\u001b[0m       \u001b[32m49.9978\u001b[0m  0.0010  0.1248\n",
      "     24       \u001b[36m56.3207\u001b[0m       \u001b[32m43.5042\u001b[0m  0.0010  0.1208\n",
      "     24       \u001b[36m53.7038\u001b[0m       \u001b[32m46.3046\u001b[0m  0.0010  0.1115\n",
      "     24       \u001b[36m53.6509\u001b[0m       \u001b[32m47.5156\u001b[0m  0.0010  0.1121\n",
      "     25       \u001b[36m51.4983\u001b[0m       50.3061  0.0010  0.1175\n",
      "     23       \u001b[36m56.7611\u001b[0m       \u001b[32m48.7176\u001b[0m  0.0010  0.1297\n",
      "     25       \u001b[36m52.4270\u001b[0m       \u001b[32m45.8929\u001b[0m  0.0010  0.1068\n",
      "     25       \u001b[36m54.5140\u001b[0m       \u001b[32m42.5389\u001b[0m  0.0010  0.1221\n",
      "     25       53.6526       \u001b[32m45.5566\u001b[0m  0.0010  0.1108\n",
      "     26       52.2631       \u001b[32m48.1786\u001b[0m  0.0010  0.1261\n",
      "     24       \u001b[36m55.9767\u001b[0m       \u001b[32m48.5050\u001b[0m  0.0010  0.1219\n",
      "     26       \u001b[36m53.1050\u001b[0m       \u001b[32m40.3970\u001b[0m  0.0010  0.1311\n",
      "     26       \u001b[36m51.6853\u001b[0m       \u001b[32m44.0247\u001b[0m  0.0010  0.1189\n",
      "     26       \u001b[36m52.2442\u001b[0m       \u001b[32m44.4035\u001b[0m  0.0010  0.1395\n",
      "     27       \u001b[36m49.9070\u001b[0m       \u001b[32m47.9499\u001b[0m  0.0010  0.1171\n",
      "     25       \u001b[36m54.8868\u001b[0m       \u001b[32m47.4924\u001b[0m  0.0010  0.1241\n",
      "     27       \u001b[36m51.1116\u001b[0m       \u001b[32m39.6440\u001b[0m  0.0010  0.1100\n",
      "     27       \u001b[36m50.7101\u001b[0m       \u001b[32m43.5252\u001b[0m  0.0010  0.1157\n",
      "     27       \u001b[36m50.3994\u001b[0m       \u001b[32m43.0698\u001b[0m  0.0010  0.1111\n",
      "     28       \u001b[36m49.3068\u001b[0m       \u001b[32m45.9343\u001b[0m  0.0010  0.0948\n",
      "     26       \u001b[36m54.1466\u001b[0m       \u001b[32m44.7477\u001b[0m  0.0010  0.0982\n",
      "     28       51.7856       \u001b[32m39.3127\u001b[0m  0.0010  0.1158\n",
      "     28       \u001b[36m50.2296\u001b[0m       \u001b[32m41.7801\u001b[0m  0.0010  0.1186\n",
      "     28       51.1636       \u001b[32m42.0456\u001b[0m  0.0010  0.1279\n",
      "     29       \u001b[36m47.2208\u001b[0m       \u001b[32m44.6714\u001b[0m  0.0010  0.1109\n",
      "     27       \u001b[36m51.8188\u001b[0m       \u001b[32m43.5908\u001b[0m  0.0010  0.1207\n",
      "     29       \u001b[36m50.3466\u001b[0m       \u001b[32m37.3134\u001b[0m  0.0010  0.1138\n",
      "     29       \u001b[36m48.8336\u001b[0m       \u001b[32m40.3907\u001b[0m  0.0010  0.1009\n",
      "     29       \u001b[36m49.4300\u001b[0m       \u001b[32m41.6089\u001b[0m  0.0010  0.1198\n",
      "     30       \u001b[36m44.8320\u001b[0m       \u001b[32m43.8401\u001b[0m  0.0010  0.1112\n",
      "     28       52.3352       45.6678  0.0010  0.1310\n",
      "     30       \u001b[36m49.9407\u001b[0m       \u001b[32m36.7449\u001b[0m  0.0010  0.1015\n",
      "     30       \u001b[36m48.7749\u001b[0m       40.4681  0.0010  0.1227\n",
      "     30       \u001b[36m46.7646\u001b[0m       \u001b[32m38.9239\u001b[0m  0.0010  0.1098\n",
      "     31       45.2219       \u001b[32m43.3824\u001b[0m  0.0010  0.1008\n",
      "     29       \u001b[36m50.9219\u001b[0m       43.8262  0.0010  0.1024\n",
      "     32       44.8438       44.0197  0.0010  0.0800\n",
      "     31       \u001b[36m46.8096\u001b[0m       \u001b[32m37.6956\u001b[0m  0.0010  0.1072\n",
      "     31       \u001b[36m47.1802\u001b[0m       \u001b[32m35.0870\u001b[0m  0.0010  0.1484\n",
      "     31       47.6986       \u001b[32m38.3770\u001b[0m  0.0010  0.1154\n",
      "     30       \u001b[36m50.8570\u001b[0m       \u001b[32m42.3078\u001b[0m  0.0010  0.1066\n",
      "     32       \u001b[36m46.5837\u001b[0m       38.8976  0.0010  0.1101\n",
      "     33       \u001b[36m42.2692\u001b[0m       \u001b[32m42.9598\u001b[0m  0.0010  0.1246\n",
      "     32       \u001b[36m45.3378\u001b[0m       \u001b[32m37.1505\u001b[0m  0.0010  0.1247\n",
      "     32       48.1934       \u001b[32m34.7830\u001b[0m  0.0010  0.1419\n",
      "     31       50.9618       \u001b[32m39.4524\u001b[0m  0.0010  0.1143\n",
      "     34       42.3581       \u001b[32m40.7697\u001b[0m  0.0010  0.1079\n",
      "     33       45.6337       \u001b[32m36.8108\u001b[0m  0.0010  0.1314\n",
      "     33       \u001b[36m46.6901\u001b[0m       35.1065  0.0010  0.1184\n",
      "     32       \u001b[36m48.1999\u001b[0m       39.8304  0.0010  0.1066\n",
      "     35       \u001b[36m39.9913\u001b[0m       \u001b[32m39.3910\u001b[0m  0.0010  0.1325\n",
      "     33       \u001b[36m45.1347\u001b[0m       \u001b[32m37.4692\u001b[0m  0.0010  0.2667\n",
      "     34       45.3966       \u001b[32m35.3623\u001b[0m  0.0010  0.1124\n",
      "     34       \u001b[36m46.3589\u001b[0m       \u001b[32m33.1014\u001b[0m  0.0010  0.1140\n",
      "     33       49.0302       \u001b[32m38.7576\u001b[0m  0.0010  0.1404\n",
      "     36       41.0457       \u001b[32m38.3606\u001b[0m  0.0010  0.1168\n",
      "     34       \u001b[36m44.0201\u001b[0m       \u001b[32m36.1439\u001b[0m  0.0010  0.1369\n",
      "     35       \u001b[36m44.8862\u001b[0m       36.0409  0.0010  0.1285\n",
      "     35       \u001b[36m44.6468\u001b[0m       33.1281  0.0010  0.1451\n",
      "     34       \u001b[36m46.7077\u001b[0m       \u001b[32m37.0349\u001b[0m  0.0010  0.1245\n",
      "     37       \u001b[36m38.4249\u001b[0m       38.8765  0.0010  0.1358\n",
      "     36       \u001b[36m42.9492\u001b[0m       \u001b[32m34.4662\u001b[0m  0.0010  0.1225\n",
      "     35       \u001b[36m43.3781\u001b[0m       \u001b[32m35.4117\u001b[0m  0.0010  0.1354\n",
      "     36       \u001b[36m43.8089\u001b[0m       \u001b[32m32.0248\u001b[0m  0.0010  0.1221\n",
      "     35       \u001b[36m46.0724\u001b[0m       \u001b[32m36.3271\u001b[0m  0.0010  0.1165\n",
      "     38       39.2269       38.5230  0.0010  0.1167\n",
      "     37       \u001b[36m42.3493\u001b[0m       34.6419  0.0010  0.1142\n",
      "     36       \u001b[36m41.7788\u001b[0m       \u001b[32m33.9083\u001b[0m  0.0010  0.1137\n",
      "     37       44.0669       \u001b[32m31.1686\u001b[0m  0.0010  0.1320\n",
      "     36       46.6166       37.9062  0.0010  0.1194\n",
      "     39       \u001b[36m38.3874\u001b[0m       \u001b[32m37.1639\u001b[0m  0.0010  0.1285\n",
      "     37       42.1190       34.2152  0.0010  0.1208\n",
      "     38       42.4633       \u001b[32m33.7685\u001b[0m  0.0010  0.1454\n",
      "     38       43.9765       \u001b[32m30.7894\u001b[0m  0.0010  0.1119\n",
      "     37       \u001b[36m45.1124\u001b[0m       \u001b[32m35.0287\u001b[0m  0.0010  0.1327\n",
      "     40       \u001b[36m37.9584\u001b[0m       \u001b[32m36.3849\u001b[0m  0.0010  0.1204\n",
      "     38       42.4190       \u001b[32m32.6373\u001b[0m  0.0010  0.1379\n",
      "     39       \u001b[36m42.6591\u001b[0m       \u001b[32m29.4977\u001b[0m  0.0010  0.1209\n",
      "     39       \u001b[36m41.1586\u001b[0m       \u001b[32m33.4640\u001b[0m  0.0010  0.1413\n",
      "     38       45.5523       35.8505  0.0010  0.1314\n",
      "     41       \u001b[36m37.1815\u001b[0m       \u001b[32m35.8557\u001b[0m  0.0010  0.1410\n",
      "     39       \u001b[36m40.3522\u001b[0m       32.8801  0.0010  0.1238\n",
      "     40       \u001b[36m42.4596\u001b[0m       29.8610  0.0010  0.1236\n",
      "     40       42.0684       \u001b[32m32.6038\u001b[0m  0.0010  0.1307\n",
      "     39       45.8974       \u001b[32m34.5383\u001b[0m  0.0010  0.1323\n",
      "     42       \u001b[36m34.3358\u001b[0m       \u001b[32m35.5501\u001b[0m  0.0010  0.1230\n",
      "     40       41.4384       \u001b[32m31.3000\u001b[0m  0.0010  0.1068\n",
      "     41       \u001b[36m41.6419\u001b[0m       \u001b[32m28.9773\u001b[0m  0.0010  0.1333\n",
      "     41       41.2172       \u001b[32m32.5751\u001b[0m  0.0010  0.1376\n",
      "     40       \u001b[36m44.3525\u001b[0m       35.4612  0.0010  0.1433\n",
      "     41       \u001b[36m39.8563\u001b[0m       31.4164  0.0010  0.1008\n",
      "     43       36.8434       35.6614  0.0010  0.1479\n",
      "     42       \u001b[36m41.0303\u001b[0m       \u001b[32m32.1587\u001b[0m  0.0010  0.1067\n",
      "     42       \u001b[36m41.0044\u001b[0m       29.5573  0.0010  0.1427\n",
      "     41       44.4425       \u001b[32m31.3594\u001b[0m  0.0010  0.1012\n",
      "     42       \u001b[36m38.8028\u001b[0m       32.2708  0.0010  0.1239\n",
      "     43       \u001b[36m39.6544\u001b[0m       \u001b[32m30.9880\u001b[0m  0.0010  0.1164\n",
      "     43       \u001b[36m39.9351\u001b[0m       \u001b[32m27.4681\u001b[0m  0.0010  0.1257\n",
      "     42       \u001b[36m42.7765\u001b[0m       32.8775  0.0010  0.1338\n",
      "     43       \u001b[36m38.5748\u001b[0m       \u001b[32m30.0835\u001b[0m  0.0010  0.1095\n",
      "     44       35.7379       \u001b[32m35.2724\u001b[0m  0.0010  0.2560\n",
      "     43       \u001b[36m42.4728\u001b[0m       33.0118  0.0010  0.1121\n",
      "     44       40.8423       28.6777  0.0010  0.1403\n",
      "     44       \u001b[36m37.8039\u001b[0m       \u001b[32m29.9511\u001b[0m  0.0010  0.1256\n",
      "     45       35.1026       \u001b[32m34.1245\u001b[0m  0.0010  0.1458\n",
      "     45       \u001b[36m39.1444\u001b[0m       27.9525  0.0010  0.1803\n",
      "     44       39.9187       \u001b[32m30.4853\u001b[0m  0.0010  0.3478\n",
      "     45       \u001b[36m37.6141\u001b[0m       \u001b[32m29.2149\u001b[0m  0.0010  0.1686\n",
      "     44       \u001b[36m40.8572\u001b[0m       32.2082  0.0010  0.1998\n",
      "     46       34.9566       35.7466  0.0010  0.1613\n",
      "     45       40.3163       30.9497  0.0010  0.1138\n",
      "     45       41.1799       \u001b[32m31.2198\u001b[0m  0.0010  0.1161\n",
      "     46       39.2974       \u001b[32m26.6778\u001b[0m  0.0010  0.1341\n",
      "     46       \u001b[36m36.4176\u001b[0m       29.2995  0.0010  0.1273\n",
      "     47       \u001b[36m34.0836\u001b[0m       34.1370  0.0010  0.1198\n",
      "     46       \u001b[36m39.4225\u001b[0m       \u001b[32m30.2652\u001b[0m  0.0010  0.1201\n",
      "     47       \u001b[36m38.8713\u001b[0m       27.4081  0.0010  0.1096\n",
      "     47       37.0153       \u001b[32m28.8871\u001b[0m  0.0010  0.1173\n",
      "     46       \u001b[36m40.5509\u001b[0m       \u001b[32m30.6595\u001b[0m  0.0010  0.1357\n",
      "     48       34.2231       \u001b[32m33.8278\u001b[0m  0.0010  0.1387\n",
      "     48       39.6268       \u001b[32m26.5093\u001b[0m  0.0010  0.1101\n",
      "     47       \u001b[36m39.3311\u001b[0m       \u001b[32m29.4662\u001b[0m  0.0010  0.1273\n",
      "     48       36.9389       \u001b[32m28.6181\u001b[0m  0.0010  0.1234\n",
      "     47       41.3392       \u001b[32m30.0098\u001b[0m  0.0010  0.1261\n",
      "     49       34.2890       \u001b[32m33.3627\u001b[0m  0.0010  0.1208\n",
      "     48       \u001b[36m38.1864\u001b[0m       29.9711  0.0010  0.1163\n",
      "     49       \u001b[36m36.2771\u001b[0m       \u001b[32m27.8381\u001b[0m  0.0010  0.1109\n",
      "     49       \u001b[36m38.2450\u001b[0m       \u001b[32m25.5939\u001b[0m  0.0010  0.1365\n",
      "     48       \u001b[36m40.2627\u001b[0m       30.7515  0.0010  0.1234\n",
      "     49       38.3229       29.6403  0.0010  0.1062\n",
      "     50       \u001b[36m33.9664\u001b[0m       33.8058  0.0010  0.1263\n",
      "     50       \u001b[36m35.7212\u001b[0m       \u001b[32m27.0578\u001b[0m  0.0010  0.1028\n",
      "     50       \u001b[36m37.1528\u001b[0m       \u001b[32m25.5226\u001b[0m  0.0010  0.1292\n",
      "     49       40.4522       \u001b[32m29.1222\u001b[0m  0.0010  0.1204\n",
      "     51       \u001b[36m33.8988\u001b[0m       \u001b[32m32.0576\u001b[0m  0.0010  0.1089\n",
      "     51       \u001b[36m34.4480\u001b[0m       \u001b[32m26.2170\u001b[0m  0.0010  0.1254\n",
      "     50       38.3208       \u001b[32m29.0245\u001b[0m  0.0010  0.1289\n",
      "     51       37.4315       \u001b[32m25.4566\u001b[0m  0.0010  0.1256\n",
      "     50       \u001b[36m39.9309\u001b[0m       \u001b[32m29.0920\u001b[0m  0.0010  0.1273\n",
      "     52       \u001b[36m33.4483\u001b[0m       32.6872  0.0010  0.0952\n",
      "     52       34.8799       \u001b[32m26.1088\u001b[0m  0.0010  0.1127\n",
      "     51       38.7153       29.4965  0.0010  0.1292\n",
      "     52       37.5062       \u001b[32m25.0632\u001b[0m  0.0010  0.1223\n",
      "     51       \u001b[36m39.5188\u001b[0m       \u001b[32m28.9181\u001b[0m  0.0010  0.1132\n",
      "     53       \u001b[36m32.2789\u001b[0m       33.7289  0.0010  0.1261\n",
      "     53       35.7192       \u001b[32m26.1021\u001b[0m  0.0010  0.1225\n",
      "     52       \u001b[36m36.8549\u001b[0m       \u001b[32m28.7117\u001b[0m  0.0010  0.1216\n",
      "     52       \u001b[36m39.0328\u001b[0m       \u001b[32m27.8377\u001b[0m  0.0010  0.1053\n",
      "     53       \u001b[36m36.3964\u001b[0m       25.3438  0.0010  0.1216\n",
      "     54       \u001b[36m32.2179\u001b[0m       \u001b[32m31.9206\u001b[0m  0.0010  0.1203\n",
      "     53       \u001b[36m36.6998\u001b[0m       \u001b[32m27.9989\u001b[0m  0.0010  0.1005\n",
      "     54       \u001b[36m33.5518\u001b[0m       \u001b[32m25.6766\u001b[0m  0.0010  0.1246\n",
      "     54       \u001b[36m36.1394\u001b[0m       \u001b[32m24.5075\u001b[0m  0.0010  0.1203\n",
      "     53       \u001b[36m38.5597\u001b[0m       29.2703  0.0010  0.1366\n",
      "     55       32.9864       \u001b[32m31.5992\u001b[0m  0.0010  0.1271\n",
      "     55       33.8687       26.4037  0.0010  0.1164\n",
      "     54       \u001b[36m36.4153\u001b[0m       \u001b[32m27.8601\u001b[0m  0.0010  0.1326\n",
      "     55       \u001b[36m35.8119\u001b[0m       \u001b[32m23.5883\u001b[0m  0.0010  0.1155\n",
      "     54       \u001b[36m38.1983\u001b[0m       29.9667  0.0010  0.1155\n",
      "     56       \u001b[36m32.0007\u001b[0m       33.9177  0.0010  0.1017\n",
      "     55       37.8167       \u001b[32m27.6294\u001b[0m  0.0010  0.1286\n",
      "     56       34.0412       \u001b[32m24.6556\u001b[0m  0.0010  0.1462\n",
      "     55       \u001b[36m37.6079\u001b[0m       28.1878  0.0010  0.1082\n",
      "     56       \u001b[36m35.0774\u001b[0m       24.1202  0.0010  0.1292\n",
      "     57       \u001b[36m31.8977\u001b[0m       \u001b[32m30.6949\u001b[0m  0.0010  0.1091\n",
      "     56       \u001b[36m35.7533\u001b[0m       \u001b[32m27.4020\u001b[0m  0.0010  0.1062\n",
      "     57       33.8179       25.0529  0.0010  0.1354\n",
      "     57       35.8743       24.0433  0.0010  0.1267\n",
      "     56       \u001b[36m37.4022\u001b[0m       \u001b[32m27.6256\u001b[0m  0.0010  0.1377\n",
      "     58       32.0683       31.8206  0.0010  0.1214\n",
      "     57       37.3208       \u001b[32m26.8719\u001b[0m  0.0010  0.1200\n",
      "     58       33.8202       \u001b[32m24.2543\u001b[0m  0.0010  0.1211\n",
      "     57       38.0023       27.8060  0.0010  0.1176\n",
      "     59       \u001b[36m31.3743\u001b[0m       \u001b[32m30.3616\u001b[0m  0.0010  0.1174\n",
      "     58       \u001b[36m33.8889\u001b[0m       24.2614  0.0010  0.1373\n",
      "     58       \u001b[36m35.0666\u001b[0m       \u001b[32m26.8148\u001b[0m  0.0010  0.1273\n",
      "     59       \u001b[36m32.8422\u001b[0m       \u001b[32m23.3429\u001b[0m  0.0010  0.1160\n",
      "     58       \u001b[36m37.2582\u001b[0m       \u001b[32m25.7484\u001b[0m  0.0010  0.1182\n",
      "     59       35.2007       \u001b[32m23.3738\u001b[0m  0.0010  0.1111\n",
      "     60       32.4594       31.2589  0.0010  0.1237\n",
      "     59       36.2196       26.8610  0.0010  0.1096\n",
      "     60       33.5802       24.2321  0.0010  0.1013\n",
      "     60       33.9799       23.3946  0.0010  0.1006\n",
      "     59       \u001b[36m36.2910\u001b[0m       25.8349  0.0010  0.1268\n",
      "     61       31.7573       31.4109  0.0010  0.1244\n",
      "     60       \u001b[36m34.6601\u001b[0m       \u001b[32m26.1918\u001b[0m  0.0010  0.1351\n",
      "     61       35.2332       \u001b[32m22.9649\u001b[0m  0.0010  0.1154\n",
      "     61       \u001b[36m31.6747\u001b[0m       23.7167  0.0010  0.1414\n",
      "     60       37.3090       \u001b[32m25.3739\u001b[0m  0.0010  0.1219\n",
      "     62       \u001b[36m29.9265\u001b[0m       30.8295  0.0010  0.1315\n",
      "     61       34.7809       26.4414  0.0010  0.1104\n",
      "     62       34.1049       23.2254  0.0010  0.1190\n",
      "     62       32.7153       24.0160  0.0010  0.1286\n",
      "     63       30.9615       31.1111  0.0010  0.1164\n",
      "     61       \u001b[36m36.2479\u001b[0m       26.1719  0.0010  0.1434\n",
      "     62       35.3767       26.2774  0.0010  0.1076\n",
      "     63       \u001b[36m33.4263\u001b[0m       \u001b[32m22.6080\u001b[0m  0.0010  0.1245\n",
      "     63       32.1926       24.1003  0.0010  0.1334\n",
      "     62       \u001b[36m35.6282\u001b[0m       25.6427  0.0010  0.1291\n",
      "     63       \u001b[36m34.5109\u001b[0m       \u001b[32m25.9936\u001b[0m  0.0010  0.1383\n",
      "     64       33.7065       \u001b[32m21.9880\u001b[0m  0.0010  0.1312\n",
      "     64       \u001b[36m31.6178\u001b[0m       \u001b[32m23.0645\u001b[0m  0.0005  0.1144\n",
      "     63       36.2493       26.1571  0.0010  0.1238\n",
      "     64       \u001b[36m33.0090\u001b[0m       \u001b[32m25.4628\u001b[0m  0.0010  0.1225\n",
      "     65       \u001b[36m32.3240\u001b[0m       \u001b[32m21.0851\u001b[0m  0.0010  0.1171\n",
      "     65       \u001b[36m31.0951\u001b[0m       \u001b[32m22.8609\u001b[0m  0.0005  0.1185\n",
      "     64       36.2278       \u001b[32m25.2533\u001b[0m  0.0010  0.1318\n",
      "     65       34.3064       25.5988  0.0010  0.1273\n",
      "     66       33.0106       22.0392  0.0010  0.1433\n",
      "     66       32.8123       \u001b[32m22.4484\u001b[0m  0.0005  0.1535\n",
      "     65       \u001b[36m34.3590\u001b[0m       \u001b[32m24.7809\u001b[0m  0.0010  0.1441\n",
      "     66       \u001b[36m32.6849\u001b[0m       \u001b[32m25.4316\u001b[0m  0.0010  0.1471\n",
      "     67       34.4589       21.1309  0.0010  0.1210\n",
      "     67       \u001b[36m30.1444\u001b[0m       22.5476  0.0005  0.1152\n",
      "     66       35.3126       \u001b[32m23.3146\u001b[0m  0.0010  0.1061\n",
      "     67       33.1491       \u001b[32m25.1865\u001b[0m  0.0010  0.1287\n",
      "     68       33.5315       21.9195  0.0010  0.1248\n",
      "     68       \u001b[36m30.0063\u001b[0m       23.1835  0.0005  0.1158\n",
      "     67       34.8045       23.8345  0.0010  0.1154\n",
      "     68       33.3236       25.1978  0.0010  0.0955\n",
      "     69       33.6201       21.7278  0.0010  0.1047\n",
      "     69       31.0145       \u001b[32m22.3602\u001b[0m  0.0005  0.1087\n",
      "     68       \u001b[36m34.2960\u001b[0m       24.7212  0.0010  0.1117\n",
      "     69       \u001b[36m32.0792\u001b[0m       \u001b[32m24.2205\u001b[0m  0.0010  0.1072\n",
      "     70       \u001b[36m32.0065\u001b[0m       \u001b[32m20.8695\u001b[0m  0.0005  0.1128\n",
      "     70       30.4330       \u001b[32m22.2828\u001b[0m  0.0005  0.1156\n",
      "     69       36.2704       25.1468  0.0010  0.1254\n",
      "     70       32.7459       24.2937  0.0010  0.1228\n",
      "     71       30.5312       \u001b[32m22.0200\u001b[0m  0.0005  0.1037\n",
      "     71       \u001b[36m30.9743\u001b[0m       20.9386  0.0005  0.1204\n",
      "     70       \u001b[36m34.1421\u001b[0m       24.5552  0.0010  0.1069\n",
      "     71       32.4941       24.8212  0.0010  0.1113\n",
      "     72       30.7485       22.0844  0.0005  0.1167\n",
      "     72       31.4852       \u001b[32m20.8377\u001b[0m  0.0005  0.1267\n",
      "     72       \u001b[36m31.6439\u001b[0m       24.4875  0.0010  0.1033\n",
      "     73       \u001b[36m29.9865\u001b[0m       \u001b[32m21.6681\u001b[0m  0.0005  0.1108\n",
      "     73       32.3926       \u001b[32m20.8197\u001b[0m  0.0005  0.1207\n",
      "     73       \u001b[36m31.5736\u001b[0m       24.3067  0.0010  0.1189\n",
      "     74       \u001b[36m29.6505\u001b[0m       \u001b[32m21.4576\u001b[0m  0.0005  0.1207\n",
      "     74       \u001b[36m30.5979\u001b[0m       20.9622  0.0005  0.1122\n",
      "     74       31.8564       \u001b[32m23.8710\u001b[0m  0.0005  0.1206\n",
      "     75       30.2825       22.0491  0.0005  0.1048\n",
      "     75       31.5309       \u001b[32m20.6192\u001b[0m  0.0005  0.1054\n",
      "     75       \u001b[36m30.1529\u001b[0m       \u001b[32m23.8363\u001b[0m  0.0005  0.1039\n",
      "     76       30.4669       21.5623  0.0005  0.1164\n",
      "     76       32.6744       \u001b[32m20.1845\u001b[0m  0.0005  0.1859\n",
      "     77       30.8067       21.9969  0.0005  0.1342\n",
      "     76       31.3639       23.8771  0.0005  0.1780\n",
      "     77       \u001b[36m30.5884\u001b[0m       \u001b[32m20.1374\u001b[0m  0.0005  0.1005\n",
      "     77       30.8124       24.1357  0.0005  0.1006\n",
      "     78       30.0625       \u001b[32m21.0434\u001b[0m  0.0005  0.1141\n",
      "     78       \u001b[36m30.5411\u001b[0m       20.3697  0.0005  0.0968\n",
      "     79       30.4981       21.7497  0.0005  0.0878\n",
      "     78       31.1428       \u001b[32m23.7786\u001b[0m  0.0005  0.1001\n",
      "     79       31.5470       20.2616  0.0005  0.0962\n",
      "     80       30.0573       21.8238  0.0005  0.1008\n",
      "     79       31.1768       \u001b[32m23.4008\u001b[0m  0.0005  0.1004\n",
      "     80       31.4595       \u001b[32m19.8510\u001b[0m  0.0005  0.1096\n",
      "     80       \u001b[36m30.1183\u001b[0m       23.5261  0.0005  0.0958\n",
      "     81       \u001b[36m28.5488\u001b[0m       21.6269  0.0005  0.1108\n",
      "     81       \u001b[36m30.4677\u001b[0m       20.1384  0.0005  0.0981\n",
      "     81       30.6871       \u001b[32m23.3253\u001b[0m  0.0005  0.0819\n",
      "     82       29.8647       21.2181  0.0005  0.0870\n",
      "     82       32.5118       \u001b[32m19.6112\u001b[0m  0.0005  0.0975\n",
      "     82       \u001b[36m29.7990\u001b[0m       \u001b[32m23.1978\u001b[0m  0.0005  0.0929\n",
      "     83       31.4222       20.7260  0.0005  0.0850\n",
      "     83       30.8677       \u001b[32m23.1284\u001b[0m  0.0005  0.0821\n",
      "     84       30.8211       \u001b[32m19.3593\u001b[0m  0.0005  0.0773\n",
      "     84       30.0538       23.2995  0.0005  0.0784\n",
      "     85       30.6985       19.4538  0.0005  0.0869\n",
      "     85       \u001b[36m29.4338\u001b[0m       23.1376  0.0005  0.0853\n",
      "     86       29.7358       \u001b[32m22.9136\u001b[0m  0.0005  0.0734\n",
      "     86       \u001b[36m30.0968\u001b[0m       19.6043  0.0005  0.1147\n",
      "     87       \u001b[36m29.4196\u001b[0m       \u001b[32m22.8539\u001b[0m  0.0005  0.0737\n",
      "     87       30.6799       19.3937  0.0005  0.0846\n",
      "     88       \u001b[36m29.2199\u001b[0m       \u001b[32m22.4834\u001b[0m  0.0005  0.0754\n",
      "     88       \u001b[36m29.5133\u001b[0m       19.4534  0.0005  0.0915\n",
      "     89       29.7225       \u001b[32m22.4087\u001b[0m  0.0005  0.0982\n",
      "     89       30.3826       \u001b[32m19.3515\u001b[0m  0.0003  0.0950\n",
      "     90       30.0425       22.8718  0.0005  0.0825\n",
      "     90       29.6847       \u001b[32m19.0517\u001b[0m  0.0003  0.0772\n",
      "     91       \u001b[36m28.8102\u001b[0m       \u001b[32m22.2957\u001b[0m  0.0005  0.0839\n",
      "     91       \u001b[36m29.5083\u001b[0m       \u001b[32m18.8270\u001b[0m  0.0003  0.0818\n",
      "     92       29.3203       \u001b[32m21.8214\u001b[0m  0.0005  0.0800\n",
      "     92       30.0093       19.1828  0.0003  0.0789\n",
      "     93       30.5787       22.2227  0.0005  0.0824\n",
      "     93       \u001b[36m28.7741\u001b[0m       19.0347  0.0003  0.0972\n",
      "     94       30.5110       22.5456  0.0005  0.0850\n",
      "     94       30.3536       18.8982  0.0003  0.0821\n",
      "     95       \u001b[36m28.0945\u001b[0m       22.2236  0.0005  0.0776\n",
      "     95       30.4010       \u001b[32m18.7772\u001b[0m  0.0003  0.0785\n",
      "     96       29.6966       22.3306  0.0005  0.0900\n",
      "     96       29.4745       18.9204  0.0003  0.0896\n",
      "     97       30.3331       18.9546  0.0003  0.0898\n",
      "     98       29.6410       18.8392  0.0003  0.0759\n",
      "     99       30.2766       18.7942  0.0003  0.0623\n",
      "RMSE for each folder: [5.87956904 5.09309729 5.85883917 4.43030728 4.4797616 ]\n",
      "RMSE mean: 5.148314877411356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m701.7604\u001b[0m      \u001b[32m465.9023\u001b[0m  0.0010  0.1618\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m640.2996\u001b[0m      \u001b[32m458.6602\u001b[0m  0.0010  0.1509\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m569.2093\u001b[0m      \u001b[32m838.9061\u001b[0m  0.0010  0.1490\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m676.5895\u001b[0m      \u001b[32m444.7971\u001b[0m  0.0010  0.1426\n",
      "      2      \u001b[36m391.0130\u001b[0m      \u001b[32m191.6481\u001b[0m  0.0010  0.1565\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m533.0908\u001b[0m      \u001b[32m440.4294\u001b[0m  0.0010  0.1422\n",
      "      2      \u001b[36m331.2604\u001b[0m      \u001b[32m189.6586\u001b[0m  0.0010  0.1368\n",
      "      2      \u001b[36m333.6544\u001b[0m      \u001b[32m336.8626\u001b[0m  0.0010  0.1445\n",
      "      3      \u001b[36m188.8320\u001b[0m      \u001b[32m145.1500\u001b[0m  0.0010  0.1374\n",
      "      2      \u001b[36m364.0546\u001b[0m      \u001b[32m183.3906\u001b[0m  0.0010  0.1543\n",
      "      2      \u001b[36m271.6770\u001b[0m      \u001b[32m190.2349\u001b[0m  0.0010  0.1314\n",
      "      3      \u001b[36m166.4569\u001b[0m      \u001b[32m148.8172\u001b[0m  0.0010  0.1381\n",
      "      3      \u001b[36m164.6791\u001b[0m      \u001b[32m214.8662\u001b[0m  0.0010  0.1429\n",
      "      3      \u001b[36m178.0269\u001b[0m      \u001b[32m144.9717\u001b[0m  0.0010  0.1558\n",
      "      3      \u001b[36m149.8682\u001b[0m      \u001b[32m153.8255\u001b[0m  0.0010  0.1703\n",
      "      4      \u001b[36m161.9166\u001b[0m      \u001b[32m134.2452\u001b[0m  0.0010  0.1848\n",
      "      4      \u001b[36m145.8264\u001b[0m      \u001b[32m138.2297\u001b[0m  0.0010  0.1912\n",
      "      4      \u001b[36m142.8110\u001b[0m      \u001b[32m194.5061\u001b[0m  0.0010  0.1944\n",
      "      4      \u001b[36m156.1245\u001b[0m      \u001b[32m136.9918\u001b[0m  0.0010  0.1614\n",
      "      4      \u001b[36m137.1184\u001b[0m      \u001b[32m142.0424\u001b[0m  0.0010  0.1533\n",
      "      5      \u001b[36m154.1965\u001b[0m      \u001b[32m128.7972\u001b[0m  0.0010  0.1727\n",
      "      5      \u001b[36m139.0814\u001b[0m      \u001b[32m132.8610\u001b[0m  0.0010  0.1513\n",
      "      5      \u001b[36m136.4596\u001b[0m      \u001b[32m185.3890\u001b[0m  0.0010  0.1651\n",
      "      5      \u001b[36m131.0460\u001b[0m      \u001b[32m135.7283\u001b[0m  0.0010  0.1308\n",
      "      5      \u001b[36m150.1574\u001b[0m      \u001b[32m133.4234\u001b[0m  0.0010  0.1655\n",
      "      6      \u001b[36m147.6586\u001b[0m      \u001b[32m125.0631\u001b[0m  0.0010  0.1464\n",
      "      6      \u001b[36m134.6996\u001b[0m      \u001b[32m129.7566\u001b[0m  0.0010  0.1397\n",
      "      6      \u001b[36m132.6705\u001b[0m      \u001b[32m175.6321\u001b[0m  0.0010  0.1425\n",
      "      6      \u001b[36m126.3648\u001b[0m      \u001b[32m132.8794\u001b[0m  0.0010  0.1497\n",
      "      6      \u001b[36m142.8731\u001b[0m      \u001b[32m130.9779\u001b[0m  0.0010  0.1593\n",
      "      7      \u001b[36m142.9846\u001b[0m      \u001b[32m122.5169\u001b[0m  0.0010  0.1650\n",
      "      7      \u001b[36m129.3499\u001b[0m      \u001b[32m127.4970\u001b[0m  0.0010  0.1757\n",
      "      7      \u001b[36m126.7285\u001b[0m      \u001b[32m170.1192\u001b[0m  0.0010  0.1602\n",
      "      7      \u001b[36m122.4248\u001b[0m      \u001b[32m130.5200\u001b[0m  0.0010  0.1539\n",
      "      7      \u001b[36m138.7170\u001b[0m      \u001b[32m127.6570\u001b[0m  0.0010  0.1484\n",
      "      8      \u001b[36m139.8576\u001b[0m      \u001b[32m120.7446\u001b[0m  0.0010  0.1412\n",
      "      8      \u001b[36m125.3534\u001b[0m      \u001b[32m125.7679\u001b[0m  0.0010  0.1431\n",
      "      8      \u001b[36m124.5430\u001b[0m      \u001b[32m162.3007\u001b[0m  0.0010  0.1535\n",
      "      8      \u001b[36m119.1695\u001b[0m      \u001b[32m128.7722\u001b[0m  0.0010  0.1586\n",
      "      8      \u001b[36m135.9208\u001b[0m      \u001b[32m126.6961\u001b[0m  0.0010  0.1474\n",
      "      9      \u001b[36m134.9213\u001b[0m      \u001b[32m119.0696\u001b[0m  0.0010  0.1580\n",
      "      9      \u001b[36m122.8483\u001b[0m      \u001b[32m124.9815\u001b[0m  0.0010  0.1434\n",
      "      9      \u001b[36m119.6817\u001b[0m      \u001b[32m156.9644\u001b[0m  0.0010  0.1478\n",
      "      9      \u001b[36m115.7584\u001b[0m      \u001b[32m126.8985\u001b[0m  0.0010  0.1585\n",
      "      9      \u001b[36m131.7587\u001b[0m      \u001b[32m126.1653\u001b[0m  0.0010  0.1586\n",
      "     10      \u001b[36m131.5476\u001b[0m      \u001b[32m118.0507\u001b[0m  0.0010  0.1637\n",
      "     10      \u001b[36m117.9736\u001b[0m      \u001b[32m123.6420\u001b[0m  0.0010  0.1410\n",
      "     10      \u001b[36m117.2193\u001b[0m      \u001b[32m151.7451\u001b[0m  0.0010  0.1614\n",
      "     10      \u001b[36m113.7490\u001b[0m      \u001b[32m125.8274\u001b[0m  0.0010  0.1657\n",
      "     10      \u001b[36m128.1687\u001b[0m      \u001b[32m124.9691\u001b[0m  0.0010  0.1535\n",
      "     11      \u001b[36m128.8049\u001b[0m      \u001b[32m117.3747\u001b[0m  0.0010  0.1482\n",
      "     11      \u001b[36m115.1177\u001b[0m      \u001b[32m123.0845\u001b[0m  0.0010  0.1547\n",
      "     11      \u001b[36m114.2023\u001b[0m      \u001b[32m147.2735\u001b[0m  0.0010  0.1499\n",
      "     11      \u001b[36m111.5147\u001b[0m      \u001b[32m124.0824\u001b[0m  0.0010  0.1497\n",
      "     11      \u001b[36m127.1991\u001b[0m      \u001b[32m124.7834\u001b[0m  0.0010  0.1689\n",
      "     12      \u001b[36m113.0442\u001b[0m      123.6574  0.0010  0.1460\n",
      "     12      \u001b[36m128.6672\u001b[0m      \u001b[32m116.7967\u001b[0m  0.0010  0.1751\n",
      "     12      114.8090      \u001b[32m145.1411\u001b[0m  0.0010  0.1503\n",
      "     12      \u001b[36m109.5509\u001b[0m      \u001b[32m122.2946\u001b[0m  0.0010  0.1546\n",
      "     12      \u001b[36m121.9032\u001b[0m      \u001b[32m123.7446\u001b[0m  0.0010  0.1586\n",
      "     13      \u001b[36m112.8244\u001b[0m      123.4338  0.0010  0.1624\n",
      "     13      \u001b[36m125.9322\u001b[0m      \u001b[32m116.6234\u001b[0m  0.0010  0.1475\n",
      "     13      \u001b[36m111.7674\u001b[0m      \u001b[32m144.0256\u001b[0m  0.0010  0.1395\n",
      "     13      \u001b[36m107.8989\u001b[0m      \u001b[32m122.2203\u001b[0m  0.0010  0.1581\n",
      "     13      \u001b[36m120.0464\u001b[0m      \u001b[32m123.6957\u001b[0m  0.0010  0.1511\n",
      "     14      \u001b[36m123.8418\u001b[0m      \u001b[32m115.8027\u001b[0m  0.0010  0.1476\n",
      "     14      \u001b[36m109.0406\u001b[0m      123.2855  0.0010  0.1608\n",
      "     14      \u001b[36m110.2391\u001b[0m      \u001b[32m140.7589\u001b[0m  0.0010  0.1488\n",
      "     14      \u001b[36m107.2541\u001b[0m      \u001b[32m120.8218\u001b[0m  0.0010  0.1327\n",
      "     14      \u001b[36m119.6159\u001b[0m      \u001b[32m123.2409\u001b[0m  0.0010  0.1283\n",
      "     15      109.9359      \u001b[32m122.2786\u001b[0m  0.0010  0.1137\n",
      "     15      \u001b[36m123.2965\u001b[0m      \u001b[32m115.5780\u001b[0m  0.0010  0.1571\n",
      "     15      \u001b[36m109.5115\u001b[0m      141.3299  0.0010  0.1232\n",
      "     15      \u001b[36m105.5416\u001b[0m      \u001b[32m119.4510\u001b[0m  0.0010  0.1354\n",
      "     15      \u001b[36m118.6285\u001b[0m      \u001b[32m122.4249\u001b[0m  0.0010  0.1321\n",
      "     16      \u001b[36m108.7360\u001b[0m      122.3151  0.0010  0.1347\n",
      "     16      \u001b[36m121.4683\u001b[0m      \u001b[32m114.9324\u001b[0m  0.0010  0.1327\n",
      "     16      \u001b[36m106.4517\u001b[0m      \u001b[32m140.3283\u001b[0m  0.0010  0.1587\n",
      "     16      105.9005      \u001b[32m118.8558\u001b[0m  0.0010  0.1437\n",
      "     16      \u001b[36m116.2658\u001b[0m      122.4507  0.0010  0.1417\n",
      "     17      \u001b[36m107.9564\u001b[0m      \u001b[32m121.5973\u001b[0m  0.0010  0.1315\n",
      "     17      \u001b[36m120.8380\u001b[0m      \u001b[32m114.2707\u001b[0m  0.0010  0.1370\n",
      "     17      \u001b[36m105.5592\u001b[0m      \u001b[32m138.5888\u001b[0m  0.0010  0.1640\n",
      "     17      \u001b[36m114.8737\u001b[0m      \u001b[32m122.1008\u001b[0m  0.0010  0.1483\n",
      "     17      \u001b[36m103.4633\u001b[0m      \u001b[32m118.7171\u001b[0m  0.0010  0.1633\n",
      "     18      \u001b[36m106.4983\u001b[0m      \u001b[32m120.2660\u001b[0m  0.0010  0.1698\n",
      "     18      \u001b[36m120.8377\u001b[0m      \u001b[32m113.4497\u001b[0m  0.0010  0.1598\n",
      "     18      \u001b[36m105.1680\u001b[0m      \u001b[32m138.0766\u001b[0m  0.0010  0.1505\n",
      "     18      \u001b[36m102.5342\u001b[0m      \u001b[32m117.9882\u001b[0m  0.0010  0.1593\n",
      "     18      \u001b[36m113.4994\u001b[0m      \u001b[32m120.7976\u001b[0m  0.0010  0.1635\n",
      "     19      \u001b[36m105.7485\u001b[0m      121.2746  0.0010  0.1804\n",
      "     19      \u001b[36m118.6739\u001b[0m      113.5129  0.0010  0.1887\n",
      "     19      \u001b[36m104.6247\u001b[0m      \u001b[32m137.5782\u001b[0m  0.0010  0.1954\n",
      "     19      \u001b[36m102.2918\u001b[0m      \u001b[32m117.1861\u001b[0m  0.0010  0.2035\n",
      "     19      \u001b[36m113.1905\u001b[0m      \u001b[32m120.5386\u001b[0m  0.0010  0.2041\n",
      "     20      \u001b[36m104.4034\u001b[0m      120.6711  0.0010  0.1906\n",
      "     20      \u001b[36m118.3035\u001b[0m      113.6823  0.0010  0.1795\n",
      "     20      \u001b[36m104.0049\u001b[0m      \u001b[32m135.8221\u001b[0m  0.0010  0.1610\n",
      "     20      \u001b[36m101.0597\u001b[0m      117.8335  0.0010  0.1496\n",
      "     20      \u001b[36m112.4936\u001b[0m      \u001b[32m120.5182\u001b[0m  0.0010  0.1618\n",
      "     21      \u001b[36m103.5797\u001b[0m      121.0890  0.0010  0.1687\n",
      "     21      \u001b[36m117.3014\u001b[0m      \u001b[32m113.2077\u001b[0m  0.0010  0.1722\n",
      "     21      \u001b[36m103.9907\u001b[0m      \u001b[32m134.9587\u001b[0m  0.0010  0.1733\n",
      "     21       \u001b[36m99.5876\u001b[0m      117.5887  0.0010  0.1880\n",
      "     21      \u001b[36m110.6913\u001b[0m      \u001b[32m119.6013\u001b[0m  0.0010  0.1854\n",
      "     22      \u001b[36m103.0282\u001b[0m      120.5175  0.0010  0.1647\n",
      "     22      117.4232      \u001b[32m112.7983\u001b[0m  0.0010  0.1885\n",
      "     22      \u001b[36m102.8358\u001b[0m      \u001b[32m134.2350\u001b[0m  0.0010  0.1661\n",
      "     22      100.4637      117.4103  0.0010  0.1572\n",
      "     22      \u001b[36m109.2123\u001b[0m      120.2844  0.0010  0.1709\n",
      "     23      \u001b[36m100.4943\u001b[0m      \u001b[32m119.2429\u001b[0m  0.0005  0.1700\n",
      "     23      \u001b[36m115.9980\u001b[0m      112.8132  0.0010  0.1686\n",
      "     23      \u001b[36m102.3506\u001b[0m      \u001b[32m133.3927\u001b[0m  0.0010  0.1570\n",
      "     23       \u001b[36m99.5576\u001b[0m      \u001b[32m116.9989\u001b[0m  0.0010  0.1490\n",
      "     23      \u001b[36m108.1272\u001b[0m      \u001b[32m119.3683\u001b[0m  0.0010  0.1686\n",
      "     24      \u001b[36m100.0123\u001b[0m      \u001b[32m118.8172\u001b[0m  0.0005  0.1484\n",
      "     24       \u001b[36m99.4189\u001b[0m      \u001b[32m132.7964\u001b[0m  0.0010  0.1493\n",
      "     24      \u001b[36m115.4012\u001b[0m      \u001b[32m112.1816\u001b[0m  0.0010  0.1596\n",
      "     24      100.2548      \u001b[32m116.4410\u001b[0m  0.0010  0.1592\n",
      "     25       \u001b[36m99.3937\u001b[0m      119.3228  0.0005  0.1527\n",
      "     24      110.4048      119.5198  0.0010  0.1599\n",
      "     25      100.3221      135.6625  0.0010  0.1569\n",
      "     25      \u001b[36m114.2629\u001b[0m      \u001b[32m112.0370\u001b[0m  0.0010  0.1577\n",
      "     25       \u001b[36m98.1244\u001b[0m      \u001b[32m116.3045\u001b[0m  0.0010  0.1601\n",
      "     26       99.9299      119.2912  0.0005  0.1675\n",
      "     25      109.2784      \u001b[32m119.0190\u001b[0m  0.0010  0.1902\n",
      "     26      100.9094      132.8177  0.0010  0.1535\n",
      "     26      114.9825      \u001b[32m111.7995\u001b[0m  0.0010  0.1724\n",
      "     26       \u001b[36m97.4057\u001b[0m      \u001b[32m115.5205\u001b[0m  0.0010  0.1640\n",
      "     27       \u001b[36m98.9365\u001b[0m      \u001b[32m118.4754\u001b[0m  0.0005  0.1435\n",
      "     27      100.0744      \u001b[32m131.4993\u001b[0m  0.0010  0.1265\n",
      "     26      \u001b[36m107.8532\u001b[0m      \u001b[32m118.1494\u001b[0m  0.0010  0.1494\n",
      "     27      \u001b[36m113.6674\u001b[0m      \u001b[32m111.5474\u001b[0m  0.0010  0.1476\n",
      "     28       99.0974      118.9769  0.0005  0.1195\n",
      "     27       \u001b[36m96.9300\u001b[0m      \u001b[32m115.5146\u001b[0m  0.0010  0.1652\n",
      "     28       \u001b[36m98.9619\u001b[0m      \u001b[32m130.5858\u001b[0m  0.0010  0.1555\n",
      "     27      \u001b[36m106.9255\u001b[0m      \u001b[32m118.0025\u001b[0m  0.0010  0.1517\n",
      "     28      \u001b[36m113.5632\u001b[0m      \u001b[32m110.8641\u001b[0m  0.0010  0.1384\n",
      "     29       99.5973      \u001b[32m117.9480\u001b[0m  0.0005  0.1645\n",
      "     29       \u001b[36m98.7692\u001b[0m      132.4619  0.0010  0.1294\n",
      "     28       97.2906      \u001b[32m115.1565\u001b[0m  0.0010  0.1679\n",
      "     28      \u001b[36m106.2097\u001b[0m      \u001b[32m117.5166\u001b[0m  0.0010  0.1582\n",
      "     29      \u001b[36m113.1967\u001b[0m      111.0536  0.0010  0.1682\n",
      "     30       \u001b[36m98.3344\u001b[0m      \u001b[32m117.8968\u001b[0m  0.0005  0.1469\n",
      "     29       \u001b[36m95.6116\u001b[0m      115.4560  0.0010  0.1439\n",
      "     30       \u001b[36m97.9799\u001b[0m      \u001b[32m130.0896\u001b[0m  0.0010  0.1554\n",
      "     29      106.2694      \u001b[32m117.4664\u001b[0m  0.0010  0.1459\n",
      "     30      113.3436      \u001b[32m110.6486\u001b[0m  0.0010  0.1565\n",
      "     31       \u001b[36m97.3130\u001b[0m      \u001b[32m117.6284\u001b[0m  0.0005  0.1486\n",
      "     30       95.9332      \u001b[32m114.7694\u001b[0m  0.0010  0.1505\n",
      "     31       \u001b[36m96.3701\u001b[0m      130.1268  0.0010  0.1495\n",
      "     30      \u001b[36m104.8898\u001b[0m      117.4711  0.0010  0.1665\n",
      "     31      \u001b[36m111.1764\u001b[0m      \u001b[32m110.3026\u001b[0m  0.0010  0.1585\n",
      "     32       97.3448      \u001b[32m117.4950\u001b[0m  0.0005  0.1670\n",
      "     31       96.0190      115.0073  0.0010  0.1915\n",
      "     32       97.7082      \u001b[32m129.2447\u001b[0m  0.0010  0.1852\n",
      "     31      105.7664      \u001b[32m116.4001\u001b[0m  0.0010  0.1808\n",
      "     32      \u001b[36m111.1297\u001b[0m      110.8087  0.0010  0.1820\n",
      "     33       \u001b[36m96.7160\u001b[0m      118.0943  0.0005  0.1779\n",
      "     33       97.2207      130.4138  0.0010  0.1578\n",
      "     32       \u001b[36m95.0301\u001b[0m      115.0202  0.0010  0.1770\n",
      "     32      105.2648      \u001b[32m116.1013\u001b[0m  0.0010  0.1467\n",
      "     33      \u001b[36m110.2718\u001b[0m      \u001b[32m110.2727\u001b[0m  0.0010  0.1475\n",
      "     34       \u001b[36m96.2861\u001b[0m      117.9899  0.0005  0.1551\n",
      "     34       \u001b[36m96.3284\u001b[0m      130.2464  0.0010  0.1586\n",
      "     33       \u001b[36m94.7716\u001b[0m      114.9523  0.0010  0.1796\n",
      "     33      \u001b[36m104.5049\u001b[0m      116.3735  0.0010  0.1888\n",
      "     34      112.0560      111.0237  0.0010  0.1778\n",
      "     35       97.1724      \u001b[32m117.3882\u001b[0m  0.0005  0.1863\n",
      "     35       \u001b[36m95.5352\u001b[0m      129.6960  0.0010  0.2057\n",
      "     34       95.8174      115.0972  0.0010  0.1971\n",
      "     34      \u001b[36m103.6973\u001b[0m      116.1715  0.0010  0.1896\n",
      "     35      \u001b[36m109.9145\u001b[0m      110.4045  0.0010  0.1943\n",
      "     36       96.6145      \u001b[32m117.0315\u001b[0m  0.0005  0.1963\n",
      "     36       96.4557      \u001b[32m129.2278\u001b[0m  0.0010  0.1943\n",
      "     35      \u001b[36m103.5357\u001b[0m      116.3493  0.0010  0.2028\n",
      "     36      111.1169      \u001b[32m110.0491\u001b[0m  0.0010  0.2009\n",
      "     37       96.7603      117.3160  0.0005  0.1864\n",
      "     37       96.0186      129.2673  0.0010  0.1925\n",
      "     36      \u001b[36m103.3021\u001b[0m      117.3028  0.0010  0.1640\n",
      "     37      \u001b[36m109.8518\u001b[0m      110.1919  0.0010  0.1623\n",
      "     38       \u001b[36m95.0038\u001b[0m      117.3534  0.0005  0.1497\n",
      "     38       \u001b[36m93.7959\u001b[0m      \u001b[32m128.9656\u001b[0m  0.0010  0.1664\n",
      "     38      109.9130      110.1653  0.0010  0.1336\n",
      "     39       95.2734      117.7476  0.0005  0.1557\n",
      "     39       94.9021      129.1319  0.0010  0.1445\n",
      "     39      \u001b[36m108.8617\u001b[0m      110.4813  0.0010  0.1521\n",
      "     40       95.4772      117.1118  0.0005  0.1383\n",
      "     40       94.6685      \u001b[32m128.9092\u001b[0m  0.0010  0.1317\n",
      "     40      \u001b[36m108.4014\u001b[0m      110.1898  0.0010  0.1567\n",
      "     41       94.0078      128.9632  0.0010  0.1494\n",
      "     41      108.5363      \u001b[32m109.9594\u001b[0m  0.0005  0.1740\n",
      "     42       94.6679      \u001b[32m127.9276\u001b[0m  0.0010  0.1466\n",
      "     42      \u001b[36m106.1347\u001b[0m      \u001b[32m109.7052\u001b[0m  0.0005  0.1222\n",
      "     43       94.4371      128.1047  0.0010  0.1090\n",
      "     43      107.1750      \u001b[32m109.0566\u001b[0m  0.0005  0.0985\n",
      "     44       94.5609      128.2347  0.0010  0.1077\n",
      "     44      \u001b[36m105.4665\u001b[0m      109.1583  0.0005  0.1034\n",
      "     45       \u001b[36m92.4551\u001b[0m      \u001b[32m124.8897\u001b[0m  0.0010  0.0982\n",
      "     45      107.2173      109.3050  0.0005  0.0895\n",
      "     46       93.7197      128.8368  0.0010  0.0954\n",
      "     46      \u001b[36m104.0724\u001b[0m      109.5064  0.0005  0.1149\n",
      "     47       94.0687      127.6705  0.0010  0.1089\n",
      "     47      105.6051      109.4526  0.0005  0.1209\n",
      "     48       92.4776      126.3454  0.0010  0.1145\n",
      "     48      105.5862      \u001b[32m108.8734\u001b[0m  0.0003  0.1173\n",
      "     49       \u001b[36m92.3049\u001b[0m      127.6709  0.0010  0.1117\n",
      "     49      105.1739      109.0789  0.0003  0.1234\n",
      "     50       \u001b[36m92.0749\u001b[0m      \u001b[32m123.1483\u001b[0m  0.0005  0.1453\n",
      "     50      105.3940      109.0740  0.0003  0.1780\n",
      "     51       \u001b[36m91.1564\u001b[0m      \u001b[32m122.9446\u001b[0m  0.0005  0.1697\n",
      "     51      104.4938      109.0243  0.0003  0.1227\n",
      "     52       \u001b[36m89.9911\u001b[0m      \u001b[32m122.5025\u001b[0m  0.0005  0.1277\n",
      "     52      \u001b[36m103.5303\u001b[0m      \u001b[32m108.7635\u001b[0m  0.0003  0.1326\n",
      "     53       90.6121      122.8767  0.0005  0.1258\n",
      "     53      103.8868      108.9247  0.0003  0.1260\n",
      "     54       91.1475      \u001b[32m122.2227\u001b[0m  0.0005  0.1099\n",
      "     54      \u001b[36m103.3686\u001b[0m      \u001b[32m108.6998\u001b[0m  0.0003  0.1085\n",
      "     55       \u001b[36m89.8651\u001b[0m      \u001b[32m122.0182\u001b[0m  0.0005  0.1121\n",
      "     55      106.0408      \u001b[32m108.6027\u001b[0m  0.0003  0.0993\n",
      "     56       90.4864      122.8523  0.0005  0.0889\n",
      "     56      107.0217      108.8885  0.0003  0.1044\n",
      "     57       91.9799      123.5786  0.0005  0.1100\n",
      "     57      103.9524      108.6420  0.0003  0.0974\n",
      "     58       90.3520      122.7576  0.0005  0.1075\n",
      "     58      103.9731      108.8272  0.0003  0.1210\n",
      "     59       90.4861      122.4059  0.0005  0.1248\n",
      "     59      104.9582      108.6527  0.0003  0.1243\n",
      "     60       \u001b[36m88.6576\u001b[0m      \u001b[32m121.3365\u001b[0m  0.0003  0.1225\n",
      "     61       90.4205      \u001b[32m120.9574\u001b[0m  0.0003  0.1314\n",
      "     62       \u001b[36m88.4988\u001b[0m      \u001b[32m120.5775\u001b[0m  0.0003  0.0870\n",
      "     63       89.3331      \u001b[32m120.5254\u001b[0m  0.0003  0.0803\n",
      "     64       89.6188      120.6559  0.0003  0.0918\n",
      "     65       89.6786      121.2949  0.0003  0.0859\n",
      "     66       \u001b[36m87.5836\u001b[0m      120.7725  0.0003  0.0776\n",
      "     67       89.7537      120.6754  0.0003  0.0875\n",
      "     68       89.9017      \u001b[32m120.4257\u001b[0m  0.0001  0.0881\n",
      "     69       88.9456      \u001b[32m120.4064\u001b[0m  0.0001  0.0814\n",
      "     70       89.2279      120.7993  0.0001  0.0765\n",
      "     71       89.3768      \u001b[32m120.3605\u001b[0m  0.0001  0.0832\n",
      "     72       88.4592      \u001b[32m120.0228\u001b[0m  0.0001  0.0820\n",
      "     73       87.9591      120.1077  0.0001  0.0824\n",
      "     74       88.8725      120.2616  0.0001  0.0768\n",
      "     75       88.5560      120.7076  0.0001  0.0805\n",
      "     76       88.8468      120.5913  0.0001  0.0846\n",
      "RMSE for each folder: [10.64508958 11.8683588  11.37481287 10.08922974  8.93139226]\n",
      "RMSE mean: 10.58177665078785\n"
     ]
    }
   ],
   "source": [
    "#Guided\n",
    "cross_validation_with_scores(x_cov_nn,y,groups,pipe_fusion,logo,rmse_scorer)\n",
    "\n",
    "#Free \n",
    "cross_validation_with_scores(x_cov_nn_f,y_f,groups_f,pipe_fusion,logo,rmse_scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5127f6c1-c9b2-47df-878b-aa7e2cfa63d6",
   "metadata": {},
   "source": [
    "![](./images/5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a2f89932-867f-4c54-9a38-d64b949fe003",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m440.9353\u001b[0m      \u001b[32m321.2495\u001b[0m  0.0010  0.0966\n",
      "      2      \u001b[36m217.0207\u001b[0m      \u001b[32m147.0345\u001b[0m  0.0010  0.0863\n",
      "      3      \u001b[36m150.9787\u001b[0m      \u001b[32m126.6180\u001b[0m  0.0010  0.0839\n",
      "      4      \u001b[36m128.9234\u001b[0m      \u001b[32m113.5365\u001b[0m  0.0010  0.0789\n",
      "      5      \u001b[36m112.8172\u001b[0m      \u001b[32m102.9149\u001b[0m  0.0010  0.0844\n",
      "      6      \u001b[36m101.2874\u001b[0m       \u001b[32m94.7909\u001b[0m  0.0010  0.0836\n",
      "      7       \u001b[36m93.6632\u001b[0m       \u001b[32m87.5280\u001b[0m  0.0010  0.0794\n",
      "      8       \u001b[36m87.4154\u001b[0m       \u001b[32m81.7757\u001b[0m  0.0010  0.0878\n",
      "      9       \u001b[36m83.5102\u001b[0m       \u001b[32m77.0341\u001b[0m  0.0010  0.0810\n",
      "     10       \u001b[36m79.0211\u001b[0m       \u001b[32m72.6960\u001b[0m  0.0010  0.0789\n",
      "     11       \u001b[36m75.2462\u001b[0m       \u001b[32m68.3102\u001b[0m  0.0010  0.0781\n",
      "     12       \u001b[36m70.8998\u001b[0m       \u001b[32m65.9711\u001b[0m  0.0010  0.0883\n",
      "     13       \u001b[36m67.6282\u001b[0m       \u001b[32m61.6043\u001b[0m  0.0010  0.0868\n",
      "     14       \u001b[36m65.3987\u001b[0m       \u001b[32m59.3400\u001b[0m  0.0010  0.0918\n",
      "     15       \u001b[36m63.0777\u001b[0m       \u001b[32m58.0101\u001b[0m  0.0010  0.0833\n",
      "     16       \u001b[36m60.6039\u001b[0m       \u001b[32m54.9997\u001b[0m  0.0010  0.0928\n",
      "     17       \u001b[36m57.8418\u001b[0m       \u001b[32m53.6849\u001b[0m  0.0010  0.0797\n",
      "     18       \u001b[36m56.5628\u001b[0m       \u001b[32m52.0810\u001b[0m  0.0010  0.0743\n",
      "     19       \u001b[36m55.6242\u001b[0m       \u001b[32m49.8632\u001b[0m  0.0010  0.0819\n",
      "     20       \u001b[36m53.7839\u001b[0m       \u001b[32m48.7777\u001b[0m  0.0010  0.0878\n",
      "     21       \u001b[36m53.2406\u001b[0m       \u001b[32m46.0751\u001b[0m  0.0010  0.0787\n",
      "     22       \u001b[36m49.5066\u001b[0m       46.0936  0.0010  0.0920\n",
      "     23       49.9333       \u001b[32m42.8342\u001b[0m  0.0010  0.0771\n",
      "     24       \u001b[36m48.1561\u001b[0m       43.6134  0.0010  0.0949\n",
      "     25       \u001b[36m46.7011\u001b[0m       \u001b[32m42.0426\u001b[0m  0.0010  0.0907\n",
      "     26       47.0397       \u001b[32m40.2726\u001b[0m  0.0010  0.0870\n",
      "     27       \u001b[36m45.9716\u001b[0m       \u001b[32m38.7841\u001b[0m  0.0010  0.0901\n",
      "     28       \u001b[36m43.8132\u001b[0m       \u001b[32m38.5716\u001b[0m  0.0010  0.0750\n",
      "     29       \u001b[36m43.7827\u001b[0m       \u001b[32m36.9755\u001b[0m  0.0010  0.0870\n",
      "     30       \u001b[36m43.2125\u001b[0m       37.5883  0.0010  0.0843\n",
      "     31       \u001b[36m41.3787\u001b[0m       \u001b[32m36.5505\u001b[0m  0.0010  0.0794\n",
      "     32       41.7147       \u001b[32m35.6906\u001b[0m  0.0010  0.0775\n",
      "     33       42.0776       \u001b[32m35.2654\u001b[0m  0.0010  0.0938\n",
      "     34       \u001b[36m40.6362\u001b[0m       \u001b[32m34.1729\u001b[0m  0.0010  0.0987\n",
      "     35       \u001b[36m39.0906\u001b[0m       \u001b[32m33.2292\u001b[0m  0.0010  0.0919\n",
      "     36       \u001b[36m39.0273\u001b[0m       \u001b[32m32.9284\u001b[0m  0.0010  0.0861\n",
      "     37       \u001b[36m38.5559\u001b[0m       \u001b[32m32.3237\u001b[0m  0.0010  0.0823\n",
      "     38       39.2386       32.5695  0.0010  0.0882\n",
      "     39       \u001b[36m38.3109\u001b[0m       \u001b[32m31.6148\u001b[0m  0.0010  0.0790\n",
      "     40       \u001b[36m37.7407\u001b[0m       31.9860  0.0010  0.0822\n",
      "     41       \u001b[36m36.9149\u001b[0m       \u001b[32m30.0468\u001b[0m  0.0010  0.0819\n",
      "     42       \u001b[36m36.6151\u001b[0m       30.0563  0.0010  0.0842\n",
      "     43       37.3279       \u001b[32m29.9757\u001b[0m  0.0010  0.0785\n",
      "     44       \u001b[36m36.4630\u001b[0m       \u001b[32m29.9541\u001b[0m  0.0010  0.0877\n",
      "     45       \u001b[36m35.4014\u001b[0m       \u001b[32m29.2054\u001b[0m  0.0010  0.1306\n",
      "     46       \u001b[36m35.0509\u001b[0m       \u001b[32m28.4452\u001b[0m  0.0010  0.0822\n",
      "     47       35.5461       \u001b[32m27.4027\u001b[0m  0.0010  0.0818\n",
      "     48       \u001b[36m35.0116\u001b[0m       27.5753  0.0010  0.0796\n",
      "     49       \u001b[36m34.4466\u001b[0m       \u001b[32m26.9079\u001b[0m  0.0010  0.0849\n",
      "     50       34.6369       \u001b[32m26.5661\u001b[0m  0.0010  0.0774\n",
      "     51       \u001b[36m33.8350\u001b[0m       27.0967  0.0010  0.0842\n",
      "     52       \u001b[36m33.1480\u001b[0m       \u001b[32m26.4088\u001b[0m  0.0010  0.0768\n",
      "     53       \u001b[36m32.9024\u001b[0m       \u001b[32m25.3684\u001b[0m  0.0010  0.0834\n",
      "     54       33.5331       26.4776  0.0010  0.0826\n",
      "     55       33.2957       25.4381  0.0010  0.0785\n",
      "     56       33.4557       25.6028  0.0010  0.0885\n",
      "     57       33.2455       \u001b[32m24.4325\u001b[0m  0.0010  0.0854\n",
      "     58       \u001b[36m32.5942\u001b[0m       25.3279  0.0010  0.0770\n",
      "     59       33.2536       25.2204  0.0010  0.0821\n",
      "     60       \u001b[36m31.5246\u001b[0m       \u001b[32m24.3119\u001b[0m  0.0010  0.0910\n",
      "     61       31.6059       24.5982  0.0010  0.0748\n",
      "     62       32.5043       24.8134  0.0010  0.0756\n",
      "     63       \u001b[36m31.2484\u001b[0m       \u001b[32m22.7708\u001b[0m  0.0010  0.0901\n",
      "     64       \u001b[36m29.9109\u001b[0m       23.4672  0.0010  0.0751\n",
      "     65       31.0488       23.6455  0.0010  0.0749\n",
      "     66       32.2985       23.5670  0.0010  0.0822\n",
      "     67       31.3520       22.9561  0.0010  0.0766\n",
      "     68       30.1231       \u001b[32m22.0886\u001b[0m  0.0005  0.0839\n",
      "     69       \u001b[36m29.6570\u001b[0m       22.8461  0.0005  0.0776\n",
      "     70       30.4230       22.7972  0.0005  0.0815\n",
      "     71       30.3282       22.7098  0.0005  0.0846\n",
      "     72       30.1843       22.5951  0.0005  0.0815\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m620.4384\u001b[0m      \u001b[32m382.0728\u001b[0m  0.0010  0.0926\n",
      "      2      \u001b[36m270.6949\u001b[0m      \u001b[32m160.4806\u001b[0m  0.0010  0.0883\n",
      "      3      \u001b[36m159.3039\u001b[0m      \u001b[32m141.4028\u001b[0m  0.0010  0.1016\n",
      "      4      \u001b[36m147.8866\u001b[0m      \u001b[32m135.0690\u001b[0m  0.0010  0.0867\n",
      "      5      \u001b[36m141.7053\u001b[0m      \u001b[32m130.9750\u001b[0m  0.0010  0.0879\n",
      "      6      \u001b[36m137.1370\u001b[0m      \u001b[32m128.6216\u001b[0m  0.0010  0.0977\n",
      "      7      \u001b[36m132.9875\u001b[0m      \u001b[32m127.1554\u001b[0m  0.0010  0.0947\n",
      "      8      \u001b[36m129.0842\u001b[0m      \u001b[32m126.0535\u001b[0m  0.0010  0.0989\n",
      "      9      \u001b[36m125.7572\u001b[0m      \u001b[32m125.0518\u001b[0m  0.0010  0.1004\n",
      "     10      \u001b[36m122.5381\u001b[0m      \u001b[32m122.8381\u001b[0m  0.0010  0.1030\n",
      "     11      \u001b[36m121.2748\u001b[0m      \u001b[32m122.6606\u001b[0m  0.0010  0.1070\n",
      "     12      \u001b[36m118.2585\u001b[0m      \u001b[32m122.0661\u001b[0m  0.0010  0.1103\n",
      "     13      \u001b[36m116.8067\u001b[0m      \u001b[32m121.1448\u001b[0m  0.0010  0.1003\n",
      "     14      \u001b[36m114.8546\u001b[0m      \u001b[32m120.8670\u001b[0m  0.0010  0.1014\n",
      "     15      \u001b[36m113.6295\u001b[0m      \u001b[32m119.8187\u001b[0m  0.0010  0.0901\n",
      "     16      \u001b[36m111.9878\u001b[0m      \u001b[32m119.1762\u001b[0m  0.0010  0.0873\n",
      "     17      \u001b[36m109.7922\u001b[0m      \u001b[32m118.9213\u001b[0m  0.0010  0.0981\n",
      "     18      109.9275      \u001b[32m118.4307\u001b[0m  0.0010  0.0854\n",
      "     19      \u001b[36m108.4722\u001b[0m      \u001b[32m117.5087\u001b[0m  0.0010  0.0900\n",
      "     20      \u001b[36m107.7092\u001b[0m      \u001b[32m117.4994\u001b[0m  0.0010  0.1031\n",
      "     21      \u001b[36m106.3049\u001b[0m      \u001b[32m117.0531\u001b[0m  0.0010  0.0932\n",
      "     22      108.6672      117.7611  0.0010  0.1018\n",
      "     23      \u001b[36m105.2838\u001b[0m      \u001b[32m116.6600\u001b[0m  0.0010  0.1009\n",
      "     24      \u001b[36m104.5858\u001b[0m      117.1554  0.0010  0.1027\n",
      "     25      106.6438      116.7557  0.0010  0.1053\n",
      "     26      \u001b[36m104.0233\u001b[0m      \u001b[32m115.8803\u001b[0m  0.0010  0.1014\n",
      "     27      \u001b[36m103.1336\u001b[0m      116.0546  0.0010  0.0949\n",
      "     28      \u001b[36m102.3531\u001b[0m      116.4495  0.0010  0.0860\n",
      "     29      102.4619      \u001b[32m115.8286\u001b[0m  0.0010  0.1118\n",
      "     30      \u001b[36m102.2263\u001b[0m      \u001b[32m115.0719\u001b[0m  0.0010  0.1142\n",
      "     31      \u001b[36m102.1617\u001b[0m      115.3190  0.0010  0.0891\n",
      "     32      102.6799      116.2675  0.0010  0.0936\n",
      "     33       \u001b[36m99.8395\u001b[0m      115.2359  0.0010  0.0920\n",
      "     34      101.9521      \u001b[32m114.5382\u001b[0m  0.0010  0.0949\n",
      "     35       99.9146      115.4401  0.0010  0.1567\n",
      "     36       \u001b[36m98.8096\u001b[0m      \u001b[32m114.1255\u001b[0m  0.0010  0.0973\n",
      "     37       99.3415      114.9211  0.0010  0.0935\n",
      "     38      100.5616      114.7043  0.0010  0.1000\n",
      "     39       99.7880      114.8439  0.0010  0.0898\n",
      "     40       99.1731      114.7869  0.0010  0.0985\n",
      "     41       \u001b[36m98.0872\u001b[0m      \u001b[32m113.3648\u001b[0m  0.0005  0.0941\n",
      "     42       98.1712      \u001b[32m113.0123\u001b[0m  0.0005  0.0915\n",
      "     43       \u001b[36m96.8556\u001b[0m      113.2048  0.0005  0.0943\n",
      "     44       \u001b[36m96.6298\u001b[0m      113.2834  0.0005  0.1115\n",
      "     45       \u001b[36m96.3159\u001b[0m      \u001b[32m112.9071\u001b[0m  0.0005  0.1091\n",
      "     46       \u001b[36m95.8443\u001b[0m      \u001b[32m112.5336\u001b[0m  0.0005  0.0942\n",
      "     47       \u001b[36m95.3122\u001b[0m      112.6587  0.0005  0.0954\n",
      "     48       96.1766      \u001b[32m112.4534\u001b[0m  0.0005  0.0971\n",
      "     49       96.3519      112.4995  0.0005  0.0987\n",
      "     50       95.9150      \u001b[32m112.4493\u001b[0m  0.0005  0.0956\n",
      "     51       96.2884      112.6814  0.0005  0.0949\n",
      "     52       96.4631      \u001b[32m112.4267\u001b[0m  0.0005  0.0970\n",
      "     53       96.3025      \u001b[32m112.3215\u001b[0m  0.0005  0.0963\n",
      "     54       95.9275      \u001b[32m111.9275\u001b[0m  0.0005  0.0964\n",
      "     55       \u001b[36m95.2876\u001b[0m      112.1129  0.0005  0.0959\n",
      "     56       95.6318      111.9564  0.0005  0.0918\n",
      "     57       95.8401      112.2952  0.0005  0.0988\n",
      "     58       96.6051      112.0851  0.0005  0.0981\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def plot(pipeline):\n",
    "    history = pipeline.named_steps['nn'].history\n",
    "    \n",
    "    epochs = history[:, 'epoch']\n",
    "    train_loss = history[:, 'train_loss']\n",
    "    valid_loss = history[:, 'valid_loss']\n",
    "    \n",
    "    plt.plot(epochs, train_loss, label='Train loss')\n",
    "    plt.plot(epochs, valid_loss, label='Valid loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.title('Train/Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "pipe_fusion.fit(x_cov_nn, y)\n",
    "plot(pipe_fusion)\n",
    "pipe_fusion.fit(x_cov_nn_f, y_f)\n",
    "plot(pipe_fusion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ad26a5e-f3e6-4a07-aae2-23535b5a84a0",
   "metadata": {},
   "source": [
    "| Method                                   | RMSE Guided | RMSE  Free |\n",
    "|------------------------------------------|-------------------|------------------:|\n",
    "| Basic Lasso                              |      18.73 | 15.78 |\n",
    "| Baseline approach                        |        –   | |\n",
    "| Covariance matrices + Lasso              |       7.36 | 10.45 |\n",
    "| Covariance matrices + Neural Network     |       5.02 | 10.61|\n",
    "| Convolutional Neural Network             |       4.54 | 11.51|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "When we compare the progression of our models, we see that incorporating covariance matrices into a simple Lasso regressor brings the rmse down about 62 % over the basic Lasso. \n",
    "\n",
    "...\n",
    "\n",
    "Moving on to a small neural network trained on those 36 tangent‐space features further reduces the error to 5.02, which represents a 39 % drop. Finally, the cnn achieves the lowest rmse of 4.87, a modest 3 % gain over the covariance nn hybrid.\n",
    "\n",
    "That last 3 % improvement, however, comes at a cost, the cnn is significantly more complex to train and deploy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416e1878-049b-4342-ad98-3c20afa1110e",
   "metadata": {},
   "source": [
    "#### Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ca1952-a0f3-45b6-8c78-0fae7b055672",
   "metadata": {},
   "source": [
    "For this question, we have ... regression models trained on different feature representations:\n",
    "\n",
    "- ...\n",
    "- Tangentes spaces of the signal using the NN + Covariance Matrice\n",
    "- Filtered raw signal data using a CNN\n",
    "\n",
    "\n",
    "To obtain each models predictions, we use cross_val_predict, which is essentially the same as cross_val_score but returns the predictions of each fold instead of the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b4538322-bae9-4ba8-9e60-3a0001b3f189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m191.3013\u001b[0m      \u001b[32m143.4341\u001b[0m  0.0010  12.4011\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m187.2134\u001b[0m      \u001b[32m107.6620\u001b[0m  0.0010  12.4745\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m183.5272\u001b[0m      \u001b[32m110.6196\u001b[0m  0.0010  12.3992\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m193.4070\u001b[0m      \u001b[32m122.9238\u001b[0m  0.0010  12.5195\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m193.1418\u001b[0m      \u001b[32m112.7577\u001b[0m  0.0010  12.6176\n",
      "      2       \u001b[36m85.6173\u001b[0m      123.5681  0.0010  12.8952\n",
      "      2       \u001b[36m83.1987\u001b[0m       \u001b[32m73.6641\u001b[0m  0.0010  12.9517\n",
      "      2       \u001b[36m90.6465\u001b[0m       \u001b[32m75.2312\u001b[0m  0.0010  13.1417\n",
      "      2       \u001b[36m90.5304\u001b[0m      204.3325  0.0010  13.1501\n",
      "      2       \u001b[36m87.8346\u001b[0m       \u001b[32m73.3328\u001b[0m  0.0010  13.4928\n",
      "      3       \u001b[36m73.2689\u001b[0m       \u001b[32m95.3369\u001b[0m  0.0010  12.5831\n",
      "      3       \u001b[36m72.4390\u001b[0m       \u001b[32m69.3808\u001b[0m  0.0010  12.5074\n",
      "      3       \u001b[36m72.2946\u001b[0m       \u001b[32m63.7577\u001b[0m  0.0010  12.4584\n",
      "      3       \u001b[36m72.9975\u001b[0m      \u001b[32m107.4342\u001b[0m  0.0010  12.3467\n",
      "      3       \u001b[36m71.5393\u001b[0m       \u001b[32m63.5965\u001b[0m  0.0010  12.6388\n",
      "      4       \u001b[36m66.5957\u001b[0m       \u001b[32m66.2993\u001b[0m  0.0010  12.4925\n",
      "      4       \u001b[36m66.3704\u001b[0m      108.6267  0.0010  12.5013\n",
      "      4       \u001b[36m68.3165\u001b[0m       \u001b[32m89.4966\u001b[0m  0.0010  12.7005\n",
      "      4       \u001b[36m64.6507\u001b[0m       65.7230  0.0010  12.6486\n",
      "      4       \u001b[36m63.2512\u001b[0m       63.9796  0.0010  12.7350\n",
      "      5       \u001b[36m60.9326\u001b[0m       76.3458  0.0010  12.4910\n",
      "      5       \u001b[36m63.1971\u001b[0m       96.2558  0.0010  12.4088\n",
      "      5       \u001b[36m60.4554\u001b[0m      \u001b[32m100.1040\u001b[0m  0.0010  12.4868\n",
      "      5       \u001b[36m58.3489\u001b[0m       65.0110  0.0010  12.6107\n",
      "      5       \u001b[36m55.9771\u001b[0m       \u001b[32m62.1559\u001b[0m  0.0010  12.7799\n",
      "      6       \u001b[36m54.7654\u001b[0m       69.3625  0.0010  12.4321\n",
      "      6       \u001b[36m53.2610\u001b[0m       \u001b[32m87.8735\u001b[0m  0.0010  12.4829\n",
      "      6       \u001b[36m57.2427\u001b[0m      108.3336  0.0010  12.5942\n",
      "      6       \u001b[36m51.7307\u001b[0m       74.4142  0.0010  12.3722\n",
      "      6       \u001b[36m47.6307\u001b[0m       \u001b[32m58.1137\u001b[0m  0.0010  12.6688\n",
      "      7       \u001b[36m48.2967\u001b[0m       \u001b[32m64.0714\u001b[0m  0.0010  12.6275\n",
      "      7       \u001b[36m47.8729\u001b[0m      102.8958  0.0010  12.3841\n",
      "      7       \u001b[36m42.8293\u001b[0m       \u001b[32m52.7586\u001b[0m  0.0010  12.5826\n",
      "      7       \u001b[36m46.4288\u001b[0m       74.5220  0.0010  12.6062\n",
      "      7       \u001b[36m41.9659\u001b[0m       84.3456  0.0010  13.0382\n",
      "      8       \u001b[36m42.6600\u001b[0m       \u001b[32m49.3985\u001b[0m  0.0010  12.5495\n",
      "      8       \u001b[36m40.6663\u001b[0m       \u001b[32m69.3896\u001b[0m  0.0010  12.6981\n",
      "      8       \u001b[36m34.4986\u001b[0m       \u001b[32m41.4997\u001b[0m  0.0010  12.5986\n",
      "      8       \u001b[36m44.3551\u001b[0m       \u001b[32m46.3683\u001b[0m  0.0005  12.5419\n",
      "      8       \u001b[36m37.4858\u001b[0m      109.6794  0.0010  12.7618\n",
      "      9       \u001b[36m36.9543\u001b[0m       \u001b[32m45.9785\u001b[0m  0.0010  12.3657\n",
      "      9       \u001b[36m36.3179\u001b[0m       \u001b[32m58.8755\u001b[0m  0.0010  12.3988\n",
      "      9       \u001b[36m40.8955\u001b[0m       \u001b[32m46.1046\u001b[0m  0.0005  12.4535\n",
      "      9       \u001b[36m28.3423\u001b[0m       68.8998  0.0010  12.6902\n",
      "      9       \u001b[36m32.6260\u001b[0m       61.6666  0.0010  12.7909\n",
      "     10       \u001b[36m32.8158\u001b[0m       54.0856  0.0010  12.4967\n",
      "     10       \u001b[36m34.1278\u001b[0m       \u001b[32m40.7055\u001b[0m  0.0010  12.3462\n",
      "     10       \u001b[36m38.0737\u001b[0m       \u001b[32m43.4007\u001b[0m  0.0005  12.3983\n",
      "     10       \u001b[36m21.5700\u001b[0m       \u001b[32m39.1737\u001b[0m  0.0010  12.5511\n",
      "     10       \u001b[36m28.7080\u001b[0m       \u001b[32m52.7360\u001b[0m  0.0010  12.9202\n",
      "     11       \u001b[36m26.7099\u001b[0m       58.8896  0.0010  12.5629\n",
      "     11       \u001b[36m30.2595\u001b[0m       \u001b[32m36.7159\u001b[0m  0.0010  12.6987\n",
      "     11       \u001b[36m34.8170\u001b[0m       \u001b[32m39.8671\u001b[0m  0.0005  12.5320\n",
      "     11       \u001b[36m18.9986\u001b[0m       55.4831  0.0010  12.4576\n",
      "     11       \u001b[36m21.7179\u001b[0m       68.1997  0.0010  13.1471\n",
      "     12       \u001b[36m20.4311\u001b[0m       \u001b[32m38.7932\u001b[0m  0.0010  12.6857\n",
      "     12       \u001b[36m26.0203\u001b[0m       \u001b[32m33.9658\u001b[0m  0.0010  12.6084\n",
      "     12       \u001b[36m30.0642\u001b[0m       \u001b[32m35.8177\u001b[0m  0.0005  12.7654\n",
      "     12       \u001b[36m15.5345\u001b[0m       \u001b[32m34.8774\u001b[0m  0.0010  12.6019\n",
      "     12       \u001b[36m18.3917\u001b[0m       \u001b[32m50.5062\u001b[0m  0.0010  12.9326\n",
      "     13       \u001b[36m16.1546\u001b[0m       \u001b[32m33.5515\u001b[0m  0.0010  12.8152\n",
      "     13       \u001b[36m18.6623\u001b[0m       34.2610  0.0010  12.8505\n",
      "     13       \u001b[36m13.1996\u001b[0m       \u001b[32m32.9793\u001b[0m  0.0010  12.7541\n",
      "     13       \u001b[36m25.2701\u001b[0m       \u001b[32m32.7426\u001b[0m  0.0005  12.9807\n",
      "     13       \u001b[36m16.3951\u001b[0m       78.6749  0.0010  12.9893\n",
      "     14       \u001b[36m14.2368\u001b[0m       35.1090  0.0010  12.5919\n",
      "     14       \u001b[36m17.1326\u001b[0m       47.7702  0.0010  12.5587\n",
      "     14       \u001b[36m11.6561\u001b[0m       39.3128  0.0010  12.5699\n",
      "     14       \u001b[36m20.2590\u001b[0m       \u001b[32m30.9174\u001b[0m  0.0005  12.8388\n",
      "     14       \u001b[36m13.5396\u001b[0m       \u001b[32m37.2039\u001b[0m  0.0010  13.2329\n",
      "     15       \u001b[36m12.3217\u001b[0m       35.3682  0.0010  13.5431\n",
      "     15       17.5658       35.8406  0.0010  13.5988\n",
      "     15       \u001b[36m11.4681\u001b[0m       45.0156  0.0010  13.5669\n",
      "     15       \u001b[36m15.5965\u001b[0m       33.5079  0.0005  13.8375\n",
      "     15       \u001b[36m12.9792\u001b[0m       41.6531  0.0010  13.5010\n",
      "     16       \u001b[36m11.9060\u001b[0m       35.8095  0.0010  13.5155\n",
      "     16       17.4472       58.4776  0.0010  13.4969\n",
      "     16       12.1437       44.4723  0.0010  13.5045\n",
      "     16       \u001b[36m13.0807\u001b[0m       35.2671  0.0005  13.9043\n",
      "     16       \u001b[36m11.5176\u001b[0m       41.6776  0.0010  14.0883\n",
      "     17       13.0469       37.7666  0.0010  14.4169\n",
      "     17       19.5129       47.1801  0.0005  14.5083\n",
      "     17       13.9264       47.2597  0.0010  14.5040\n",
      "     17       \u001b[36m11.8712\u001b[0m       35.3136  0.0005  14.5766\n",
      "     17        \u001b[36m9.9765\u001b[0m       \u001b[32m35.9728\u001b[0m  0.0010  14.5806\n",
      "     18       14.5015       58.3813  0.0005  14.1000\n",
      "     18       \u001b[36m13.4146\u001b[0m       56.7674  0.0005  14.0594\n",
      "     18       14.3646       40.8977  0.0005  14.0183\n",
      "     18       \u001b[36m11.1747\u001b[0m       38.8779  0.0005  14.3624\n",
      "     18        \u001b[36m8.8920\u001b[0m       \u001b[32m28.8290\u001b[0m  0.0010  14.8685\n",
      "     19       14.6908       40.0944  0.0005  15.2098\n",
      "     19       \u001b[36m11.2065\u001b[0m       52.4707  0.0005  15.0629\n",
      "     19       14.6819       \u001b[32m30.8881\u001b[0m  0.0005  15.4894\n",
      "     19       11.3644       35.6383  0.0003  15.3425\n",
      "     19        \u001b[36m8.4791\u001b[0m       29.4661  0.0010  15.2601\n",
      "     20       12.6330       \u001b[32m31.0221\u001b[0m  0.0005  14.0365\n",
      "     20       \u001b[36m10.3248\u001b[0m       48.7610  0.0005  14.0295\n",
      "     20       11.4723       \u001b[32m28.6897\u001b[0m  0.0005  13.9513\n",
      "     20       11.2666       34.7707  0.0003  14.2695\n",
      "     20        8.6827       29.6378  0.0010  14.3392\n",
      "     21       11.0110       39.5447  0.0003  13.8599\n",
      "     21       \u001b[36m10.3737\u001b[0m       \u001b[32m24.8800\u001b[0m  0.0005  14.0102\n",
      "     21        \u001b[36m9.3396\u001b[0m       \u001b[32m28.3130\u001b[0m  0.0005  13.8436\n",
      "     21       \u001b[36m10.0869\u001b[0m       32.4201  0.0003  14.0295\n",
      "     21        9.1624       35.0527  0.0010  14.2232\n",
      "     22        \u001b[36m8.8365\u001b[0m       \u001b[32m22.9145\u001b[0m  0.0005  14.2765\n",
      "     22        \u001b[36m8.3296\u001b[0m       31.2055  0.0005  14.1428\n",
      "     22        \u001b[36m9.5403\u001b[0m       32.2496  0.0003  14.1080\n",
      "     22       10.4408       \u001b[32m23.1027\u001b[0m  0.0010  13.7713\n",
      "     23        \u001b[36m7.9966\u001b[0m       \u001b[32m22.1398\u001b[0m  0.0005  12.4001\n",
      "     23        \u001b[36m7.7722\u001b[0m       31.8540  0.0005  12.5424\n",
      "     23        9.8017       \u001b[32m26.0947\u001b[0m  0.0001  12.4493\n",
      "     23       13.1850       50.3232  0.0010  12.2192\n",
      "     24        \u001b[36m7.5101\u001b[0m       \u001b[32m21.9303\u001b[0m  0.0005  12.2381\n",
      "     24        \u001b[36m7.4039\u001b[0m       30.9173  0.0005  12.0947\n",
      "     24        9.7018       \u001b[32m24.8376\u001b[0m  0.0001  12.3379\n",
      "     24       13.4678       57.4976  0.0010  12.5241\n",
      "     25        \u001b[36m7.1856\u001b[0m       22.2517  0.0005  12.2820\n",
      "     25        \u001b[36m7.1555\u001b[0m       28.8672  0.0005  12.5573\n",
      "     25        \u001b[36m9.2954\u001b[0m       \u001b[32m24.1999\u001b[0m  0.0001  12.8928\n",
      "     25       12.0048       62.7420  0.0010  12.8880\n",
      "     26        \u001b[36m6.9720\u001b[0m       22.6270  0.0005  12.5417\n",
      "     26        7.2332       \u001b[32m26.2472\u001b[0m  0.0003  12.3500\n",
      "     26        \u001b[36m8.9932\u001b[0m       \u001b[32m23.8787\u001b[0m  0.0001  12.6407\n",
      "     26       12.1950       71.4889  0.0010  12.4711\n",
      "     27        \u001b[36m6.8117\u001b[0m       22.8275  0.0005  12.3880\n",
      "     27        \u001b[36m7.1388\u001b[0m       \u001b[32m25.4286\u001b[0m  0.0003  12.2707\n",
      "     27        \u001b[36m8.7580\u001b[0m       \u001b[32m23.6550\u001b[0m  0.0001  12.5718\n",
      "     27       12.2368       32.1585  0.0005  12.6419\n",
      "     28        \u001b[36m6.6883\u001b[0m       23.0444  0.0005  12.4913\n",
      "     28        \u001b[36m6.8543\u001b[0m       \u001b[32m25.2774\u001b[0m  0.0003  12.3263\n",
      "     28        \u001b[36m8.5658\u001b[0m       \u001b[32m23.4302\u001b[0m  0.0001  12.4662\n",
      "     28       10.3125       42.5112  0.0005  12.5714\n",
      "     29        \u001b[36m6.6642\u001b[0m       \u001b[32m25.0794\u001b[0m  0.0003  12.3810\n",
      "     29        6.9072       26.6199  0.0003  12.5191\n",
      "     29        \u001b[36m8.3949\u001b[0m       \u001b[32m23.2811\u001b[0m  0.0001  12.7010\n",
      "     29        8.7155       39.8786  0.0005  12.7755\n",
      "     30        \u001b[36m6.5119\u001b[0m       \u001b[32m24.9521\u001b[0m  0.0003  12.7019\n",
      "     30        6.7959       26.0822  0.0003  12.6443\n",
      "     30        \u001b[36m8.2458\u001b[0m       \u001b[32m23.1318\u001b[0m  0.0001  12.8260\n",
      "     30        \u001b[36m7.8604\u001b[0m       38.1980  0.0005  12.8044\n",
      "     31        \u001b[36m6.3754\u001b[0m       \u001b[32m24.8816\u001b[0m  0.0003  12.7204\n",
      "     31        \u001b[36m6.5588\u001b[0m       25.6003  0.0003  12.8605\n",
      "     31        \u001b[36m8.1156\u001b[0m       \u001b[32m23.0514\u001b[0m  0.0001  12.9847\n",
      "     31        \u001b[36m7.1549\u001b[0m       38.6074  0.0003  13.1233\n",
      "     32        \u001b[36m6.2609\u001b[0m       24.9198  0.0003  12.8625\n",
      "     32        \u001b[36m6.4069\u001b[0m       25.2299  0.0003  13.0991\n",
      "     32        \u001b[36m7.9947\u001b[0m       \u001b[32m23.0069\u001b[0m  0.0001  13.5463\n",
      "     33        \u001b[36m6.1579\u001b[0m       24.9249  0.0003  13.3624\n",
      "     33        \u001b[36m6.2693\u001b[0m       22.4378  0.0001  13.3200\n",
      "     33        \u001b[36m7.8882\u001b[0m       \u001b[32m22.9525\u001b[0m  0.0001  12.7145\n",
      "     34        \u001b[36m6.0697\u001b[0m       \u001b[32m24.8241\u001b[0m  0.0003  11.4113\n",
      "     34        \u001b[36m7.7926\u001b[0m       \u001b[32m22.9331\u001b[0m  0.0001  11.1660\n",
      "     35        \u001b[36m5.9854\u001b[0m       \u001b[32m24.7150\u001b[0m  0.0003  9.7239\n",
      "     35        \u001b[36m7.7040\u001b[0m       \u001b[32m22.8620\u001b[0m  0.0001  9.3594\n",
      "     36        \u001b[36m5.9103\u001b[0m       24.7205  0.0003  9.4413\n",
      "     36        \u001b[36m7.6223\u001b[0m       \u001b[32m22.8076\u001b[0m  0.0001  9.5732\n",
      "     37        \u001b[36m5.8402\u001b[0m       \u001b[32m24.6936\u001b[0m  0.0003  9.3430\n",
      "     37        \u001b[36m7.5479\u001b[0m       22.8648  0.0001  9.4168\n",
      "     38        \u001b[36m5.7764\u001b[0m       24.7319  0.0003  9.3261\n",
      "     38        \u001b[36m7.4775\u001b[0m       22.8380  0.0001  9.5770\n",
      "     39        \u001b[36m5.7154\u001b[0m       \u001b[32m24.6554\u001b[0m  0.0003  9.3633\n",
      "     39        \u001b[36m7.4118\u001b[0m       22.9115  0.0001  9.4993\n",
      "     40        \u001b[36m5.6600\u001b[0m       \u001b[32m24.6295\u001b[0m  0.0003  9.2925\n",
      "     40        \u001b[36m7.3497\u001b[0m       22.8619  0.0001  9.3992\n",
      "     41        \u001b[36m5.6087\u001b[0m       24.6803  0.0003  9.3264\n",
      "     41        \u001b[36m7.2689\u001b[0m       \u001b[32m19.5509\u001b[0m  0.0001  9.5152\n",
      "     42        \u001b[36m5.5561\u001b[0m       24.7249  0.0003  9.4180\n",
      "     42        \u001b[36m7.2084\u001b[0m       \u001b[32m19.1376\u001b[0m  0.0001  9.6125\n",
      "     43        \u001b[36m5.5093\u001b[0m       \u001b[32m24.5194\u001b[0m  0.0003  9.6002\n",
      "     43        \u001b[36m7.1333\u001b[0m       \u001b[32m18.9653\u001b[0m  0.0001  9.6817\n",
      "     44        \u001b[36m5.4634\u001b[0m       \u001b[32m24.3428\u001b[0m  0.0003  9.5045\n",
      "     44        \u001b[36m7.0704\u001b[0m       \u001b[32m18.8939\u001b[0m  0.0001  9.6885\n",
      "     45        \u001b[36m5.4192\u001b[0m       \u001b[32m24.1782\u001b[0m  0.0003  9.8089\n",
      "     45        \u001b[36m7.0145\u001b[0m       \u001b[32m18.8732\u001b[0m  0.0001  9.9096\n",
      "     46        \u001b[36m5.3752\u001b[0m       \u001b[32m24.0901\u001b[0m  0.0003  9.3018\n",
      "     46        \u001b[36m6.9633\u001b[0m       18.8774  0.0001  9.4641\n",
      "     47        \u001b[36m5.3386\u001b[0m       \u001b[32m23.9445\u001b[0m  0.0003  9.3825\n",
      "     47        \u001b[36m6.9155\u001b[0m       18.8777  0.0001  9.6220\n",
      "     48        \u001b[36m5.2982\u001b[0m       \u001b[32m23.6093\u001b[0m  0.0003  9.5179\n",
      "     48        \u001b[36m6.8713\u001b[0m       18.8949  0.0001  9.4595\n",
      "     49        \u001b[36m5.2620\u001b[0m       \u001b[32m23.2704\u001b[0m  0.0003  9.3415\n",
      "     49        \u001b[36m6.8305\u001b[0m       18.8918  0.0001  9.5182\n",
      "     50        \u001b[36m5.2225\u001b[0m       \u001b[32m23.1653\u001b[0m  0.0003  9.3556\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "     50        \u001b[36m6.7499\u001b[0m       19.9238  0.0000  9.7417\n",
      "     51        \u001b[36m5.1888\u001b[0m       \u001b[32m22.8289\u001b[0m  0.0003  9.6261\n",
      "     51        \u001b[36m6.6869\u001b[0m       20.3609  0.0000  9.5170\n",
      "     52        \u001b[36m5.1553\u001b[0m       \u001b[32m22.5028\u001b[0m  0.0003  9.4808\n",
      "     52        \u001b[36m6.6459\u001b[0m       20.4418  0.0000  9.5652\n",
      "     53        \u001b[36m5.1198\u001b[0m       \u001b[32m22.2021\u001b[0m  0.0003  9.4331\n",
      "     53        \u001b[36m6.6167\u001b[0m       20.5255  0.0000  9.7341\n",
      "     54        \u001b[36m5.0870\u001b[0m       \u001b[32m21.8637\u001b[0m  0.0003  9.5087\n",
      "     54        \u001b[36m6.4689\u001b[0m       21.4668  0.0000  9.6896\n",
      "     55        \u001b[36m5.0524\u001b[0m       \u001b[32m21.5638\u001b[0m  0.0003  9.4550\n",
      "     56        \u001b[36m5.0211\u001b[0m       \u001b[32m21.0931\u001b[0m  0.0003  8.9020\n",
      "     57        \u001b[36m4.9899\u001b[0m       \u001b[32m20.5562\u001b[0m  0.0003  7.3422\n",
      "     58        \u001b[36m4.9636\u001b[0m       \u001b[32m20.3505\u001b[0m  0.0003  7.2534\n",
      "     59        \u001b[36m4.9386\u001b[0m       \u001b[32m19.9916\u001b[0m  0.0003  7.1460\n",
      "     60        \u001b[36m4.9138\u001b[0m       \u001b[32m19.6619\u001b[0m  0.0003  7.3313\n",
      "     61        \u001b[36m4.8957\u001b[0m       \u001b[32m19.4317\u001b[0m  0.0003  7.2102\n",
      "     62        \u001b[36m4.8726\u001b[0m       \u001b[32m19.2174\u001b[0m  0.0003  7.2875\n",
      "     63        \u001b[36m4.8575\u001b[0m       \u001b[32m19.0365\u001b[0m  0.0003  7.2298\n",
      "     64        \u001b[36m4.8422\u001b[0m       \u001b[32m18.8630\u001b[0m  0.0003  7.3194\n",
      "     65        \u001b[36m4.8313\u001b[0m       18.9213  0.0003  7.0807\n",
      "     66        4.8340       18.9482  0.0003  7.3276\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "     67        4.8367       18.9951  0.0003  7.3917\n",
      "     68        4.8544       19.1790  0.0003  7.3399\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "     69        5.8393       21.5874  0.0001  7.3270\n",
      "     70        6.0639       22.6467  0.0001  7.7078\n",
      "     71        6.0984       22.5207  0.0001  7.2803\n",
      "     72        6.1482       22.0084  0.0001  7.6603\n",
      "     73        6.3372       20.8621  0.0001  7.2537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m202.3417\u001b[0m      \u001b[32m246.4941\u001b[0m  0.0010  16.1925\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m222.5009\u001b[0m      \u001b[32m325.6901\u001b[0m  0.0010  16.3302\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m235.5505\u001b[0m      \u001b[32m158.1878\u001b[0m  0.0010  17.4147\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m229.7112\u001b[0m      \u001b[32m236.8569\u001b[0m  0.0010  17.4311\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m218.7376\u001b[0m      \u001b[32m240.9786\u001b[0m  0.0010  17.6249\n",
      "      2      \u001b[36m150.4913\u001b[0m      \u001b[32m146.1385\u001b[0m  0.0010  16.7478\n",
      "      2      \u001b[36m155.3715\u001b[0m      \u001b[32m257.0338\u001b[0m  0.0010  16.9931\n",
      "      2      \u001b[36m167.3280\u001b[0m      \u001b[32m138.6889\u001b[0m  0.0010  17.2556\n",
      "      2      \u001b[36m170.9561\u001b[0m      \u001b[32m195.3715\u001b[0m  0.0010  17.2244\n",
      "      2      \u001b[36m163.4856\u001b[0m      \u001b[32m167.2292\u001b[0m  0.0010  17.1803\n",
      "      3      \u001b[36m140.9198\u001b[0m      158.2351  0.0010  17.3639\n",
      "      3      \u001b[36m146.9679\u001b[0m      \u001b[32m249.5285\u001b[0m  0.0010  17.2677\n",
      "      3      \u001b[36m161.8754\u001b[0m      \u001b[32m136.1989\u001b[0m  0.0010  17.0697\n",
      "      3      \u001b[36m156.6344\u001b[0m      173.0070  0.0010  17.2113\n",
      "      3      172.7805      205.9556  0.0010  17.4114\n",
      "      4      \u001b[36m135.8490\u001b[0m      \u001b[32m133.4189\u001b[0m  0.0010  17.2622\n",
      "      4      \u001b[36m142.4993\u001b[0m      253.5025  0.0010  17.5063\n",
      "      4      \u001b[36m159.2917\u001b[0m      \u001b[32m129.3888\u001b[0m  0.0010  17.9433\n",
      "      4      \u001b[36m169.6346\u001b[0m      210.8242  0.0010  17.7456\n",
      "      4      \u001b[36m153.8800\u001b[0m      183.1125  0.0010  18.0587\n",
      "      5      \u001b[36m130.2726\u001b[0m      145.8603  0.0010  16.9672\n",
      "      5      \u001b[36m138.5250\u001b[0m      253.4192  0.0010  17.3096\n",
      "      5      \u001b[36m155.0662\u001b[0m      \u001b[32m127.9681\u001b[0m  0.0010  17.4141\n",
      "      5      \u001b[36m164.5506\u001b[0m      198.9598  0.0010  17.1880\n",
      "      5      \u001b[36m149.1103\u001b[0m      182.3093  0.0010  17.3513\n",
      "      6      \u001b[36m123.2763\u001b[0m      139.3692  0.0010  17.0454\n",
      "      6      \u001b[36m134.6153\u001b[0m      257.0504  0.0010  17.0772\n",
      "      6      \u001b[36m150.5348\u001b[0m      131.2783  0.0010  16.7363\n",
      "      6      \u001b[36m160.1443\u001b[0m      \u001b[32m177.2179\u001b[0m  0.0010  17.1116\n",
      "      6      \u001b[36m144.8764\u001b[0m      177.1311  0.0010  17.2539\n",
      "      7      \u001b[36m116.5147\u001b[0m      148.3477  0.0010  16.6196\n",
      "      7      \u001b[36m129.6175\u001b[0m      \u001b[32m246.4716\u001b[0m  0.0010  16.8067\n",
      "      7      \u001b[36m145.7991\u001b[0m      136.4913  0.0010  17.4319\n",
      "      7      \u001b[36m155.0543\u001b[0m      \u001b[32m166.1346\u001b[0m  0.0010  17.4483\n",
      "      7      151.7805      \u001b[32m162.6049\u001b[0m  0.0005  17.4385\n",
      "      8      \u001b[36m112.2451\u001b[0m      159.9032  0.0010  17.0664\n",
      "      8      \u001b[36m122.4666\u001b[0m      286.8637  0.0010  17.1778\n",
      "      8      \u001b[36m139.0445\u001b[0m      \u001b[32m121.4419\u001b[0m  0.0010  17.5697\n",
      "      8      \u001b[36m152.8364\u001b[0m      \u001b[32m163.7455\u001b[0m  0.0010  17.1426\n",
      "      8      150.6368      \u001b[32m154.4068\u001b[0m  0.0005  16.7780\n",
      "      9      \u001b[36m102.4161\u001b[0m      134.6488  0.0005  17.3238\n",
      "      9      \u001b[36m117.4420\u001b[0m      274.9976  0.0010  17.0800\n",
      "      9      \u001b[36m148.2135\u001b[0m      \u001b[32m162.5368\u001b[0m  0.0010  17.9566\n",
      "      9      145.1157      154.7826  0.0005  18.0737\n",
      "      9      \u001b[36m132.1503\u001b[0m      126.9386  0.0010  18.2397\n",
      "     10       \u001b[36m99.4746\u001b[0m      142.2954  0.0005  17.8545\n",
      "     10      \u001b[36m113.6010\u001b[0m      275.6342  0.0010  18.0321\n",
      "     10      \u001b[36m147.2149\u001b[0m      163.7781  0.0010  17.1951\n",
      "     10      \u001b[36m140.3092\u001b[0m      158.8232  0.0005  17.0894\n",
      "     10      \u001b[36m127.3742\u001b[0m      \u001b[32m118.1357\u001b[0m  0.0010  17.1996\n",
      "     11       \u001b[36m96.8560\u001b[0m      146.8953  0.0005  17.2233\n",
      "     11      \u001b[36m109.9136\u001b[0m      282.2448  0.0010  16.7271\n",
      "     11      \u001b[36m145.5258\u001b[0m      \u001b[32m155.1107\u001b[0m  0.0010  17.3552\n",
      "     11      \u001b[36m137.8683\u001b[0m      161.2494  0.0005  17.6042\n",
      "     11      \u001b[36m125.1920\u001b[0m      \u001b[32m117.6336\u001b[0m  0.0010  17.4849\n",
      "     12       \u001b[36m94.3496\u001b[0m      151.2332  0.0005  16.9249\n",
      "     12      \u001b[36m104.2333\u001b[0m      293.6159  0.0005  16.9771\n",
      "     12      \u001b[36m144.6825\u001b[0m      158.4863  0.0010  17.3925\n",
      "     12      \u001b[36m119.1449\u001b[0m      118.7207  0.0010  17.4056\n",
      "     12      \u001b[36m133.8216\u001b[0m      161.4641  0.0005  17.5377\n",
      "     13       \u001b[36m87.4986\u001b[0m      \u001b[32m126.1853\u001b[0m  0.0003  18.0947\n",
      "     13      \u001b[36m101.7577\u001b[0m      312.7279  0.0005  17.7891\n",
      "     13      145.6038      162.2673  0.0010  17.5108\n",
      "     13      \u001b[36m118.7659\u001b[0m      119.8299  0.0010  17.3259\n",
      "     13      135.7442      \u001b[32m136.2397\u001b[0m  0.0003  17.8839\n",
      "     14       \u001b[36m85.7310\u001b[0m      \u001b[32m120.2300\u001b[0m  0.0003  18.3567\n",
      "     14       \u001b[36m98.3617\u001b[0m      322.0720  0.0005  18.5474\n",
      "     14      146.3726      159.1449  0.0010  18.8038\n",
      "     14      \u001b[36m112.1749\u001b[0m      117.9936  0.0010  18.8986\n",
      "     14      136.7941      138.5123  0.0003  18.6968\n",
      "     15       \u001b[36m83.6302\u001b[0m      \u001b[32m118.0060\u001b[0m  0.0003  19.0110\n",
      "     15       \u001b[36m95.2830\u001b[0m      329.2592  0.0005  19.1483\n",
      "     15      \u001b[36m142.9001\u001b[0m      187.7770  0.0010  19.6789\n",
      "     15      \u001b[36m110.4474\u001b[0m      125.1311  0.0010  19.7069\n",
      "     15      135.2183      141.1469  0.0003  19.7436\n",
      "     16       \u001b[36m81.7777\u001b[0m      \u001b[32m116.7228\u001b[0m  0.0003  19.4916\n",
      "     16       \u001b[36m89.5599\u001b[0m      300.0324  0.0003  20.1884\n",
      "     16      155.4158      \u001b[32m124.5825\u001b[0m  0.0005  20.9527\n",
      "     16      122.1174      122.9879  0.0005  20.8022\n",
      "     16      134.1295      142.4532  0.0003  20.8560\n",
      "     17       \u001b[36m79.8933\u001b[0m      \u001b[32m115.7625\u001b[0m  0.0003  19.0758\n",
      "     17      143.8461      128.7548  0.0005  17.8243\n",
      "     17      125.3802      126.7925  0.0005  17.7115\n",
      "     17      \u001b[36m133.3229\u001b[0m      143.3602  0.0003  17.9678\n",
      "     18       \u001b[36m77.9724\u001b[0m      116.4575  0.0003  16.5691\n",
      "     18      \u001b[36m141.0994\u001b[0m      125.1605  0.0005  15.8509\n",
      "     18      118.7837      \u001b[32m117.2616\u001b[0m  0.0005  16.0843\n",
      "     18      137.5577      \u001b[32m128.8515\u001b[0m  0.0001  16.0314\n",
      "     19       \u001b[36m76.0496\u001b[0m      117.1144  0.0003  15.4976\n",
      "     19      \u001b[36m138.7691\u001b[0m      126.0456  0.0005  15.7890\n",
      "     19      118.8090      \u001b[32m116.6545\u001b[0m  0.0005  15.9683\n",
      "     19      \u001b[36m130.2762\u001b[0m      \u001b[32m127.3466\u001b[0m  0.0001  16.1555\n",
      "     20       \u001b[36m74.0922\u001b[0m      120.4100  0.0003  15.7154\n",
      "     20      \u001b[36m137.4210\u001b[0m      127.5024  0.0005  16.4381\n",
      "     20      116.1739      \u001b[32m111.2175\u001b[0m  0.0005  16.1185\n",
      "     20      \u001b[36m128.5605\u001b[0m      \u001b[32m126.3824\u001b[0m  0.0001  16.1920\n",
      "     21       \u001b[36m72.1254\u001b[0m      121.2854  0.0003  16.0486\n",
      "     21      140.1731      \u001b[32m113.4749\u001b[0m  0.0003  16.7858\n",
      "     21      113.2466      \u001b[32m110.0287\u001b[0m  0.0005  16.6055\n",
      "     21      \u001b[36m127.1338\u001b[0m      \u001b[32m126.1740\u001b[0m  0.0001  16.7080\n",
      "     22       \u001b[36m69.9344\u001b[0m      \u001b[32m111.2247\u001b[0m  0.0001  16.2237\n",
      "     22      \u001b[36m109.8886\u001b[0m      \u001b[32m108.7928\u001b[0m  0.0005  15.9394\n",
      "     22      138.7968      \u001b[32m110.9105\u001b[0m  0.0003  16.0678\n",
      "     22      \u001b[36m125.9051\u001b[0m      \u001b[32m126.0600\u001b[0m  0.0001  16.0443\n",
      "     23       \u001b[36m68.5269\u001b[0m      115.7048  0.0001  15.7865\n",
      "     23      \u001b[36m107.3384\u001b[0m      109.2470  0.0005  16.7122\n",
      "     23      \u001b[36m136.9595\u001b[0m      \u001b[32m110.8443\u001b[0m  0.0003  16.5981\n",
      "     23      \u001b[36m124.7643\u001b[0m      \u001b[32m125.8193\u001b[0m  0.0001  16.2541\n",
      "     24       \u001b[36m67.4115\u001b[0m      118.1676  0.0001  16.0454\n",
      "     24      \u001b[36m104.4760\u001b[0m      109.2041  0.0005  15.9249\n",
      "     24      \u001b[36m134.8554\u001b[0m      \u001b[32m109.9196\u001b[0m  0.0003  16.0839\n",
      "     24      \u001b[36m123.6395\u001b[0m      \u001b[32m125.6879\u001b[0m  0.0001  16.2616\n",
      "     25       \u001b[36m66.4506\u001b[0m      120.3517  0.0001  15.6480\n",
      "     25      \u001b[36m132.8388\u001b[0m      \u001b[32m108.9290\u001b[0m  0.0003  16.0290\n",
      "     25      \u001b[36m101.8594\u001b[0m      110.2863  0.0005  16.2120\n",
      "     25      \u001b[36m122.5882\u001b[0m      \u001b[32m125.6439\u001b[0m  0.0001  16.0834\n",
      "     26       \u001b[36m65.5473\u001b[0m      122.2481  0.0001  15.4790\n",
      "     26       \u001b[36m98.3483\u001b[0m      110.5799  0.0005  16.0494\n",
      "     26      \u001b[36m130.8126\u001b[0m      \u001b[32m108.8965\u001b[0m  0.0003  16.1001\n",
      "     26      \u001b[36m121.5640\u001b[0m      \u001b[32m125.3996\u001b[0m  0.0001  15.9286\n",
      "     27       66.0804      148.8828  0.0001  15.6070\n",
      "     27      110.2526      115.3781  0.0003  15.7938\n",
      "     27      \u001b[36m128.8198\u001b[0m      110.9735  0.0003  15.9694\n",
      "     27      \u001b[36m120.5560\u001b[0m      \u001b[32m125.0011\u001b[0m  0.0001  16.1309\n",
      "     28       66.2379      142.5345  0.0001  15.2444\n",
      "     28      107.8301      127.3933  0.0003  15.7499\n",
      "     28      \u001b[36m126.9931\u001b[0m      114.5147  0.0003  16.1423\n",
      "     28      \u001b[36m119.5772\u001b[0m      \u001b[32m124.9638\u001b[0m  0.0001  15.7173\n",
      "     29       65.6194      138.4768  0.0001  15.5042\n",
      "     29      106.6490      158.2967  0.0003  15.9288\n",
      "     29      \u001b[36m125.4882\u001b[0m      116.5081  0.0003  16.1262\n",
      "     29      \u001b[36m118.5918\u001b[0m      \u001b[32m124.4423\u001b[0m  0.0001  16.0589\n",
      "     30       \u001b[36m65.0252\u001b[0m      137.7380  0.0001  15.6428\n",
      "     30      104.7310      161.6177  0.0003  15.7168\n",
      "     30      \u001b[36m124.1444\u001b[0m      116.6763  0.0003  16.2408\n",
      "     30      \u001b[36m117.6205\u001b[0m      \u001b[32m124.0218\u001b[0m  0.0001  15.9803\n",
      "     31       66.9689      111.7790  0.0000  15.3681\n",
      "     31      106.5012      127.8885  0.0001  15.6834\n",
      "     31      124.8289      124.5244  0.0001  15.6593\n",
      "     31      \u001b[36m116.6386\u001b[0m      \u001b[32m123.5497\u001b[0m  0.0001  16.1040\n",
      "     32      \u001b[36m112.4610\u001b[0m      126.4957  0.0001  13.9352\n",
      "     32      \u001b[36m115.6610\u001b[0m      \u001b[32m123.2384\u001b[0m  0.0001  13.3391\n",
      "     33      \u001b[36m111.3624\u001b[0m      126.1347  0.0001  11.3898\n",
      "     33      \u001b[36m114.6944\u001b[0m      \u001b[32m122.8433\u001b[0m  0.0001  11.3981\n",
      "     34      \u001b[36m113.7259\u001b[0m      \u001b[32m122.5641\u001b[0m  0.0001  11.2962\n",
      "     34      \u001b[36m110.2773\u001b[0m      125.9565  0.0001  11.3775\n",
      "     35      \u001b[36m112.7470\u001b[0m      \u001b[32m122.3928\u001b[0m  0.0001  11.1442\n",
      "     35      \u001b[36m107.3558\u001b[0m      115.0266  0.0001  11.2307\n",
      "     36      \u001b[36m111.7639\u001b[0m      \u001b[32m122.1266\u001b[0m  0.0001  11.2555\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "     37      \u001b[36m110.7930\u001b[0m      \u001b[32m122.1072\u001b[0m  0.0001  9.1360\n",
      "     38      \u001b[36m109.7659\u001b[0m      \u001b[32m122.1058\u001b[0m  0.0001  8.8015\n",
      "     39      \u001b[36m108.7721\u001b[0m      122.2335  0.0001  8.7858\n",
      "     40      \u001b[36m107.7177\u001b[0m      122.2656  0.0001  12.7669\n",
      "     41      \u001b[36m106.6656\u001b[0m      122.5789  0.0001  11.2675\n",
      "     42      \u001b[36m106.0820\u001b[0m      129.2844  0.0001  9.3350\n",
      "     43       \u001b[36m96.6308\u001b[0m      128.2021  0.0001  10.1710\n",
      "     44       \u001b[36m96.0855\u001b[0m      128.6596  0.0001  8.8313\n",
      "     45       \u001b[36m95.3985\u001b[0m      128.8392  0.0001  9.2132\n",
      "     46       \u001b[36m93.7975\u001b[0m      161.4873  0.0000  8.8875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m467.1504\u001b[0m      \u001b[32m389.7523\u001b[0m  0.0010  0.1806\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m463.8901\u001b[0m      \u001b[32m394.7422\u001b[0m  0.0010  0.1725\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m466.2013\u001b[0m      \u001b[32m377.7791\u001b[0m  0.0010  0.1789\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m462.8545\u001b[0m      \u001b[32m387.7727\u001b[0m  0.0010  0.1818\n",
      "      2      \u001b[36m276.3389\u001b[0m      \u001b[32m178.4213\u001b[0m  0.0010  0.1911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2      \u001b[36m275.5117\u001b[0m      \u001b[32m171.5878\u001b[0m  0.0010  0.1601\n",
      "      2      \u001b[36m269.1371\u001b[0m      \u001b[32m174.5052\u001b[0m  0.0010  0.1755\n",
      "      2      \u001b[36m274.2269\u001b[0m      \u001b[32m175.8727\u001b[0m  0.0010  0.1609\n",
      "      3      \u001b[36m162.3603\u001b[0m      \u001b[32m138.3615\u001b[0m  0.0010  0.1358\n",
      "      3      \u001b[36m168.8254\u001b[0m      \u001b[32m135.7524\u001b[0m  0.0010  0.1704\n",
      "      3      \u001b[36m168.9225\u001b[0m      \u001b[32m142.9974\u001b[0m  0.0010  0.1588\n",
      "      3      \u001b[36m166.6689\u001b[0m      \u001b[32m137.2368\u001b[0m  0.0010  0.1450\n",
      "      4      \u001b[36m148.0222\u001b[0m      \u001b[32m131.0682\u001b[0m  0.0010  0.1717\n",
      "      4      \u001b[36m143.0514\u001b[0m      \u001b[32m121.9338\u001b[0m  0.0010  0.1784\n",
      "      4      \u001b[36m139.4087\u001b[0m      \u001b[32m125.0824\u001b[0m  0.0010  0.1913\n",
      "      4      \u001b[36m142.7894\u001b[0m      \u001b[32m123.0034\u001b[0m  0.0010  0.1874\n",
      "      5      \u001b[36m125.6431\u001b[0m      \u001b[32m111.8823\u001b[0m  0.0010  0.1612\n",
      "      5      \u001b[36m122.4297\u001b[0m      \u001b[32m113.3558\u001b[0m  0.0010  0.1591\n",
      "      5      \u001b[36m131.5352\u001b[0m      \u001b[32m121.3479\u001b[0m  0.0010  0.1868\n",
      "      5      \u001b[36m127.5069\u001b[0m      \u001b[32m110.6329\u001b[0m  0.0010  0.1424\n",
      "      6      \u001b[36m109.5848\u001b[0m      \u001b[32m103.5144\u001b[0m  0.0010  0.1642\n",
      "      6      \u001b[36m107.6336\u001b[0m      \u001b[32m104.8038\u001b[0m  0.0010  0.1748\n",
      "      6      \u001b[36m117.9264\u001b[0m      \u001b[32m108.9451\u001b[0m  0.0010  0.1897\n",
      "      6      \u001b[36m113.2596\u001b[0m      \u001b[32m100.1538\u001b[0m  0.0010  0.2346\n",
      "      7       \u001b[36m98.2783\u001b[0m       \u001b[32m98.7486\u001b[0m  0.0010  0.2131\n",
      "      7      \u001b[36m100.8915\u001b[0m       \u001b[32m97.4816\u001b[0m  0.0010  0.2581\n",
      "      7      \u001b[36m105.4786\u001b[0m      \u001b[32m101.5861\u001b[0m  0.0010  0.2180\n",
      "      7      \u001b[36m101.8031\u001b[0m       \u001b[32m91.9769\u001b[0m  0.0010  0.1724\n",
      "      8       \u001b[36m93.1629\u001b[0m       \u001b[32m93.6624\u001b[0m  0.0010  0.1462\n",
      "      8       \u001b[36m96.3869\u001b[0m       \u001b[32m92.2915\u001b[0m  0.0010  0.1415\n",
      "      8       \u001b[36m95.0571\u001b[0m       \u001b[32m91.6773\u001b[0m  0.0010  0.1511\n",
      "      8       \u001b[36m95.7482\u001b[0m       \u001b[32m84.0319\u001b[0m  0.0010  0.1590\n",
      "      9       \u001b[36m88.1604\u001b[0m       \u001b[32m87.6548\u001b[0m  0.0010  0.1294\n",
      "      9       \u001b[36m90.4258\u001b[0m       \u001b[32m85.8284\u001b[0m  0.0010  0.1485\n",
      "      9       \u001b[36m93.0140\u001b[0m       \u001b[32m86.8482\u001b[0m  0.0010  0.1615\n",
      "      9       \u001b[36m90.5291\u001b[0m       \u001b[32m79.9271\u001b[0m  0.0010  0.1595\n",
      "     10       \u001b[36m84.9557\u001b[0m       \u001b[32m83.3901\u001b[0m  0.0010  0.1373\n",
      "     10       \u001b[36m85.2866\u001b[0m       \u001b[32m79.3568\u001b[0m  0.0010  0.1445\n",
      "     10       \u001b[36m88.3352\u001b[0m       \u001b[32m83.6069\u001b[0m  0.0010  0.1594\n",
      "     10       \u001b[36m87.4270\u001b[0m       \u001b[32m76.2389\u001b[0m  0.0010  0.1477\n",
      "     11       \u001b[36m79.9640\u001b[0m       \u001b[32m79.7485\u001b[0m  0.0010  0.1660\n",
      "     11       \u001b[36m80.1682\u001b[0m       \u001b[32m74.6641\u001b[0m  0.0010  0.1456\n",
      "     11       \u001b[36m85.0138\u001b[0m       \u001b[32m77.1694\u001b[0m  0.0010  0.1480\n",
      "     11       \u001b[36m82.5603\u001b[0m       \u001b[32m71.3512\u001b[0m  0.0010  0.1553\n",
      "     12       \u001b[36m77.2103\u001b[0m       \u001b[32m74.7706\u001b[0m  0.0010  0.1645\n",
      "     12       \u001b[36m76.8319\u001b[0m       \u001b[32m71.2427\u001b[0m  0.0010  0.1495\n",
      "     12       \u001b[36m81.3619\u001b[0m       \u001b[32m76.0830\u001b[0m  0.0010  0.1721\n",
      "     12       \u001b[36m78.9430\u001b[0m       \u001b[32m67.6078\u001b[0m  0.0010  0.1759\n",
      "     13       \u001b[36m73.8007\u001b[0m       \u001b[32m71.4233\u001b[0m  0.0010  0.1756\n",
      "     13       \u001b[36m73.1036\u001b[0m       \u001b[32m66.3746\u001b[0m  0.0010  0.1830\n",
      "     13       \u001b[36m78.3262\u001b[0m       \u001b[32m69.1998\u001b[0m  0.0010  0.1932\n",
      "     13       \u001b[36m75.2671\u001b[0m       \u001b[32m63.9133\u001b[0m  0.0010  0.1725\n",
      "     14       \u001b[36m72.0789\u001b[0m       \u001b[32m68.1648\u001b[0m  0.0010  0.1660\n",
      "     14       \u001b[36m69.9109\u001b[0m       \u001b[32m63.6415\u001b[0m  0.0010  0.1622\n",
      "     14       \u001b[36m73.1770\u001b[0m       \u001b[32m68.1538\u001b[0m  0.0010  0.1747\n",
      "     14       \u001b[36m71.6851\u001b[0m       \u001b[32m60.7021\u001b[0m  0.0010  0.1667\n",
      "     15       \u001b[36m68.7502\u001b[0m       \u001b[32m65.3176\u001b[0m  0.0010  0.1652\n",
      "     15       \u001b[36m68.0503\u001b[0m       \u001b[32m60.7338\u001b[0m  0.0010  0.1629\n",
      "     15       \u001b[36m71.1287\u001b[0m       \u001b[32m64.5446\u001b[0m  0.0010  0.1916\n",
      "     16       \u001b[36m66.0953\u001b[0m       \u001b[32m61.4350\u001b[0m  0.0010  0.1599\n",
      "     15       \u001b[36m69.1184\u001b[0m       \u001b[32m57.9675\u001b[0m  0.0010  0.2030\n",
      "     16       \u001b[36m66.6954\u001b[0m       \u001b[32m57.8676\u001b[0m  0.0010  0.1603\n",
      "     16       \u001b[36m68.7163\u001b[0m       \u001b[32m61.5666\u001b[0m  0.0010  0.1683\n",
      "     16       \u001b[36m66.3546\u001b[0m       \u001b[32m56.6959\u001b[0m  0.0010  0.1744\n",
      "     17       \u001b[36m61.5791\u001b[0m       \u001b[32m55.2214\u001b[0m  0.0010  0.1738\n",
      "     17       \u001b[36m64.0200\u001b[0m       \u001b[32m59.1232\u001b[0m  0.0010  0.1887\n",
      "     17       \u001b[36m64.9753\u001b[0m       \u001b[32m60.2638\u001b[0m  0.0010  0.1490\n",
      "     18       \u001b[36m61.4469\u001b[0m       \u001b[32m57.1918\u001b[0m  0.0010  0.1533\n",
      "     18       \u001b[36m60.7238\u001b[0m       \u001b[32m54.2525\u001b[0m  0.0010  0.1696\n",
      "     17       \u001b[36m63.7346\u001b[0m       \u001b[32m53.7534\u001b[0m  0.0010  0.1786\n",
      "     18       \u001b[36m64.1443\u001b[0m       \u001b[32m56.8559\u001b[0m  0.0010  0.1708\n",
      "     19       \u001b[36m59.3200\u001b[0m       \u001b[32m55.1333\u001b[0m  0.0010  0.1766\n",
      "     19       \u001b[36m58.9430\u001b[0m       \u001b[32m51.2090\u001b[0m  0.0010  0.1686\n",
      "     18       \u001b[36m62.2160\u001b[0m       \u001b[32m52.7111\u001b[0m  0.0010  0.1715\n",
      "     19       \u001b[36m61.7793\u001b[0m       \u001b[32m54.9551\u001b[0m  0.0010  0.1740\n",
      "     20       \u001b[36m58.4977\u001b[0m       \u001b[32m53.4607\u001b[0m  0.0010  0.1765\n",
      "     20       \u001b[36m57.8836\u001b[0m       51.6265  0.0010  0.1764\n",
      "     19       \u001b[36m60.0417\u001b[0m       \u001b[32m49.5275\u001b[0m  0.0010  0.1677\n",
      "     20       \u001b[36m59.5847\u001b[0m       \u001b[32m53.7748\u001b[0m  0.0010  0.1767\n",
      "     21       \u001b[36m55.2155\u001b[0m       \u001b[32m52.5015\u001b[0m  0.0010  0.1787\n",
      "     20       \u001b[36m59.9477\u001b[0m       \u001b[32m48.3287\u001b[0m  0.0010  0.1850\n",
      "     21       \u001b[36m56.4891\u001b[0m       \u001b[32m49.5505\u001b[0m  0.0010  0.1938\n",
      "     21       \u001b[36m58.7550\u001b[0m       \u001b[32m51.0016\u001b[0m  0.0010  0.1909\n",
      "     21       \u001b[36m57.2612\u001b[0m       \u001b[32m47.8218\u001b[0m  0.0010  0.1625\n",
      "     22       \u001b[36m54.2355\u001b[0m       \u001b[32m51.0907\u001b[0m  0.0010  0.1903\n",
      "     22       \u001b[36m54.8650\u001b[0m       \u001b[32m49.4122\u001b[0m  0.0010  0.1987\n",
      "     22       \u001b[36m57.2360\u001b[0m       \u001b[32m49.6734\u001b[0m  0.0010  0.1673\n",
      "     23       \u001b[36m52.4764\u001b[0m       \u001b[32m50.9877\u001b[0m  0.0010  0.1625\n",
      "     22       \u001b[36m55.5617\u001b[0m       \u001b[32m46.8657\u001b[0m  0.0010  0.1769\n",
      "     23       \u001b[36m52.9753\u001b[0m       \u001b[32m46.6456\u001b[0m  0.0010  0.1815\n",
      "     23       \u001b[36m56.3156\u001b[0m       49.7315  0.0010  0.1596\n",
      "     23       \u001b[36m54.0132\u001b[0m       \u001b[32m45.2157\u001b[0m  0.0010  0.1481\n",
      "     24       52.9257       \u001b[32m49.1883\u001b[0m  0.0010  0.1639\n",
      "     24       \u001b[36m51.4613\u001b[0m       \u001b[32m45.2892\u001b[0m  0.0010  0.1398\n",
      "     24       \u001b[36m54.9028\u001b[0m       \u001b[32m47.1780\u001b[0m  0.0010  0.1470\n",
      "     25       \u001b[36m51.8935\u001b[0m       49.3370  0.0010  0.1574\n",
      "     24       \u001b[36m52.2534\u001b[0m       \u001b[32m43.5991\u001b[0m  0.0010  0.1703\n",
      "     25       \u001b[36m50.6425\u001b[0m       \u001b[32m43.3970\u001b[0m  0.0010  0.1491\n",
      "     25       \u001b[36m53.9340\u001b[0m       \u001b[32m45.1268\u001b[0m  0.0010  0.1661\n",
      "     25       \u001b[36m52.0425\u001b[0m       \u001b[32m41.9102\u001b[0m  0.0010  0.1345\n",
      "     26       \u001b[36m50.2628\u001b[0m       \u001b[32m48.0392\u001b[0m  0.0010  0.1402\n",
      "     26       \u001b[36m49.2762\u001b[0m       \u001b[32m42.0369\u001b[0m  0.0010  0.1689\n",
      "     26       \u001b[36m53.1805\u001b[0m       \u001b[32m42.7889\u001b[0m  0.0010  0.1569\n",
      "     26       \u001b[36m51.1413\u001b[0m       42.6531  0.0010  0.1584\n",
      "     27       \u001b[36m47.1831\u001b[0m       \u001b[32m45.9847\u001b[0m  0.0010  0.1621\n",
      "     27       \u001b[36m47.6893\u001b[0m       \u001b[32m40.1889\u001b[0m  0.0010  0.1703\n",
      "     27       \u001b[36m50.1832\u001b[0m       42.8836  0.0010  0.1634\n",
      "     28       48.4005       \u001b[32m44.5282\u001b[0m  0.0010  0.1463\n",
      "     27       \u001b[36m50.1793\u001b[0m       \u001b[32m39.8989\u001b[0m  0.0010  0.1812\n",
      "     28       \u001b[36m46.3704\u001b[0m       \u001b[32m38.5307\u001b[0m  0.0010  0.1741\n",
      "     28       \u001b[36m49.5632\u001b[0m       \u001b[32m41.1406\u001b[0m  0.0010  0.1466\n",
      "     29       \u001b[36m46.4582\u001b[0m       \u001b[32m43.5345\u001b[0m  0.0010  0.1695\n",
      "     28       \u001b[36m48.5656\u001b[0m       \u001b[32m38.0832\u001b[0m  0.0010  0.1736\n",
      "     29       46.8798       \u001b[32m37.2551\u001b[0m  0.0010  0.1663\n",
      "     29       \u001b[36m49.2832\u001b[0m       42.4293  0.0010  0.1611\n",
      "     30       \u001b[36m44.9226\u001b[0m       \u001b[32m43.4351\u001b[0m  0.0010  0.1818\n",
      "     29       \u001b[36m47.1917\u001b[0m       38.4365  0.0010  0.1798\n",
      "     30       \u001b[36m45.2059\u001b[0m       37.5355  0.0010  0.1621\n",
      "     30       \u001b[36m48.9235\u001b[0m       \u001b[32m39.6966\u001b[0m  0.0010  0.1701\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m453.7679\u001b[0m      \u001b[32m369.6895\u001b[0m  0.0010  0.1467\n",
      "     31       \u001b[36m43.3780\u001b[0m       \u001b[32m42.7760\u001b[0m  0.0010  0.1603\n",
      "     30       47.4618       \u001b[32m36.7618\u001b[0m  0.0010  0.1349\n",
      "     31       \u001b[36m48.2072\u001b[0m       \u001b[32m38.4131\u001b[0m  0.0010  0.1568\n",
      "     31       \u001b[36m44.6443\u001b[0m       \u001b[32m35.0138\u001b[0m  0.0010  0.1730\n",
      "      2      \u001b[36m258.8349\u001b[0m      \u001b[32m168.4307\u001b[0m  0.0010  0.1648\n",
      "     32       \u001b[36m43.3768\u001b[0m       \u001b[32m40.9365\u001b[0m  0.0010  0.1289\n",
      "     31       \u001b[36m44.9284\u001b[0m       36.9397  0.0010  0.1676\n",
      "     32       \u001b[36m45.6138\u001b[0m       39.1959  0.0010  0.1542\n",
      "      3      \u001b[36m164.5318\u001b[0m      \u001b[32m136.4153\u001b[0m  0.0010  0.1419\n",
      "     32       \u001b[36m43.2475\u001b[0m       35.1703  0.0010  0.1775\n",
      "     33       \u001b[36m41.5493\u001b[0m       \u001b[32m39.7153\u001b[0m  0.0010  0.1475\n",
      "     32       \u001b[36m42.7670\u001b[0m       \u001b[32m35.0494\u001b[0m  0.0010  0.1461\n",
      "     33       \u001b[36m45.1942\u001b[0m       \u001b[32m35.9819\u001b[0m  0.0010  0.1644\n",
      "     33       43.2699       \u001b[32m34.6291\u001b[0m  0.0010  0.1419\n",
      "      4      \u001b[36m145.8459\u001b[0m      \u001b[32m126.1108\u001b[0m  0.0010  0.1636\n",
      "     34       \u001b[36m41.3130\u001b[0m       40.5340  0.0010  0.1804\n",
      "     33       \u001b[36m42.5201\u001b[0m       \u001b[32m35.0109\u001b[0m  0.0010  0.1707\n",
      "     34       \u001b[36m44.6113\u001b[0m       \u001b[32m35.3434\u001b[0m  0.0010  0.1421\n",
      "      5      \u001b[36m131.0754\u001b[0m      \u001b[32m113.1863\u001b[0m  0.0010  0.1525\n",
      "     34       \u001b[36m42.1460\u001b[0m       \u001b[32m32.7228\u001b[0m  0.0010  0.1563\n",
      "     35       42.6960       \u001b[32m39.0013\u001b[0m  0.0010  0.1498\n",
      "     34       42.9766       \u001b[32m33.5143\u001b[0m  0.0010  0.1556\n",
      "      6      \u001b[36m115.1096\u001b[0m      \u001b[32m100.2820\u001b[0m  0.0010  0.1457\n",
      "     35       \u001b[36m43.4952\u001b[0m       \u001b[32m33.6046\u001b[0m  0.0010  0.1707\n",
      "     35       \u001b[36m41.8882\u001b[0m       \u001b[32m32.3514\u001b[0m  0.0010  0.1553\n",
      "     36       \u001b[36m39.9131\u001b[0m       39.0612  0.0010  0.1441\n",
      "     35       43.5623       \u001b[32m33.0859\u001b[0m  0.0010  0.1547\n",
      "      7      \u001b[36m103.2901\u001b[0m       \u001b[32m90.2452\u001b[0m  0.0010  0.1492\n",
      "     36       \u001b[36m42.3962\u001b[0m       \u001b[32m31.3131\u001b[0m  0.0010  0.1536\n",
      "     36       \u001b[36m40.7291\u001b[0m       \u001b[32m32.2691\u001b[0m  0.0010  0.1804\n",
      "     37       \u001b[36m39.5054\u001b[0m       39.0517  0.0010  0.1702\n",
      "     36       \u001b[36m41.3136\u001b[0m       \u001b[32m31.5844\u001b[0m  0.0010  0.1496\n",
      "     37       43.1057       33.2044  0.0010  0.1415\n",
      "      8       \u001b[36m96.3491\u001b[0m       \u001b[32m84.0778\u001b[0m  0.0010  0.1580\n",
      "     37       41.1375       \u001b[32m31.8497\u001b[0m  0.0010  0.1605\n",
      "     38       \u001b[36m39.3601\u001b[0m       \u001b[32m37.8358\u001b[0m  0.0010  0.1477\n",
      "     37       \u001b[36m40.7111\u001b[0m       32.7479  0.0010  0.1533\n",
      "      9       \u001b[36m90.8293\u001b[0m       \u001b[32m80.3936\u001b[0m  0.0010  0.1385\n",
      "     38       \u001b[36m41.7862\u001b[0m       \u001b[32m31.2933\u001b[0m  0.0010  0.1626\n",
      "     38       \u001b[36m39.0451\u001b[0m       \u001b[32m30.9694\u001b[0m  0.0010  0.1437\n",
      "     39       \u001b[36m37.7148\u001b[0m       \u001b[32m37.3179\u001b[0m  0.0010  0.1475\n",
      "     38       \u001b[36m39.9249\u001b[0m       \u001b[32m31.3576\u001b[0m  0.0010  0.1440\n",
      "     10       \u001b[36m86.5847\u001b[0m       \u001b[32m75.4512\u001b[0m  0.0010  0.1404\n",
      "     39       42.5249       \u001b[32m30.4254\u001b[0m  0.0010  0.1744\n",
      "     39       39.2475       \u001b[32m30.7504\u001b[0m  0.0010  0.1833\n",
      "     40       38.7405       37.7595  0.0010  0.1930\n",
      "     39       40.2224       \u001b[32m30.2163\u001b[0m  0.0010  0.2018\n",
      "     11       \u001b[36m84.5500\u001b[0m       \u001b[32m72.3386\u001b[0m  0.0010  0.1812\n",
      "     40       \u001b[36m40.9059\u001b[0m       32.1898  0.0010  0.2057\n",
      "     40       \u001b[36m38.6463\u001b[0m       \u001b[32m29.6691\u001b[0m  0.0010  0.1989\n",
      "     41       38.8554       38.2778  0.0010  0.2113\n",
      "     40       40.3403       \u001b[32m29.5767\u001b[0m  0.0010  0.1972\n",
      "     12       \u001b[36m81.9325\u001b[0m       \u001b[32m69.8530\u001b[0m  0.0010  0.1908\n",
      "     41       41.2375       31.0140  0.0010  0.2099\n",
      "     41       \u001b[36m38.1360\u001b[0m       \u001b[32m29.5214\u001b[0m  0.0010  0.2158\n",
      "     42       37.9515       37.9582  0.0010  0.1954\n",
      "     13       \u001b[36m77.1273\u001b[0m       \u001b[32m66.3855\u001b[0m  0.0010  0.1908\n",
      "     41       \u001b[36m37.9304\u001b[0m       29.6092  0.0010  0.2110\n",
      "     42       \u001b[36m39.5618\u001b[0m       30.7437  0.0010  0.1892\n",
      "     42       \u001b[36m37.3204\u001b[0m       \u001b[32m29.0234\u001b[0m  0.0010  0.1868\n",
      "     43       \u001b[36m37.5836\u001b[0m       \u001b[32m36.0924\u001b[0m  0.0010  0.1935\n",
      "     14       \u001b[36m73.9710\u001b[0m       \u001b[32m65.0403\u001b[0m  0.0010  0.1858\n",
      "     42       \u001b[36m37.7007\u001b[0m       \u001b[32m28.0055\u001b[0m  0.0010  0.1999\n",
      "     43       39.9095       \u001b[32m29.9725\u001b[0m  0.0010  0.1849\n",
      "     43       \u001b[36m36.3655\u001b[0m       \u001b[32m28.6622\u001b[0m  0.0010  0.1943\n",
      "     44       \u001b[36m37.2564\u001b[0m       \u001b[32m35.0814\u001b[0m  0.0010  0.1873\n",
      "     15       \u001b[36m72.2114\u001b[0m       \u001b[32m60.7784\u001b[0m  0.0010  0.1856\n",
      "     43       37.8879       28.8779  0.0010  0.1902\n",
      "     44       \u001b[36m39.1140\u001b[0m       \u001b[32m28.4848\u001b[0m  0.0010  0.2066\n",
      "     44       36.4927       29.8703  0.0010  0.1841\n",
      "     45       \u001b[36m36.2802\u001b[0m       \u001b[32m34.9837\u001b[0m  0.0010  0.1725\n",
      "     16       \u001b[36m69.1121\u001b[0m       \u001b[32m58.6365\u001b[0m  0.0010  0.1874\n",
      "     44       \u001b[36m36.5182\u001b[0m       \u001b[32m26.9957\u001b[0m  0.0010  0.2270\n",
      "     45       \u001b[36m38.1891\u001b[0m       \u001b[32m28.1948\u001b[0m  0.0010  0.2312\n",
      "     45       36.9868       \u001b[32m27.6269\u001b[0m  0.0010  0.2230\n",
      "     46       \u001b[36m36.0559\u001b[0m       35.2451  0.0010  0.2096\n",
      "     17       \u001b[36m67.0783\u001b[0m       \u001b[32m56.2799\u001b[0m  0.0010  0.2247\n",
      "     45       \u001b[36m35.2921\u001b[0m       \u001b[32m26.4128\u001b[0m  0.0010  0.2006\n",
      "     46       38.3747       \u001b[32m26.9772\u001b[0m  0.0010  0.1824\n",
      "     47       \u001b[36m34.4843\u001b[0m       \u001b[32m34.6637\u001b[0m  0.0010  0.2052\n",
      "     46       \u001b[36m36.2958\u001b[0m       27.7379  0.0010  0.2265\n",
      "     18       \u001b[36m64.1128\u001b[0m       \u001b[32m55.2741\u001b[0m  0.0010  0.2474\n",
      "     46       36.0050       27.2637  0.0010  0.3024\n",
      "     47       38.4899       \u001b[32m26.9674\u001b[0m  0.0010  0.2921\n",
      "     48       35.9526       \u001b[32m33.3635\u001b[0m  0.0010  0.2772\n",
      "     47       \u001b[36m35.7019\u001b[0m       \u001b[32m26.9435\u001b[0m  0.0010  0.2669\n",
      "     19       64.4659       \u001b[32m53.2488\u001b[0m  0.0010  0.2071\n",
      "     47       36.2387       \u001b[32m25.9793\u001b[0m  0.0010  0.1768\n",
      "     48       \u001b[36m36.5746\u001b[0m       27.3026  0.0010  0.1772\n",
      "     49       \u001b[36m33.7661\u001b[0m       34.0590  0.0010  0.1701\n",
      "     48       \u001b[36m34.8706\u001b[0m       \u001b[32m26.7387\u001b[0m  0.0010  0.1680\n",
      "     20       \u001b[36m61.7202\u001b[0m       \u001b[32m51.5014\u001b[0m  0.0010  0.1759\n",
      "     48       35.4181       \u001b[32m25.1843\u001b[0m  0.0010  0.1691\n",
      "     49       37.9183       \u001b[32m26.1963\u001b[0m  0.0010  0.1495\n",
      "     50       34.1518       \u001b[32m33.1541\u001b[0m  0.0010  0.1670\n",
      "     49       35.1970       \u001b[32m26.0264\u001b[0m  0.0010  0.1689\n",
      "     21       \u001b[36m59.8024\u001b[0m       \u001b[32m50.2658\u001b[0m  0.0010  0.1402\n",
      "     49       \u001b[36m34.8803\u001b[0m       25.3466  0.0010  0.1873\n",
      "     50       38.4201       \u001b[32m25.9245\u001b[0m  0.0010  0.1718\n",
      "     51       33.9825       33.9766  0.0010  0.1771\n",
      "     50       \u001b[36m34.4931\u001b[0m       \u001b[32m25.5494\u001b[0m  0.0010  0.1729\n",
      "     22       \u001b[36m57.4037\u001b[0m       \u001b[32m49.1238\u001b[0m  0.0010  0.1782\n",
      "     50       35.2327       26.2001  0.0010  0.1491\n",
      "     51       37.6750       \u001b[32m25.4101\u001b[0m  0.0010  0.1657\n",
      "     52       34.4695       \u001b[32m32.6845\u001b[0m  0.0010  0.1811\n",
      "     23       58.5325       \u001b[32m47.2437\u001b[0m  0.0010  0.1856\n",
      "     51       \u001b[36m34.3456\u001b[0m       25.9778  0.0010  0.1984\n",
      "     51       \u001b[36m33.5593\u001b[0m       25.3768  0.0010  0.2159\n",
      "     52       36.7203       \u001b[32m25.2882\u001b[0m  0.0010  0.1964\n",
      "     53       \u001b[36m33.6317\u001b[0m       \u001b[32m32.0044\u001b[0m  0.0010  0.1923\n",
      "     24       \u001b[36m57.1277\u001b[0m       \u001b[32m45.7058\u001b[0m  0.0010  0.1876\n",
      "     52       34.5317       26.1434  0.0010  0.1932\n",
      "     53       36.5978       \u001b[32m24.9650\u001b[0m  0.0010  0.1924\n",
      "     52       33.6907       \u001b[32m24.8251\u001b[0m  0.0010  0.2178\n",
      "     54       \u001b[36m32.7793\u001b[0m       \u001b[32m31.1569\u001b[0m  0.0010  0.2080\n",
      "     25       \u001b[36m55.9067\u001b[0m       \u001b[32m45.6874\u001b[0m  0.0010  0.2062\n",
      "     53       \u001b[36m33.2574\u001b[0m       \u001b[32m24.7961\u001b[0m  0.0010  0.2189\n",
      "     54       \u001b[36m36.1971\u001b[0m       25.7108  0.0010  0.2055\n",
      "     53       \u001b[36m33.0250\u001b[0m       \u001b[32m24.4312\u001b[0m  0.0010  0.2121\n",
      "     55       32.9248       31.5366  0.0010  0.1804\n",
      "     26       \u001b[36m53.4849\u001b[0m       \u001b[32m43.6200\u001b[0m  0.0010  0.1822\n",
      "     54       33.9520       24.8510  0.0010  0.1797\n",
      "     55       \u001b[36m35.2174\u001b[0m       \u001b[32m23.6831\u001b[0m  0.0010  0.1640\n",
      "     54       \u001b[36m32.9283\u001b[0m       24.9660  0.0010  0.1647\n",
      "     56       \u001b[36m32.6876\u001b[0m       32.1867  0.0010  0.1440\n",
      "     27       \u001b[36m52.7678\u001b[0m       \u001b[32m42.1547\u001b[0m  0.0010  0.1384\n",
      "     55       33.3161       \u001b[32m24.4102\u001b[0m  0.0010  0.1678\n",
      "     56       \u001b[36m35.0748\u001b[0m       24.4186  0.0010  0.1628\n",
      "     55       33.3911       24.8553  0.0010  0.1529\n",
      "     57       \u001b[36m32.5366\u001b[0m       31.3785  0.0010  0.1610\n",
      "     28       \u001b[36m52.1020\u001b[0m       \u001b[32m41.5899\u001b[0m  0.0010  0.1626\n",
      "     56       33.5090       24.5112  0.0010  0.1571\n",
      "     57       35.5662       24.5130  0.0010  0.1649\n",
      "     56       \u001b[36m31.7828\u001b[0m       \u001b[32m24.2299\u001b[0m  0.0010  0.1784\n",
      "     58       32.8078       31.7400  0.0010  0.1603\n",
      "     29       \u001b[36m52.0323\u001b[0m       41.6036  0.0010  0.1627\n",
      "     57       \u001b[36m32.8830\u001b[0m       \u001b[32m24.3618\u001b[0m  0.0010  0.1666\n",
      "     30       \u001b[36m50.7531\u001b[0m       \u001b[32m39.3140\u001b[0m  0.0010  0.1405\n",
      "     58       36.0386       \u001b[32m23.4167\u001b[0m  0.0010  0.1834\n",
      "     59       \u001b[36m32.3122\u001b[0m       \u001b[32m30.9610\u001b[0m  0.0005  0.1693\n",
      "     57       32.7891       \u001b[32m23.8632\u001b[0m  0.0010  0.1749\n",
      "     58       32.8889       \u001b[32m23.9852\u001b[0m  0.0010  0.1641\n",
      "     31       \u001b[36m49.5911\u001b[0m       \u001b[32m37.8066\u001b[0m  0.0010  0.1525\n",
      "     60       \u001b[36m31.8525\u001b[0m       \u001b[32m30.7121\u001b[0m  0.0005  0.1599\n",
      "     59       35.5971       24.0132  0.0010  0.1655\n",
      "     58       31.9802       23.9150  0.0010  0.1726\n",
      "     59       \u001b[36m31.8959\u001b[0m       \u001b[32m23.8759\u001b[0m  0.0010  0.1891\n",
      "     32       \u001b[36m48.9894\u001b[0m       \u001b[32m37.6840\u001b[0m  0.0010  0.1591\n",
      "     60       35.7863       24.0199  0.0010  0.1574\n",
      "     61       \u001b[36m30.8625\u001b[0m       31.2557  0.0005  0.1786\n",
      "     59       \u001b[36m31.6936\u001b[0m       \u001b[32m23.5297\u001b[0m  0.0010  0.1664\n",
      "     60       32.9055       \u001b[32m23.5850\u001b[0m  0.0010  0.1679\n",
      "     60       \u001b[36m30.8990\u001b[0m       \u001b[32m23.2561\u001b[0m  0.0010  0.1171\n",
      "     33       \u001b[36m48.6973\u001b[0m       \u001b[32m36.0227\u001b[0m  0.0010  0.1730\n",
      "     61       \u001b[36m33.6917\u001b[0m       23.7675  0.0010  0.1531\n",
      "     62       \u001b[36m30.5577\u001b[0m       \u001b[32m30.1440\u001b[0m  0.0005  0.1384\n",
      "     61       \u001b[36m31.8048\u001b[0m       23.7149  0.0010  0.1583\n",
      "     34       \u001b[36m47.7461\u001b[0m       \u001b[32m35.8893\u001b[0m  0.0010  0.1505\n",
      "     63       32.4067       30.4413  0.0005  0.1529\n",
      "     62       34.4494       23.5374  0.0010  0.1714\n",
      "     61       31.8528       \u001b[32m22.4901\u001b[0m  0.0010  0.1837\n",
      "     62       \u001b[36m31.7593\u001b[0m       \u001b[32m22.8839\u001b[0m  0.0010  0.1763\n",
      "     35       \u001b[36m46.6099\u001b[0m       \u001b[32m34.3820\u001b[0m  0.0010  0.1466\n",
      "     64       31.2651       31.2057  0.0005  0.1586\n",
      "     62       31.3371       22.7095  0.0010  0.2023\n",
      "     63       32.4159       23.2062  0.0010  0.1791\n",
      "     36       \u001b[36m44.9633\u001b[0m       \u001b[32m32.5601\u001b[0m  0.0010  0.1774\n",
      "     65       31.0653       \u001b[32m29.7056\u001b[0m  0.0005  0.1619\n",
      "     63       31.4113       23.0115  0.0010  0.1875\n",
      "     66       31.0295       30.6155  0.0005  0.1632\n",
      "     64       32.2906       23.1567  0.0010  0.1966\n",
      "     37       \u001b[36m44.3658\u001b[0m       \u001b[32m32.2893\u001b[0m  0.0010  0.1962\n",
      "     64       \u001b[36m30.6068\u001b[0m       22.9724  0.0010  0.2112\n",
      "     67       31.3371       \u001b[32m29.6058\u001b[0m  0.0005  0.2091\n",
      "     65       \u001b[36m30.8474\u001b[0m       \u001b[32m22.4584\u001b[0m  0.0010  0.2165\n",
      "     38       \u001b[36m43.7135\u001b[0m       \u001b[32m31.5162\u001b[0m  0.0010  0.2069\n",
      "     65       30.7434       \u001b[32m22.3414\u001b[0m  0.0010  0.1864\n",
      "     68       30.6511       29.6648  0.0005  0.1836\n",
      "     39       \u001b[36m42.9798\u001b[0m       \u001b[32m31.1747\u001b[0m  0.0010  0.1727\n",
      "     66       \u001b[36m30.6905\u001b[0m       \u001b[32m21.7599\u001b[0m  0.0010  0.1887\n",
      "     66       31.4364       22.8888  0.0010  0.1619\n",
      "     69       31.2170       29.7846  0.0005  0.1695\n",
      "     40       \u001b[36m42.4887\u001b[0m       \u001b[32m30.5570\u001b[0m  0.0010  0.1691\n",
      "     67       30.7858       \u001b[32m21.5809\u001b[0m  0.0010  0.1789\n",
      "     67       \u001b[36m30.0531\u001b[0m       \u001b[32m22.1451\u001b[0m  0.0010  0.1722\n",
      "     70       30.7371       30.2870  0.0005  0.1520\n",
      "     41       \u001b[36m41.3031\u001b[0m       \u001b[32m30.0036\u001b[0m  0.0010  0.1517\n",
      "     68       31.1237       22.2629  0.0010  0.1431\n",
      "     68       \u001b[36m29.5752\u001b[0m       \u001b[32m21.2332\u001b[0m  0.0010  0.1681\n",
      "     71       \u001b[36m29.6403\u001b[0m       30.3253  0.0005  0.1572\n",
      "     69       31.7158       21.8337  0.0010  0.1467\n",
      "     42       \u001b[36m40.2558\u001b[0m       \u001b[32m29.9343\u001b[0m  0.0010  0.1546\n",
      "     69       30.9723       21.5928  0.0010  0.1512\n",
      "     43       \u001b[36m40.1795\u001b[0m       \u001b[32m28.3444\u001b[0m  0.0010  0.1406\n",
      "     72       \u001b[36m29.5842\u001b[0m       \u001b[32m29.0184\u001b[0m  0.0003  0.1631\n",
      "     70       \u001b[36m30.3392\u001b[0m       21.8160  0.0010  0.1572\n",
      "     70       \u001b[36m29.2868\u001b[0m       21.4696  0.0010  0.1522\n",
      "     71       \u001b[36m29.8246\u001b[0m       21.7083  0.0010  0.1633\n",
      "     44       40.4192       \u001b[32m27.6903\u001b[0m  0.0010  0.1889\n",
      "     73       31.2566       29.3112  0.0003  0.1839\n",
      "     71       \u001b[36m29.0909\u001b[0m       21.4249  0.0010  0.1609\n",
      "     45       \u001b[36m39.6578\u001b[0m       \u001b[32m27.5440\u001b[0m  0.0010  0.1599\n",
      "     72       30.6367       \u001b[32m21.4573\u001b[0m  0.0005  0.1775\n",
      "     74       \u001b[36m29.0979\u001b[0m       29.4621  0.0003  0.1673\n",
      "     72       29.6845       22.1719  0.0010  0.1959\n",
      "     46       39.6602       27.6463  0.0010  0.1644\n",
      "     75       30.2462       29.3752  0.0003  0.1551\n",
      "     73       \u001b[36m29.4156\u001b[0m       \u001b[32m21.3873\u001b[0m  0.0005  0.1733\n",
      "     73       \u001b[36m28.4860\u001b[0m       \u001b[32m20.9048\u001b[0m  0.0005  0.1661\n",
      "     47       39.9991       \u001b[32m26.9028\u001b[0m  0.0010  0.1409\n",
      "     76       30.7809       \u001b[32m28.9689\u001b[0m  0.0003  0.1596\n",
      "     74       \u001b[36m28.7736\u001b[0m       \u001b[32m21.3431\u001b[0m  0.0005  0.1494\n",
      "     74       29.0958       \u001b[32m20.7308\u001b[0m  0.0005  0.1717\n",
      "     77       \u001b[36m28.8744\u001b[0m       29.5707  0.0003  0.1509\n",
      "     48       \u001b[36m38.2353\u001b[0m       \u001b[32m26.3967\u001b[0m  0.0010  0.1830\n",
      "     75       \u001b[36m28.3389\u001b[0m       21.4039  0.0005  0.1703\n",
      "     75       28.7557       20.9286  0.0005  0.2351\n",
      "     49       \u001b[36m37.8932\u001b[0m       \u001b[32m26.1352\u001b[0m  0.0010  0.2403\n",
      "     78       30.8209       29.0286  0.0003  0.2710\n",
      "     76       29.1276       21.9970  0.0005  0.2623\n",
      "     76       \u001b[36m28.2486\u001b[0m       \u001b[32m20.5603\u001b[0m  0.0005  0.1383\n",
      "     50       \u001b[36m37.6585\u001b[0m       \u001b[32m25.4301\u001b[0m  0.0010  0.1599\n",
      "     79       29.7365       29.0376  0.0003  0.1587\n",
      "     77       29.7624       \u001b[32m21.2914\u001b[0m  0.0005  0.1649\n",
      "     77       \u001b[36m27.7545\u001b[0m       21.2577  0.0005  0.1789\n",
      "     51       \u001b[36m37.4068\u001b[0m       \u001b[32m25.2208\u001b[0m  0.0010  0.1573\n",
      "     80       29.7540       29.7037  0.0003  0.1409\n",
      "     78       28.5413       21.3433  0.0005  0.1583\n",
      "     78       28.7985       20.8079  0.0005  0.1600\n",
      "     52       \u001b[36m35.6420\u001b[0m       \u001b[32m25.0077\u001b[0m  0.0010  0.1486\n",
      "     79       29.0421       21.4580  0.0005  0.1395\n",
      "     79       28.6532       21.0819  0.0005  0.1258\n",
      "     53       37.8995       \u001b[32m24.9398\u001b[0m  0.0010  0.1281\n",
      "     80       30.5985       \u001b[32m21.2063\u001b[0m  0.0005  0.1256\n",
      "     80       28.2590       20.7638  0.0005  0.1336\n",
      "     54       \u001b[36m34.7197\u001b[0m       \u001b[32m24.9295\u001b[0m  0.0010  0.1601\n",
      "     81       \u001b[36m28.0088\u001b[0m       \u001b[32m20.9426\u001b[0m  0.0005  0.1727\n",
      "     55       36.0973       \u001b[32m24.4240\u001b[0m  0.0010  0.1786\n",
      "     82       28.9567       21.0544  0.0005  0.1616\n",
      "     56       36.5594       \u001b[32m24.4203\u001b[0m  0.0010  0.1345\n",
      "     83       29.1315       \u001b[32m20.6897\u001b[0m  0.0005  0.1278\n",
      "     57       35.7665       \u001b[32m23.4163\u001b[0m  0.0010  0.1137\n",
      "     84       28.5208       21.2089  0.0005  0.1217\n",
      "     58       36.5863       \u001b[32m23.1270\u001b[0m  0.0010  0.0967\n",
      "     85       28.6558       \u001b[32m20.4117\u001b[0m  0.0005  0.1233\n",
      "     59       35.0013       23.9977  0.0010  0.1130\n",
      "     86       28.0206       \u001b[32m20.2031\u001b[0m  0.0005  0.1202\n",
      "     60       \u001b[36m33.0165\u001b[0m       23.8481  0.0010  0.1147\n",
      "     87       28.3099       20.2469  0.0005  0.1144\n",
      "     61       33.9265       23.2533  0.0010  0.1026\n",
      "     88       29.9909       20.5133  0.0005  0.1021\n",
      "     62       34.3470       \u001b[32m22.7040\u001b[0m  0.0010  0.0929\n",
      "     89       28.6158       20.2212  0.0005  0.0871\n",
      "     63       33.7226       \u001b[32m21.9537\u001b[0m  0.0010  0.0918\n",
      "     90       29.3689       20.3506  0.0005  0.0947\n",
      "     64       \u001b[36m32.6209\u001b[0m       22.7850  0.0010  0.0921\n",
      "     65       33.0631       22.3069  0.0010  0.0796\n",
      "     66       32.9823       22.4891  0.0010  0.1119\n",
      "     67       32.8591       22.3308  0.0010  0.1104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m694.9308\u001b[0m      \u001b[32m444.4900\u001b[0m  0.0010  0.2270\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m683.9242\u001b[0m      \u001b[32m469.9486\u001b[0m  0.0010  0.1965\n",
      "      2      \u001b[36m362.9705\u001b[0m      \u001b[32m180.0991\u001b[0m  0.0010  0.2443\n",
      "      2      \u001b[36m391.0144\u001b[0m      \u001b[32m198.4933\u001b[0m  0.0010  0.2144\n",
      "      3      \u001b[36m181.8966\u001b[0m      \u001b[32m143.9808\u001b[0m  0.0010  0.2087\n",
      "      3      \u001b[36m188.7483\u001b[0m      \u001b[32m152.0237\u001b[0m  0.0010  0.2122\n",
      "      4      \u001b[36m162.4341\u001b[0m      \u001b[32m135.3354\u001b[0m  0.0010  0.2059\n",
      "      4      \u001b[36m160.1824\u001b[0m      \u001b[32m142.2912\u001b[0m  0.0010  0.2179\n",
      "      5      \u001b[36m155.8434\u001b[0m      \u001b[32m131.4308\u001b[0m  0.0010  0.2065\n",
      "      5      \u001b[36m151.6390\u001b[0m      \u001b[32m136.5566\u001b[0m  0.0010  0.2302\n",
      "      6      \u001b[36m150.1104\u001b[0m      \u001b[32m128.2111\u001b[0m  0.0010  0.1979\n",
      "      6      \u001b[36m144.7474\u001b[0m      \u001b[32m132.1397\u001b[0m  0.0010  0.2181\n",
      "      7      \u001b[36m146.6436\u001b[0m      \u001b[32m125.3755\u001b[0m  0.0010  0.2130\n",
      "      7      \u001b[36m140.2036\u001b[0m      \u001b[32m129.7116\u001b[0m  0.0010  0.1609\n",
      "      8      \u001b[36m142.3299\u001b[0m      \u001b[32m123.8134\u001b[0m  0.0010  0.1649\n",
      "      8      \u001b[36m135.9207\u001b[0m      \u001b[32m127.2033\u001b[0m  0.0010  0.1460\n",
      "      9      \u001b[36m137.5516\u001b[0m      \u001b[32m122.6759\u001b[0m  0.0010  0.1940\n",
      "      9      \u001b[36m130.1084\u001b[0m      \u001b[32m125.6570\u001b[0m  0.0010  0.2358\n",
      "     10      \u001b[36m136.0162\u001b[0m      \u001b[32m121.6479\u001b[0m  0.0010  0.2255\n",
      "     10      \u001b[36m128.7882\u001b[0m      \u001b[32m124.4349\u001b[0m  0.0010  0.2128\n",
      "     11      \u001b[36m133.5752\u001b[0m      \u001b[32m120.2656\u001b[0m  0.0010  0.1664\n",
      "     11      \u001b[36m125.5833\u001b[0m      \u001b[32m123.7562\u001b[0m  0.0010  0.1880\n",
      "     12      \u001b[36m129.8755\u001b[0m      \u001b[32m119.1768\u001b[0m  0.0010  0.2234\n",
      "     12      \u001b[36m122.1569\u001b[0m      \u001b[32m122.8067\u001b[0m  0.0010  0.2129\n",
      "     13      \u001b[36m128.1818\u001b[0m      \u001b[32m118.8399\u001b[0m  0.0010  0.1783\n",
      "     13      \u001b[36m119.4089\u001b[0m      \u001b[32m122.7590\u001b[0m  0.0010  0.2119\n",
      "     14      \u001b[36m126.4939\u001b[0m      \u001b[32m117.1308\u001b[0m  0.0010  0.2242\n",
      "     14      \u001b[36m117.7629\u001b[0m      \u001b[32m121.8082\u001b[0m  0.0010  0.2055\n",
      "     15      \u001b[36m125.2339\u001b[0m      117.2017  0.0010  0.1764\n",
      "     15      \u001b[36m116.3110\u001b[0m      \u001b[32m121.1132\u001b[0m  0.0010  0.1969\n",
      "     16      \u001b[36m123.6691\u001b[0m      \u001b[32m116.8047\u001b[0m  0.0010  0.1805\n",
      "     16      \u001b[36m115.1048\u001b[0m      121.1974  0.0010  0.1763\n",
      "     17      \u001b[36m121.7602\u001b[0m      116.9112  0.0010  0.1733\n",
      "     17      \u001b[36m113.4576\u001b[0m      \u001b[32m120.4847\u001b[0m  0.0010  0.1697\n",
      "     18      \u001b[36m119.9403\u001b[0m      \u001b[32m116.1679\u001b[0m  0.0010  0.1600\n",
      "     18      114.3486      \u001b[32m119.8502\u001b[0m  0.0010  0.1976\n",
      "     19      121.6108      \u001b[32m115.5880\u001b[0m  0.0010  0.2143\n",
      "     19      \u001b[36m111.3509\u001b[0m      \u001b[32m119.3856\u001b[0m  0.0010  0.2180\n",
      "     20      \u001b[36m118.6955\u001b[0m      116.4137  0.0010  0.1901\n",
      "     20      111.9689      \u001b[32m118.9284\u001b[0m  0.0010  0.2309\n",
      "     21      \u001b[36m116.9731\u001b[0m      \u001b[32m115.5829\u001b[0m  0.0010  0.2182\n",
      "     21      111.6958      \u001b[32m118.3888\u001b[0m  0.0010  0.2393\n",
      "     22      117.2807      116.1644  0.0010  0.2519\n",
      "     22      111.4890      \u001b[32m118.1043\u001b[0m  0.0010  0.2298\n",
      "     23      \u001b[36m115.9882\u001b[0m      \u001b[32m115.3827\u001b[0m  0.0010  0.2254\n",
      "     23      \u001b[36m109.2850\u001b[0m      \u001b[32m117.9473\u001b[0m  0.0010  0.2206\n",
      "     24      \u001b[36m115.9707\u001b[0m      \u001b[32m115.3630\u001b[0m  0.0010  0.2206\n",
      "     24      \u001b[36m109.1522\u001b[0m      \u001b[32m117.8095\u001b[0m  0.0010  0.2461\n",
      "     25      \u001b[36m113.3412\u001b[0m      \u001b[32m115.2763\u001b[0m  0.0010  0.2615\n",
      "     25      \u001b[36m108.4439\u001b[0m      117.8270  0.0010  0.1992\n",
      "     26      115.7074      \u001b[32m115.0661\u001b[0m  0.0010  0.1984\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m633.7286\u001b[0m      \u001b[32m447.8440\u001b[0m  0.0010  0.2035\n",
      "     26      \u001b[36m107.3040\u001b[0m      \u001b[32m117.6777\u001b[0m  0.0010  0.2347\n",
      "     27      \u001b[36m113.0054\u001b[0m      \u001b[32m114.5200\u001b[0m  0.0010  0.2082\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m542.2255\u001b[0m      \u001b[32m451.4002\u001b[0m  0.0010  0.2127\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m555.2214\u001b[0m      \u001b[32m783.7911\u001b[0m  0.0010  0.2156\n",
      "      2      \u001b[36m317.4122\u001b[0m      \u001b[32m181.8135\u001b[0m  0.0010  0.1959\n",
      "     27      \u001b[36m106.6271\u001b[0m      \u001b[32m117.1675\u001b[0m  0.0010  0.2241\n",
      "     28      116.3604      114.9952  0.0010  0.2079\n",
      "      2      \u001b[36m275.3840\u001b[0m      \u001b[32m185.7874\u001b[0m  0.0010  0.2052\n",
      "      2      \u001b[36m290.0066\u001b[0m      \u001b[32m282.7404\u001b[0m  0.0010  0.2067\n",
      "      3      \u001b[36m162.0359\u001b[0m      \u001b[32m146.1982\u001b[0m  0.0010  0.1970\n",
      "     28      \u001b[36m106.0501\u001b[0m      118.5836  0.0010  0.1978\n",
      "     29      \u001b[36m111.7353\u001b[0m      114.5681  0.0010  0.2076\n",
      "      3      \u001b[36m149.0741\u001b[0m      \u001b[32m150.6941\u001b[0m  0.0010  0.1915\n",
      "      3      \u001b[36m153.5793\u001b[0m      \u001b[32m212.6316\u001b[0m  0.0010  0.1861\n",
      "      4      \u001b[36m142.1543\u001b[0m      \u001b[32m137.1457\u001b[0m  0.0010  0.1968\n",
      "     29      \u001b[36m106.0447\u001b[0m      117.3388  0.0010  0.1979\n",
      "     30      \u001b[36m110.6204\u001b[0m      \u001b[32m114.0137\u001b[0m  0.0010  0.1977\n",
      "      4      \u001b[36m137.2072\u001b[0m      \u001b[32m140.2229\u001b[0m  0.0010  0.1810\n",
      "      4      \u001b[36m139.9788\u001b[0m      \u001b[32m196.8639\u001b[0m  0.0010  0.2075\n",
      "      5      \u001b[36m138.4828\u001b[0m      \u001b[32m132.7454\u001b[0m  0.0010  0.1994\n",
      "     30      \u001b[36m104.9548\u001b[0m      117.2241  0.0010  0.2301\n",
      "      5      \u001b[36m129.9232\u001b[0m      \u001b[32m133.9078\u001b[0m  0.0010  0.2212\n",
      "     31      112.0470      \u001b[32m113.6147\u001b[0m  0.0010  0.2454\n",
      "      5      \u001b[36m135.3812\u001b[0m      \u001b[32m188.5743\u001b[0m  0.0010  0.2143\n",
      "      6      \u001b[36m130.2026\u001b[0m      \u001b[32m129.1400\u001b[0m  0.0010  0.2075\n",
      "     31      \u001b[36m103.7144\u001b[0m      \u001b[32m117.0951\u001b[0m  0.0010  0.2026\n",
      "      6      \u001b[36m126.6112\u001b[0m      \u001b[32m130.8426\u001b[0m  0.0010  0.1795\n",
      "     32      111.9815      114.1174  0.0010  0.2020\n",
      "      6      \u001b[36m130.7113\u001b[0m      \u001b[32m180.2173\u001b[0m  0.0010  0.1793\n",
      "      7      \u001b[36m127.6637\u001b[0m      \u001b[32m127.5981\u001b[0m  0.0010  0.1915\n",
      "      7      \u001b[36m122.6907\u001b[0m      \u001b[32m127.6136\u001b[0m  0.0010  0.1875\n",
      "     32      104.8622      \u001b[32m116.6520\u001b[0m  0.0010  0.1992\n",
      "      7      \u001b[36m125.0621\u001b[0m      \u001b[32m171.6245\u001b[0m  0.0010  0.1780\n",
      "     33      112.5480      113.7892  0.0010  0.1928\n",
      "      8      \u001b[36m122.9063\u001b[0m      \u001b[32m125.8464\u001b[0m  0.0010  0.2113\n",
      "      8      \u001b[36m120.4460\u001b[0m      \u001b[32m125.4836\u001b[0m  0.0010  0.1942\n",
      "     33      104.5370      \u001b[32m116.1772\u001b[0m  0.0010  0.2166\n",
      "      8      \u001b[36m121.3678\u001b[0m      \u001b[32m165.2531\u001b[0m  0.0010  0.2073\n",
      "     34      112.2589      \u001b[32m112.9815\u001b[0m  0.0010  0.2296\n",
      "      9      \u001b[36m119.4130\u001b[0m      \u001b[32m125.7766\u001b[0m  0.0010  0.2986\n",
      "      9      \u001b[36m115.8994\u001b[0m      \u001b[32m123.9106\u001b[0m  0.0010  0.2954\n",
      "      9      \u001b[36m118.9361\u001b[0m      \u001b[32m159.4529\u001b[0m  0.0010  0.2889\n",
      "     34      105.7058      116.5201  0.0010  0.3065\n",
      "     35      \u001b[36m110.4365\u001b[0m      113.2823  0.0010  0.2824\n",
      "     10      \u001b[36m116.8473\u001b[0m      \u001b[32m155.4851\u001b[0m  0.0010  0.1406\n",
      "     10      \u001b[36m114.4129\u001b[0m      \u001b[32m121.6749\u001b[0m  0.0010  0.2002\n",
      "     10      \u001b[36m116.3350\u001b[0m      \u001b[32m125.1787\u001b[0m  0.0010  0.2432\n",
      "     36      \u001b[36m110.0375\u001b[0m      113.2964  0.0010  0.1731\n",
      "     35      104.1019      \u001b[32m116.0369\u001b[0m  0.0010  0.2069\n",
      "     11      \u001b[36m114.6052\u001b[0m      \u001b[32m152.3143\u001b[0m  0.0010  0.1811\n",
      "     11      \u001b[36m114.7942\u001b[0m      \u001b[32m123.9721\u001b[0m  0.0010  0.1555\n",
      "     11      \u001b[36m112.3587\u001b[0m      \u001b[32m120.8694\u001b[0m  0.0010  0.1943\n",
      "     36      104.6028      116.1934  0.0010  0.1575\n",
      "     37      \u001b[36m109.7190\u001b[0m      113.1062  0.0010  0.1765\n",
      "     12      \u001b[36m111.3540\u001b[0m      124.0519  0.0010  0.1584\n",
      "     12      \u001b[36m113.1072\u001b[0m      \u001b[32m150.7111\u001b[0m  0.0010  0.1699\n",
      "     12      \u001b[36m110.1781\u001b[0m      \u001b[32m119.6250\u001b[0m  0.0010  0.1629\n",
      "     38      110.0053      \u001b[32m112.6287\u001b[0m  0.0010  0.1637\n",
      "     37      \u001b[36m103.6413\u001b[0m      \u001b[32m115.7574\u001b[0m  0.0010  0.1765\n",
      "     13      111.3991      124.3555  0.0010  0.1664\n",
      "     13      \u001b[36m107.7394\u001b[0m      \u001b[32m118.8554\u001b[0m  0.0010  0.1869\n",
      "     13      \u001b[36m111.5812\u001b[0m      \u001b[32m148.0176\u001b[0m  0.0010  0.2153\n",
      "     38      \u001b[36m103.0347\u001b[0m      116.0291  0.0010  0.1702\n",
      "     39      \u001b[36m108.5711\u001b[0m      112.8636  0.0010  0.1817\n",
      "     14      \u001b[36m109.3365\u001b[0m      124.5871  0.0010  0.1550\n",
      "     14      \u001b[36m107.2634\u001b[0m      \u001b[32m118.4258\u001b[0m  0.0010  0.1703\n",
      "     14      \u001b[36m108.5015\u001b[0m      \u001b[32m145.6542\u001b[0m  0.0010  0.1868\n",
      "     39      \u001b[36m101.4941\u001b[0m      \u001b[32m114.9153\u001b[0m  0.0010  0.1994\n",
      "     40      109.0224      112.9522  0.0010  0.2104\n",
      "     15      \u001b[36m107.7960\u001b[0m      \u001b[32m123.7485\u001b[0m  0.0010  0.1897\n",
      "     15      \u001b[36m106.1711\u001b[0m      \u001b[32m118.0052\u001b[0m  0.0010  0.1996\n",
      "     15      \u001b[36m107.6004\u001b[0m      145.7805  0.0010  0.1986\n",
      "     40      \u001b[36m101.3835\u001b[0m      115.8057  0.0010  0.1993\n",
      "     41      109.8599      \u001b[32m112.6047\u001b[0m  0.0010  0.2272\n",
      "     16      \u001b[36m106.8380\u001b[0m      \u001b[32m123.0621\u001b[0m  0.0010  0.2348\n",
      "     16      \u001b[36m105.9045\u001b[0m      \u001b[32m116.9624\u001b[0m  0.0010  0.2372\n",
      "     16      \u001b[36m107.3488\u001b[0m      \u001b[32m144.4442\u001b[0m  0.0010  0.2419\n",
      "     41      102.0347      \u001b[32m114.7382\u001b[0m  0.0010  0.2608\n",
      "     42      109.5700      \u001b[32m111.4692\u001b[0m  0.0010  0.2216\n",
      "     17      \u001b[36m106.4182\u001b[0m      123.2557  0.0010  0.2240\n",
      "     17      \u001b[36m102.3818\u001b[0m      \u001b[32m116.9010\u001b[0m  0.0010  0.1966\n",
      "     17      \u001b[36m106.6078\u001b[0m      \u001b[32m141.8553\u001b[0m  0.0010  0.2210\n",
      "     42      \u001b[36m100.3340\u001b[0m      114.9913  0.0010  0.2147\n",
      "     43      \u001b[36m107.2315\u001b[0m      111.8909  0.0010  0.2123\n",
      "     18      \u001b[36m104.4892\u001b[0m      \u001b[32m122.2145\u001b[0m  0.0010  0.2096\n",
      "     18      103.1123      \u001b[32m116.1600\u001b[0m  0.0010  0.2010\n",
      "     18      \u001b[36m105.6077\u001b[0m      141.8897  0.0010  0.2312\n",
      "     43      101.2581      115.2325  0.0010  0.2067\n",
      "     44      \u001b[36m105.6110\u001b[0m      112.5897  0.0010  0.2189\n",
      "     19      105.3553      122.4321  0.0010  0.2115\n",
      "     19      \u001b[36m102.3288\u001b[0m      \u001b[32m116.0169\u001b[0m  0.0010  0.2050\n",
      "     19      \u001b[36m103.6588\u001b[0m      \u001b[32m141.2248\u001b[0m  0.0010  0.2150\n",
      "     44      100.5566      115.5276  0.0010  0.2153\n",
      "     45      107.3168      111.8271  0.0010  0.2026\n",
      "     20      \u001b[36m104.4309\u001b[0m      122.3397  0.0010  0.2205\n",
      "     20      \u001b[36m101.6239\u001b[0m      116.4199  0.0010  0.2215\n",
      "     45      101.1587      116.3669  0.0010  0.1902\n",
      "     46      107.4398      111.6184  0.0010  0.1937\n",
      "     20      \u001b[36m102.5953\u001b[0m      \u001b[32m139.0610\u001b[0m  0.0010  0.2193\n",
      "     21      \u001b[36m102.4062\u001b[0m      \u001b[32m121.3375\u001b[0m  0.0010  0.2218\n",
      "     21      101.7738      \u001b[32m115.7718\u001b[0m  0.0010  0.2139\n",
      "     47      \u001b[36m104.6442\u001b[0m      \u001b[32m111.3613\u001b[0m  0.0005  0.1986\n",
      "     21      103.3278      \u001b[32m138.1399\u001b[0m  0.0010  0.2230\n",
      "     22      103.0087      122.4077  0.0010  0.2233\n",
      "     22      \u001b[36m101.3983\u001b[0m      \u001b[32m115.2316\u001b[0m  0.0010  0.2174\n",
      "     48      105.1382      \u001b[32m110.9459\u001b[0m  0.0005  0.2164\n",
      "     22      102.5970      138.8668  0.0010  0.2317\n",
      "     23       \u001b[36m99.3801\u001b[0m      115.3215  0.0010  0.1765\n",
      "     23      \u001b[36m100.9518\u001b[0m      \u001b[32m121.0853\u001b[0m  0.0010  0.2132\n",
      "     49      104.6999      111.0672  0.0005  0.2064\n",
      "     23      \u001b[36m100.5668\u001b[0m      \u001b[32m136.3886\u001b[0m  0.0010  0.1823\n",
      "     24       \u001b[36m98.6506\u001b[0m      115.3492  0.0010  0.2071\n",
      "     24       \u001b[36m99.7313\u001b[0m      121.4624  0.0010  0.2277\n",
      "     50      104.8330      111.1561  0.0005  0.2188\n",
      "     24      \u001b[36m100.3215\u001b[0m      136.8871  0.0010  0.2043\n",
      "     25       98.8231      \u001b[32m115.0885\u001b[0m  0.0010  0.1753\n",
      "     25      101.7229      121.4502  0.0010  0.1724\n",
      "     51      \u001b[36m103.3614\u001b[0m      \u001b[32m110.6703\u001b[0m  0.0005  0.2069\n",
      "     25      101.0224      \u001b[32m135.9772\u001b[0m  0.0010  0.1934\n",
      "     26       \u001b[36m97.2149\u001b[0m      \u001b[32m114.8234\u001b[0m  0.0010  0.2008\n",
      "     26       \u001b[36m98.9597\u001b[0m      121.7594  0.0010  0.1798\n",
      "     26       \u001b[36m98.9511\u001b[0m      \u001b[32m135.5949\u001b[0m  0.0010  0.1764\n",
      "     52      105.4310      110.9607  0.0005  0.2012\n",
      "     27       99.8473      121.6448  0.0010  0.1501\n",
      "     27       \u001b[36m96.8861\u001b[0m      115.4782  0.0010  0.1823\n",
      "     27       99.2086      \u001b[32m134.4696\u001b[0m  0.0010  0.1463\n",
      "     53      \u001b[36m103.0184\u001b[0m      111.0101  0.0005  0.1446\n",
      "     28       99.2046      \u001b[32m120.2537\u001b[0m  0.0005  0.1568\n",
      "     28       97.1251      115.4343  0.0010  0.1397\n",
      "     28       99.8331      135.3412  0.0010  0.1751\n",
      "     54      104.2899      \u001b[32m110.6024\u001b[0m  0.0005  0.1598\n",
      "     29       \u001b[36m97.2070\u001b[0m      \u001b[32m119.8359\u001b[0m  0.0005  0.1619\n",
      "     29       \u001b[36m96.6032\u001b[0m      \u001b[32m114.2896\u001b[0m  0.0010  0.1602\n",
      "     29       \u001b[36m97.9611\u001b[0m      135.9852  0.0010  0.1755\n",
      "     55      \u001b[36m102.1730\u001b[0m      110.7980  0.0005  0.1756\n",
      "     30       97.6040      114.9318  0.0010  0.1639\n",
      "     30       97.7786      120.2950  0.0005  0.1750\n",
      "     56      104.3743      \u001b[32m110.5267\u001b[0m  0.0005  0.1622\n",
      "     30       \u001b[36m97.8936\u001b[0m      135.6204  0.0010  0.1806\n",
      "     31       96.6172      \u001b[32m114.2738\u001b[0m  0.0010  0.2076\n",
      "     31       \u001b[36m96.7139\u001b[0m      \u001b[32m119.3959\u001b[0m  0.0005  0.2141\n",
      "     31       98.3839      134.8243  0.0010  0.1803\n",
      "     57      \u001b[36m101.9070\u001b[0m      \u001b[32m110.4643\u001b[0m  0.0005  0.2187\n",
      "     32       \u001b[36m96.2153\u001b[0m      114.4976  0.0010  0.1595\n",
      "     32       \u001b[36m96.5927\u001b[0m      119.4888  0.0005  0.1691\n",
      "     58      102.6285      110.5533  0.0005  0.1655\n",
      "     32       \u001b[36m95.3336\u001b[0m      \u001b[32m129.6188\u001b[0m  0.0005  0.1925\n",
      "     33       \u001b[36m95.2829\u001b[0m      114.4345  0.0010  0.1600\n",
      "     33       97.3701      119.8548  0.0005  0.1597\n",
      "     59      104.7150      110.6192  0.0005  0.1692\n",
      "     34       \u001b[36m96.3193\u001b[0m      119.9303  0.0005  0.1304\n",
      "     33       \u001b[36m94.5265\u001b[0m      129.8883  0.0005  0.1730\n",
      "     34       95.4037      114.4335  0.0010  0.1698\n",
      "     35       \u001b[36m95.0667\u001b[0m      \u001b[32m114.1391\u001b[0m  0.0010  0.1362\n",
      "     35       \u001b[36m95.8258\u001b[0m      \u001b[32m119.3190\u001b[0m  0.0005  0.1640\n",
      "     60      103.2675      \u001b[32m110.2253\u001b[0m  0.0005  0.1669\n",
      "     34       96.0443      \u001b[32m128.5142\u001b[0m  0.0005  0.1657\n",
      "     36       \u001b[36m94.5922\u001b[0m      114.1493  0.0010  0.1528\n",
      "     35       95.4196      \u001b[32m128.1297\u001b[0m  0.0005  0.1385\n",
      "     36       96.5931      \u001b[32m119.1248\u001b[0m  0.0005  0.1550\n",
      "     61      \u001b[36m101.7525\u001b[0m      \u001b[32m110.2124\u001b[0m  0.0005  0.1661\n",
      "     37       \u001b[36m93.8620\u001b[0m      \u001b[32m113.5472\u001b[0m  0.0010  0.1560\n",
      "     36       95.6783      128.6695  0.0005  0.1603\n",
      "     37       \u001b[36m95.6652\u001b[0m      119.2987  0.0005  0.1774\n",
      "     62      101.9625      \u001b[32m110.0267\u001b[0m  0.0005  0.1854\n",
      "     38       94.1735      \u001b[32m112.8873\u001b[0m  0.0010  0.1573\n",
      "     37       95.2260      129.1439  0.0005  0.1582\n",
      "     38       96.7338      \u001b[32m118.9949\u001b[0m  0.0005  0.1495\n",
      "     63      \u001b[36m101.5691\u001b[0m      110.3516  0.0005  0.1335\n",
      "     39       \u001b[36m93.1993\u001b[0m      113.0681  0.0010  0.1329\n",
      "     38       \u001b[36m93.6551\u001b[0m      128.2609  0.0005  0.1517\n",
      "     64      101.7670      \u001b[32m109.9309\u001b[0m  0.0005  0.1408\n",
      "     39       \u001b[36m95.1862\u001b[0m      119.2649  0.0005  0.1738\n",
      "     40       \u001b[36m92.0464\u001b[0m      113.1870  0.0010  0.1628\n",
      "     65      102.5106      110.0222  0.0005  0.1599\n",
      "     39       94.7275      129.1652  0.0005  0.1831\n",
      "     40       96.4735      \u001b[32m118.6418\u001b[0m  0.0005  0.1562\n",
      "     41       \u001b[36m91.6673\u001b[0m      112.9392  0.0010  0.1675\n",
      "     66      102.1259      109.9499  0.0005  0.1794\n",
      "     41       96.0015      118.9683  0.0005  0.1578\n",
      "     40       \u001b[36m92.1644\u001b[0m      \u001b[32m126.5097\u001b[0m  0.0003  0.1874\n",
      "     42       92.3221      \u001b[32m112.6688\u001b[0m  0.0010  0.1735\n",
      "     67      101.9999      \u001b[32m109.7648\u001b[0m  0.0005  0.1878\n",
      "     42       95.3123      118.8318  0.0005  0.1875\n",
      "     41       92.8317      \u001b[32m126.4883\u001b[0m  0.0003  0.1784\n",
      "     43       91.8337      113.0304  0.0010  0.1717\n",
      "     68      101.8471      \u001b[32m109.4518\u001b[0m  0.0005  0.1531\n",
      "     43       \u001b[36m94.8624\u001b[0m      \u001b[32m118.4383\u001b[0m  0.0005  0.1766\n",
      "     42       93.1617      126.7976  0.0003  0.1843\n",
      "     44       \u001b[36m90.9121\u001b[0m      112.7828  0.0010  0.1659\n",
      "     69      102.2975      \u001b[32m109.1432\u001b[0m  0.0005  0.1719\n",
      "     44       \u001b[36m94.6194\u001b[0m      118.7337  0.0005  0.1527\n",
      "     43       93.0369      \u001b[32m126.3320\u001b[0m  0.0003  0.1708\n",
      "     45       91.3268      \u001b[32m112.5090\u001b[0m  0.0010  0.1661\n",
      "     45       94.8834      119.0490  0.0005  0.1669\n",
      "     70      102.5169      \u001b[32m109.0405\u001b[0m  0.0005  0.1865\n",
      "     44       93.1351      126.4725  0.0003  0.1705\n",
      "     46       92.1055      113.4297  0.0010  0.1768\n",
      "     46       \u001b[36m93.8726\u001b[0m      119.0818  0.0005  0.1468\n",
      "     71      102.5635      \u001b[32m108.8354\u001b[0m  0.0005  0.1928\n",
      "     45       92.9968      126.3494  0.0003  0.1733\n",
      "     47       91.0050      112.7364  0.0010  0.1749\n",
      "     47       94.8321      119.0976  0.0005  0.1770\n",
      "     46       93.4721      126.4901  0.0003  0.1670\n",
      "     72      102.5671      109.1024  0.0005  0.1894\n",
      "     48       91.4590      113.0613  0.0010  0.1614\n",
      "     48       93.9562      \u001b[32m118.3950\u001b[0m  0.0003  0.1578\n",
      "     47       92.9697      \u001b[32m125.7205\u001b[0m  0.0003  0.1657\n",
      "     73      \u001b[36m101.1320\u001b[0m      108.8817  0.0005  0.1670\n",
      "     49       \u001b[36m89.5907\u001b[0m      113.0471  0.0010  0.1678\n",
      "     49       \u001b[36m93.5934\u001b[0m      \u001b[32m118.2862\u001b[0m  0.0003  0.1701\n",
      "     48       93.0276      \u001b[32m125.6119\u001b[0m  0.0003  0.1671\n",
      "     74      102.6565      \u001b[32m108.5576\u001b[0m  0.0005  0.1823\n",
      "     50       \u001b[36m92.4534\u001b[0m      118.4422  0.0003  0.1439\n",
      "     49       93.2571      125.6277  0.0003  0.1603\n",
      "     75      \u001b[36m100.2697\u001b[0m      \u001b[32m108.4480\u001b[0m  0.0005  0.1847\n",
      "     51       93.7873      \u001b[32m118.0965\u001b[0m  0.0003  0.1505\n",
      "     50       92.4021      126.2684  0.0003  0.1368\n",
      "     76      102.4552      108.6119  0.0005  0.1566\n",
      "     52       92.8440      118.2179  0.0003  0.1570\n",
      "     51       93.2144      126.6361  0.0003  0.1448\n",
      "     77      100.6184      108.8455  0.0005  0.1566\n",
      "     53       \u001b[36m92.4401\u001b[0m      118.6602  0.0003  0.1515\n",
      "     52       92.6876      126.5327  0.0003  0.1441\n",
      "     78      101.6336      \u001b[32m108.3157\u001b[0m  0.0005  0.1445\n",
      "     54       92.8027      118.5124  0.0003  0.1692\n",
      "     79      102.0439      \u001b[32m107.9819\u001b[0m  0.0005  0.2559\n",
      "     55       92.6951      118.4028  0.0003  0.2360\n",
      "     80      100.6565      108.3275  0.0005  0.1229\n",
      "     81      101.6201      108.4170  0.0005  0.1255\n",
      "     82      102.4932      108.5095  0.0005  0.1075\n",
      "     83      101.3521      108.4879  0.0005  0.1247\n"
     ]
    }
   ],
   "source": [
    "# Prediction of each model for the guided and free dataset \n",
    "\n",
    "... #Other models \n",
    "\n",
    "y_g_predict_cnn = cross_val_predict(pipe_cnn,x, y, groups=groups, cv=logo, n_jobs=-1)\n",
    "y_f_predict_cnn = cross_val_predict(pipe_cnn,x_f, y_f, groups=groups_f, cv=logo, n_jobs=-1)\n",
    "\n",
    "y_g_predict_nn_covariance = cross_val_predict(pipe_fusion, x_cov_nn, y, groups=groups, cv=logo, n_jobs=-1)\n",
    "y_f_predict_nn_covariance = cross_val_predict(pipe_fusion, x_cov_nn_f, y_f, groups=groups_f, cv=logo, n_jobs=-1)\n",
    "\n",
    "\n",
    "# Rajouter les prédictions de vos modéles \n",
    "y_g_predict_ensemble = (y_g_predict_cnn + y_g_predict_nn_covariance) / 2 \n",
    "y_f_predict_ensemble = (y_f_predict_cnn + y_f_predict_nn_covariance) / 2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "663b6d7d-2915-465a-bbb8-220eac33dfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.605793431091196 5.153238977914858 11.808756563134953 10.61510051232294\n",
      "0.04567055 0.057172593 0.2410565 0.19478644\n",
      "RMSE scores of the average method guided: 4.479452907671961 and free : 10.578227055337646\n",
      "NMSE scores of the average method guided: 0.043199357 and free : 0.19343555\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(y,y_predict):\n",
    "    mse  = mean_squared_error(y, y_predict)\n",
    "    rmse = np.sqrt(mse)\n",
    "    nmse = mse / np.var(y)  \n",
    "    return rmse, nmse \n",
    "\n",
    "\n",
    "# Metrics Guided\n",
    "rmse_g_cnn,   nmse_g_cnn   = calculate_metrics(y, y_g_predict_cnn)\n",
    "rmse_g_nn,    nmse_g_nn    = calculate_metrics(y, y_g_predict_nn_covariance)\n",
    "rmse_g_ens,   nmse_g_ens   = calculate_metrics(y, y_g_predict_ensemble)\n",
    "\n",
    "\n",
    "# RMSE Free\n",
    "rmse_f_cnn,   nmse_f_cnn   = calculate_metrics(y_f, y_f_predict_cnn)\n",
    "rmse_f_nn,    nmse_f_nn    = calculate_metrics(y_f, y_f_predict_nn_covariance)\n",
    "rmse_f_ens,   nmse_f_ens   = calculate_metrics(y_f, y_f_predict_ensemble)\n",
    "\n",
    "\n",
    "print(rmse_g_cnn,rmse_g_nn,rmse_f_cnn, rmse_f_nn)\n",
    "print(nmse_g_cnn,nmse_g_nn,nmse_f_cnn, nmse_f_nn)\n",
    "\n",
    "print(\"RMSE scores of the average method guided:\", rmse_g_ens, \"and free :\",rmse_f_ens)\n",
    "print(\"NMSE scores of the average method guided:\", nmse_g_ens, \"and free :\",nmse_f_ens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcfe570-c122-49e5-97cd-df0cc4f7733f",
   "metadata": {},
   "source": [
    "At this point we have our ensemble predictions (y_ensemble) and all the metrics we need for the average method. The power of this approach depends not only on each models individual performance but also on the diversity of their errors. Because we weight every model equally, a very weak model can drag the average down.\n",
    "\n",
    "However, when all the models are accurate enough and make different kinds of mistakes, averaging becomes extremely effective. For example, our CNN excels at learning local, spatial patterns in the raw filtered signal, while our nn + covariance network captures global statistical dependencies over time. Their errors rarely coincide, so when the CNN prediction is off for a given sample, the nn model often compensates it, and vice-versa. By averaging their outputs, we reduce the total variance and achieve a lower rmse/nmse than either model on its own, all without any extra hyperparameter tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db5d2bb-967f-4213-b2f5-84c46be9f023",
   "metadata": {},
   "source": [
    "For the second method, we concatenate the predictions of our models into a new dataset. This dataset will have the form (n_windows, targets × M), with M being the number of different models. After that, we train our model on this data and calculate the RMSE compared to the true y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "45e80d0e-410f-49a8-8f90-6c86d5af21fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# Creation of the new dataset with the prediction of our models \n",
    "x_g_meta = np.concatenate([y_g_predict_cnn, y_g_predict_nn_covariance], axis=1)\n",
    "x_f_meta = np.concatenate([y_f_predict_cnn, y_f_predict_nn_covariance], axis=1) \n",
    "\n",
    "# model \n",
    "lasso_model = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('lasso',  Lasso(alpha=0.1, max_iter=20000))\n",
    "    ])\n",
    "\n",
    "# Calculate the prediction\n",
    "y_g_predict_stack = cross_val_predict(lasso_model,x_g_meta,y,groups=groups,cv=logo,n_jobs=-1)\n",
    "y_f_predict_stack = cross_val_predict(lasso_model,x_f_meta,y_f,groups=groups_f,cv=logo,n_jobs=-1)\n",
    "\n",
    "\n",
    "#Calculate the metrics \n",
    "rmse_g_stack, nmse_g_stack = calculate_metrics(y, y_g_predict_stack)\n",
    "rmse_f_stack, nmse_f_stack = calculate_metrics(y_f, y_f_predict_stack)\n",
    "\n",
    "print(\"Stacking Guided RMSE:\",rmse_g_stack, \"NMSE:\",nmse_g_stack)\n",
    "print(\"Stacking Free RMSE:\",rmse_f_stack,\" NMSE:\", nmse_f_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a8b16d-4605-49a5-b1f5-de60823fa035",
   "metadata": {},
   "source": [
    "| Model / Ensembling       | RMSE Guided       | NMSE Guided       | RMSE Free         | NMSE Free         |\n",
    "| ---------------------- | ----------------- | ----------------- | ----------------- | ----------------- |\n",
    "| ...    |     |     |     |     |\n",
    "| Covariance matrices + Neural Network   | 5.19      | 0.044      | 10.70      | 0.24      |\n",
    "| CNN | 4.53   |  0.05  | 11.82   | 0.19   |\n",
    "| Averaging   | 4.46 | 0.042 | 10.67 | 0.196 |\n",
    "| Stacking    | 4.40    | 0.040    | 11.55    | 0.23    |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcfa3eb-c771-4065-b45a-7d3da757802d",
   "metadata": {},
   "source": [
    "# à completer \n",
    "We can see a small upgrade ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7e4785-3c25-4626-8b9b-d8e24a39591f",
   "metadata": {},
   "source": [
    "While the averaging method assigns equal weight to each base learner, stacking learns an optimal combination of their predictions and assigns weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d014d8bd-e2f8-4633-b2b1-650516a719bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with all the dataset \n",
    "lasso_model.fit(x_g_meta,y)\n",
    "\n",
    "# Have the coefficient for each output compare to each input\n",
    "# the matrix will have the shape (51, 51 * m)\n",
    "coeff_matrice = lasso_model.named_steps['lasso'].coef_\n",
    "\n",
    "def model_contributions(coeff_matrice,model_names):\n",
    "    m = len(model_names) \n",
    "    n_outputs, n_features = coeff_matrice.shape\n",
    "    per_model = n_features // m\n",
    "    \n",
    "    coef_abs = np.abs(coeff_matrice)\n",
    "    \n",
    "    # Calculate the coeef for each model and put them in a dico \n",
    "    sums = {}\n",
    "    for i in range(m):\n",
    "        start = i * per_model\n",
    "        end   = (i + 1) * per_model\n",
    "        model_sum = coef_abs[:, start:end].sum()\n",
    "        sums[model_names[i]] = model_sum\n",
    "    \n",
    "    # Normalize so that the contributions sum to 1\n",
    "    total = sum(sums.values())\n",
    "    for name in sums:\n",
    "        sums[name] /= total\n",
    "        \n",
    "    return sums\n",
    "\n",
    "sums = model_contributions(coeff_matrice,[\"CNN\",\"NN + Covariance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b30465cf-441e-4d2f-8e67-f5394c9fd9a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = list(sums.keys())\n",
    "contribs = list(sums.values())\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(models, contribs)\n",
    "plt.ylabel('Relative Contribution')\n",
    "plt.xlabel('Base Model')\n",
    "plt.title('Model Contributions in Meta-Learner')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b753342-6a6e-4e29-b51d-931728596593",
   "metadata": {},
   "source": [
    "# à completer \n",
    "\"Discuss how the bias-variance tradeoff relates to the observed (or expected)\n",
    "evolution of performance.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7200fc39-0b93-4182-b893-c935d537e03b",
   "metadata": {},
   "source": [
    "#### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0592a278-3414-48f7-bed5-c8f8125312f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x_train, y_train, test_x):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_predict = model.predict(test_X)\n",
    "    return y_predict \n",
    "\n",
    "\n",
    "def build_csv(predict_guided, predict_free, name = \"team2_submission.csv\"):\n",
    "    submission_array = np.vstack((predict_guided, predict_free))\n",
    "    df = pd.DataFrame(submission_array)\n",
    "    df.to_csv(name, index=False, header=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
