{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "895a75fb",
   "metadata": {},
   "source": [
    "# INFO-f422: ML Project\n",
    "\n",
    "authors:\n",
    "+ 1 \n",
    "+ 2\n",
    "+ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dcc5fc",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17578f83-f52e-47ee-9d73-b33d910722dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.set_printoptions(threshold=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493fe755",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31ae6158-83fc-4039-ad35-4168b9fed8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data\"\n",
    "\n",
    "X_g_train = np.load(\"../guided/guided_dataset_X.npy\")\n",
    "y_g_train = np.load(\"../guided/guided_dataset_y.npy\")\n",
    "X_g_test = np.load(\"../guided/guided_testset_X.npy\")\n",
    "\n",
    "X_f_train = np.load(\"../freemoves/freemoves_dataset_X.npy\")\n",
    "y_f_train = np.load(\"../freemoves/freemoves_dataset_y.npy\")\n",
    "X_f_test = np.load(\"../freemoves/freemoves_testset_X.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5d74e88-bad4-4a00-9cdf-b5b5b1f6cfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guided:\n",
      "X_g_train (5, 8, 230000) / y_g_train(5, 51, 230000) / X_g_test(5, 332, 8, 500)\n",
      "\n",
      "Free moves:\n",
      "X_f_train(5, 8, 270000) / y_f_train(5, 51, 270000) / X_f_test(5, 308, 8, 500)\n"
     ]
    }
   ],
   "source": [
    "print(\"Guided:\")\n",
    "print(f\"X_g_train {X_g_train.shape} / y_g_train{y_g_train.shape} / X_g_test{X_g_test.shape}\\n\")\n",
    "print(\"Free moves:\")\n",
    "print(f\"X_f_train{X_f_train.shape} / y_f_train{y_f_train.shape} / X_f_test{X_f_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a29437e-9e32-45e1-a305-0bfad39af254",
   "metadata": {},
   "source": [
    "### 1) Signal filtering\n",
    "\n",
    "TODO: data exploration to take informed decision on filter (type of noise,....) to use and on filter parametres (no magic number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4331fabb-bc67-49c5-8d18-9572b5dc3e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import butter, sosfiltfilt, firwin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bdf832f-16c7-4dbf-b0cc-43fc14aa64c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def filtrage(x):\n",
    "    nyq  = 1024 / 2\n",
    "    low  = 20  / nyq\n",
    "    high = 450 / nyq\n",
    "    \n",
    "    sos = butter(4,[low,high], btype='band', output= 'sos')\n",
    "    \n",
    "    for sess in range(x.shape[0]):\n",
    "        for elec in range(x.shape[1]):\n",
    "            # Application of the filtrage for x\n",
    "            x[sess, elec, :] = sosfiltfilt(sos, x[sess, elec, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1dfcb3-48a7-4df1-80ec-79e5fd3bd630",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2) Dataset preparation\n",
    "\n",
    "At the beginning, we implemented a naive function that loops for each windows needed. \n",
    "\n",
    "This version work but:\n",
    "- Only when the step size can divides the total number of samples.  \n",
    "- Copies every window into a new array, incurring  CPU overhead and unnecessary memory usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24c97d3d-aa09-418c-bafd-3d7be401f11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap(data, overlap=0.5, size=500):\n",
    "    Data = []\n",
    "    step = int(size * (1 - overlap))\n",
    "    n = (data.shape[2] - size) // step + 1\n",
    "    fin = n * step\n",
    "    \n",
    "    for start in range(0, fin, step):\n",
    "        end = start + size\n",
    "        W = data[... , start:end]\n",
    "        Data.append(W)\n",
    "        \n",
    "    Data = np.array(Data)\n",
    "    Data = Data.transpose(1, 0, 2, 3)\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29750bdf-f261-4fd6-84b0-d4dc3bd2aa55",
   "metadata": {},
   "source": [
    "But after some research, we decided to use the sliding_window_view function from the Numpy library for several reasons:\n",
    "\n",
    "+ Fast vectorized numpy operations, compiled c-code (no python overhead, interpreter).\n",
    "\n",
    "+ sliding_window_view function returns a view, no copy.\n",
    "\n",
    "+ The function simplifies the implementation by automating window creation and indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f74676a2-d393-4e44-98ce-48190ed197a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guided windowed:\n",
      "X_g_train_wdw (5, 919, 8, 500) / y_g_train_wdw(5, 919, 51) / X_g_test(5, 332, 8, 500)\n",
      "X_f_train_wdw (5, 1079, 8, 500) / y_f_train_wdw(5, 1079, 51) / X_f_test(5, 308, 8, 500)\n"
     ]
    }
   ],
   "source": [
    "def create_overlap_windows(x, y, window_size, overlap, axis):\n",
    "\n",
    "    step = int(window_size * (1 - overlap))\n",
    "\n",
    "    # sliding_windows_view Generate all possible windows with the corresponding step, that not what we want.\n",
    "    x_w = sliding_window_view(x,window_size,axis)\n",
    "    y_w = sliding_window_view(y,window_size,axis)\n",
    "\n",
    "    # only keep windows where the step is a multiple of our step \n",
    "    x_w = x_w[:,:,::step,:]\n",
    "    y_w = y_w[:,:,::step,:]\n",
    "\n",
    "    # We transpose the axes windows and electrode/signal \n",
    "    x_w = x_w.transpose(0, 2, 1, 3)     #  (session, window, electrode, time) and not  (session, electrode, window, time) TODO??\n",
    "    y_w = y_w.transpose(0, 2, 1, 3)     # (session, window, signals, time)\n",
    "\n",
    "    # Finaly, we keep only the last hand position (targets) for y, because for this project\n",
    "    # we need to predict, for each window in x, the final hand position in the\n",
    "    # same windows in the dataset y\n",
    "    y_w = y_w[..., -1]  # (sessions, windows, targets)\n",
    "\n",
    "    return x_w, y_w\n",
    "\n",
    "\n",
    "X_g_train_wdw, y_g_train_wdw = create_overlap_windows(X_g_train, y_g_train, window_size=500, overlap=0.5, axis=2)\n",
    "X_f_train_wdw, y_f_train_wdw = create_overlap_windows(X_f_train, y_f_train, window_size=500, overlap=0.5, axis=2)\n",
    "# !! windowed data is a view --> share original data memory (modify one, modify both)\n",
    "\n",
    "print(\"Guided windowed:\")\n",
    "print(f\"X_g_train_wdw {X_g_train_wdw.shape} / y_g_train_wdw{y_g_train_wdw.shape} / X_g_test{X_g_test.shape}\")\n",
    "print(f\"X_f_train_wdw {X_f_train_wdw.shape} / y_f_train_wdw{y_f_train_wdw.shape} / X_f_test{X_f_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a5557ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_windows_tests(x, y):\n",
    "    # (maybe automate tests given windowsize and overlap and consider internal frag (shoudl be discarded)\n",
    "    \n",
    "    x_w, y_w = create_overlap_windows(x, y, window_size=500, overlap=0.5, axis=2)    \n",
    "    \n",
    "    assert np.array_equal(x_w[0, 0, 0, :10], x[0, 0, :10]) # (sess 0) first 10 of electrode 0 in window 0\n",
    "    assert np.array_equal(x_w[0, 1, 0, :10], x[0, 0, 250:260]) # (sess 0) first 10 of electrode 0 in window 1\n",
    "    assert np.array_equal(x_w[0, 1, 4, :10], x[0, 4, 250:260]) # (sess 0) first 10 of electrode 4 in window 1\n",
    "    assert np.array_equal(x_w[0, 918, 0, -10:], x[0, 0, 229990:230000]) # (sess 0) last 10 of electrode 0 in last window (918) - (perfect fit!)\n",
    "\n",
    "quick_windows_tests(X_g_train, y_g_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efa6666-9c5a-422c-8c26-79b8f6594316",
   "metadata": {},
   "source": [
    "#### 3) Cross validation strategy\n",
    "\n",
    "For this question, we have thought about various methods of cross validation. First, our data are continous because it's a signal, so preserving temporal structure is important. We can’t use a method of cross validation which randomly shuffles our windows. \n",
    "\n",
    "We also need to prevents data leaking so we can't use a methode who use the windows of one session for training AND validation because we have overlapping data in each session, two windows in the same session can share the same datas, and if these two windows are in train and validation, it will lead to data leakage and overly optimistic performance (data in the train set will also be in the validation set). \n",
    "\n",
    "So it's naturally that we have chosen the \"Leave One Group Out\" method, this method will use each session as the validation set once and the other for training. We completly prevent data leakage because each session is indepandent from the other, and we reduce the bias because each session will be used for validation.\n",
    "\n",
    "In our case, \"LOGO\" and \"GroupKFold(5)\" produce the same splits, but we choose \"LOGO\" because it's more explicit, readers will immediatly see that we use one session for validation each time while \"GroupKFold\" need to have 5 in parameter to do the same thong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d586a98-56d4-4372-a33f-01ba5ec537cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "groups(4595,)\n",
      "\n",
      "groups_f(5395,)\n",
      "\n",
      "Guided windowed flattened:\n",
      "X_g_train_wdw_flat(4595, 4000) / y_g_train_wdw_flat(4595, 51)\n",
      "Free windowed flattened:\n",
      "X_f_train_wdw_flat(5395, 4000) / y_f_train_wdw_flat(5395, 51)\n"
     ]
    }
   ],
   "source": [
    "x_shape = X_g_train_wdw.shape\n",
    "y_shape = y_g_train_wdw.shape\n",
    "\n",
    "x_f_shape = X_f_train_wdw.shape\n",
    "y_f_shape = y_f_train_wdw.shape\n",
    "\n",
    "\n",
    "groups = np.repeat(np.arange(1,x_shape[0]+1),x_shape[1] ) # 111 (919 times), 222 (919 times), ...\n",
    "print(f\"groups{groups.shape}\\n\")\n",
    "\n",
    "groups_f = np.repeat(np.arange(1,x_f_shape[0]+1),x_f_shape[1] ) \n",
    "print(f\"groups_f{groups_f.shape}\\n\")\n",
    "\n",
    "# We need to flatten the dataset x and y because the function logo (and latter \"croos_val_score\")\n",
    "# want all the data in a 2d list, we will know have  the dataset X for exemple.\n",
    "# [4595, 4000] and not [5,919,8,500], 4595 is the multiplication of 5 and 919 (3500 = 8*500), and y \n",
    "# [4595,51] and not [5,919,51].\n",
    "# Now all the windows are store in a list and the \"groups\" list above allow the function \n",
    "# logo to know at wich session each windows belong\n",
    "# The windows 3 for example (x_windows_flat[2]) belong to the sessions groups[2] = 1\n",
    "X_g_train_wdw_flat = X_g_train_wdw.reshape(x_shape[0] * x_shape[1], x_shape[2] * x_shape[3])\n",
    "y_g_train_wdw_flat = y_g_train_wdw.reshape(y_shape[0] * y_shape[1], y_shape[2])\n",
    "\n",
    "X_f_train_wdw_flat = X_f_train_wdw.reshape(x_f_shape[0] * x_f_shape[1], x_f_shape[2] * x_f_shape[3])\n",
    "y_f_train_wdw_flat = y_f_train_wdw.reshape(y_f_shape[0] * y_f_shape[1], y_f_shape[2])\n",
    "\n",
    "print(\"Guided windowed flattened:\")\n",
    "print(f\"X_g_train_wdw_flat{X_g_train_wdw_flat.shape} / y_g_train_wdw_flat{y_g_train_wdw_flat.shape}\")\n",
    "\n",
    "print(\"Free windowed flattened:\")\n",
    "print(f\"X_f_train_wdw_flat{X_f_train_wdw_flat.shape} / y_f_train_wdw_flat{y_f_train_wdw_flat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92449bff-754d-4a5a-b2ca-d88385116328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut, cross_val_score,cross_val_predict\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "07e1243c-045c-436b-85b1-725a43ead35f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# np.random.seed(0)\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "lasso_model = Lasso(max_iter=10) # If the iteration is higher, it take to much time, even on collab \n",
    "'''# If you don't want the warning but prepare your afternoon for the runtime \n",
    "lasso_model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lasso',   Lasso(alpha=0.1, max_iter=20000))\n",
    "])'''\n",
    "\n",
    "\n",
    "rmse_scorer = make_scorer(\n",
    "    lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "    greater_is_better=False  # Score near 0 is better \n",
    ")\n",
    "\n",
    "def cross_validation_with_scores(X,Y,groups,model,cv,scoring):\n",
    "    # The cross_val_score function by sklearn will execute our cv and return a tab \n",
    "    neg_rmse_scores = cross_val_score(\n",
    "        model,\n",
    "        X,\n",
    "        Y,\n",
    "        groups=groups,\n",
    "        cv=cv,\n",
    "        scoring=rmse_scorer,\n",
    "        n_jobs=-1 # Use all cores \n",
    "    )\n",
    "    \n",
    "    # Conversion of negatifs scores into positifs (convention of sklearn)\n",
    "    rmse_scores = -neg_rmse_scores  \n",
    "    print(\"RMSE for each folder:\", rmse_scores)\n",
    "    print(\"RMSE mean:\", rmse_scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "366607c5-4aac-4b22-9010-2a1f919dd752",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for each folder: [17.94011704 18.68652126 20.46060078 17.30326328 19.3046778 ]\n",
      "RMSE mean: 18.739036031720634\n",
      "RMSE for each folder: [17.80702643 16.56398358 16.04999994 15.49969335 13.02059498]\n",
      "RMSE mean: 15.788259656326385\n"
     ]
    }
   ],
   "source": [
    "#Guided \n",
    "cross_validation_with_scores(X_g_train_wdw_flat,y_g_train_wdw_flat,groups,lasso_model,logo,rmse_scorer)\n",
    "\n",
    "#Free\n",
    "cross_validation_with_scores(X_f_train_wdw_flat,y_f_train_wdw_flat,groups_f,lasso_model,logo,rmse_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "384848b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session 0 target info:\n",
      "  min = -108.68231864942676\n",
      "  max = 44.76897408739836\n",
      "  mean = -5.73247691191569\n"
     ]
    }
   ],
   "source": [
    "# rmse context\n",
    "sess = 0\n",
    "y_max = np.max(y_g_train_wdw[sess])\n",
    "y_min = np.min(y_g_train_wdw[sess])\n",
    "y_mean = np.mean(y_g_train_wdw[sess])\n",
    "\n",
    "print(f\"Session {sess} target info:\\n  min = {y_min}\\n  max = {y_max}\\n  mean = {y_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecd17ef4-108a-480b-aba8-33de84479a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "   train groups: [2 3 4 5]\n",
      "   test groups: [1]\n",
      "Fold 1\n",
      "   train groups: [1 3 4 5]\n",
      "   test groups: [2]\n",
      "Fold 2\n",
      "   train groups: [1 2 4 5]\n",
      "   test groups: [3]\n",
      "Fold 3\n",
      "   train groups: [1 2 3 5]\n",
      "   test groups: [4]\n",
      "Fold 4\n",
      "   train groups: [1 2 3 4]\n",
      "   test groups: [5]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, (train_index, test_index) in enumerate(logo.split(X_g_train_wdw_flat, y_g_train_wdw_flat, groups)):\n",
    "    print(f\"Fold {i}\")\n",
    "    print(f\"   train groups: {np.unique(groups[train_index])}\")\n",
    "    print(f\"   test groups: {np.unique(groups[test_index])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb3b954-0157-4eae-a02d-355a20329d9a",
   "metadata": {},
   "source": [
    "#### 5) More sophisticated approach\n",
    "\n",
    "For this question, we decided to implement the two approaches in order to have a better understanding and more methods to compare.\n",
    "\n",
    "We started with the covariance approch, following the steps in section 3.2:\n",
    "\n",
    "- We first calculate the covaraince of each windows with the PyRiemmann Covariances class, which expects a 3d array, so we need to reshape it into the form (windows,electrode,time). After using this class, we obtain an array of 8×8 covariance matrices (SPD_tab) for each window.\n",
    "\n",
    "- Next, we map each SPD matrix into their tangent space using the TangentSpace class. This projection transforms our SPD matrices into Euclidean vectors. Thanks to this, our dataset becomes 2D again, and we can directly use a traditional regression algorithms and sklearn function.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ffd5b0f-b9dc-406e-a930-9ec1ba230d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pyriemann\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from sklearn.pipeline     import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c4668e27-97cc-4c58-b39f-1d67dcbb7ce4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Covariances method\n",
    "\n",
    "\n",
    "# Reshape the flattten dataset\n",
    "X_g_train_reshape = X_g_train_wdw_flat.reshape(4595,8,500)\n",
    "\n",
    "X_f_train_reshape = X_f_train_wdw_flat.reshape(5395,8,500)\n",
    "\n",
    "'''\n",
    "covariance = Covariances(estimator='oas')\n",
    "SPD_tab = covariance.fit_transform(X_g_train_reshape) \n",
    "# print(SPD_tab.shape) # (4595,8,8)\n",
    "ts = TangentSpace()\n",
    "tangent_tab = ts.fit_transform(SPD_tab)\n",
    "# print(tangent_tab.shape) # (4595,36) Now that we have a 2d tab, we can use traditional regression algorithms\n",
    "'''\n",
    "\n",
    "pipe_cov = Pipeline([\n",
    "    ('cov',    Covariances(estimator='oas')),   # Covariances matrices of each windows\n",
    "    ('ts',     TangentSpace()),                 # This projection will transform our SPD matrices into euclidean vector\n",
    "    ('scale', StandardScaler()),          # Standardize each feature (mean =0, std =1) \n",
    "    ('lasso',  lasso_model)               # Lasso model \n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "96428ea2-8afb-40d0-a5d8-5d0a9b1db6d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for each folder: [8.0414517  7.14881399 7.4050069  6.7920822  7.41891918]\n",
      "RMSE mean: 7.361254792960136\n",
      "RMSE for each folder: [10.6355356  11.95213066 10.92915443  9.83407385  8.91441275]\n",
      "RMSE mean: 10.453061456436913\n"
     ]
    }
   ],
   "source": [
    "# Now we juste need to use the cv function we build above \n",
    "#Guided \n",
    "cross_validation_with_scores(X_g_train_reshape,y_g_train_wdw_flat,groups,pipe_cov,logo,rmse_scorer)\n",
    "\n",
    "#Free \n",
    "cross_validation_with_scores(X_f_train_reshape,y_f_train_wdw_flat,groups_f,pipe_cov,logo,rmse_scorer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744f83a5-45be-463c-93c1-254dec71ef8e",
   "metadata": {},
   "source": [
    "For the neural network approch, we have done:\n",
    "\n",
    "##### Simple model\n",
    "We started by a simple model composed of 3 linear layers.\n",
    "\n",
    "To evaluate it, we used Skorch, which lets us plug a PyTorch nn.Module into a sklearn function.\n",
    "Thanks to Skorch, we could reuse our existing cross_validation_with_scores() function.\n",
    "\n",
    "The rmse mean was 17.03, which beat the vanilla lasso model but not the covriance matrices lasso. We analyzed the value of the training loss and validation loss for each epoch and we saw that the model is underfitting.\n",
    "So we decide to complexify it.\n",
    "\n",
    "##### Model complexity\n",
    "\n",
    "We upgraded to a small CNN with three 1D convolutional layers because the dataset has to much datas for an nn classique and to automatically learn local temporal patterns in the EMG signal. \n",
    "\n",
    "After each convolutional layer:\n",
    "\n",
    "We normalize our data to stabilize and speed up training.\n",
    "\n",
    "We introduce a simple Relu activation so the network can learn more complex features.\n",
    "\n",
    "We “cut” the time dimension in half, keeping only the strongest responses and reducing data size.\n",
    "\n",
    "Once the three convolutional blocks are done, we flatten the output tensor and pass it into a two layer head, to convert these extracted features into the 51 joint-angle predictions\n",
    "\n",
    "##### Early stopping and LR scheduling\n",
    "\n",
    "We added two Skorch callbacks:\n",
    "\n",
    "-EarlyStopping to stop the training when no improvement is seen for 5 epochs.\n",
    "\n",
    "-LRScheduler to cut the learning rate by half when the validation loss stalls 3 epochs consecutives.\n",
    "\n",
    "This combination prevents wasted epochs once the model converges and refines the learning rate to squeeze out extra gains.\n",
    "\n",
    "##### Batch size reduction\n",
    "\n",
    "We lowered the batch size from 128 to 64. Using smaller batches adds a bit of randomness to each weight update, which helps the model generalize better without altering its structure.\n",
    "\n",
    "With these three changes, the nn average RMSE dropped to ~5.07, a dramatic improvement over the initial ~17."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8074777f-d8a4-4172-b732-6a425d3f0fbc",
   "metadata": {},
   "source": [
    "| Simple Model | Complex Model | Complex Model + Early Stopping & LR Scheduler | Same as #3 but Batch Size = 64 |\n",
    "|:------------:|:-------------:|:---------------------------------------------:|:-----------------------------:|\n",
    "| ![](./images/1.png) | ![](./images/2.png) | ![](./images/3.png) | ![](./images/4.png) |\n",
    "| **RMSE mean:** 17.03 | **RMSE mean:** 6.06 | **RMSE mean:** 5.89 | **RMSE mean:** 4.87 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8a70a53-6562-4aa8-b197-af063ca6a13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U skorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skorch import NeuralNetRegressor\n",
    "from skorch.callbacks import EarlyStopping\n",
    "from skorch.callbacks import LRScheduler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42cd60a5-d6f7-4e74-91f0-f26c7c8b7ec8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "## NN method\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.f = nn.Sequential(\n",
    "            nn.Unflatten(1, (8,500)), #unflatten data for  convolution (64 (batch_size), 8,500)\n",
    "\n",
    "            nn.Conv1d(8, 32, kernel_size=11, padding=5), # 8 input channels (electrodes) and 32 is the output, the number of feature he learn.\n",
    "            # Thanks to padding, the output length remains 500 (64,32,500) \n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),  # Halve the temporal dimension  (64,32,250)          \n",
    "            \n",
    "            nn.Conv1d(32, 64, kernel_size=9, padding=4), #(64,64,250)\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),  #(64,64,125)      \n",
    "\n",
    "            nn.Conv1d(64, 128, kernel_size=7, padding=3), # (64,128,125)\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),  # (64,128,62)    \n",
    "            \n",
    "            nn.Flatten(), # Reflatten our data for the next part (64,128*62)\n",
    "        )\n",
    "        self.r = nn.Sequential(\n",
    "            nn.Linear(7936, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 51),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.f(x)\n",
    "        return self.r(x)\n",
    "\n",
    "      \n",
    "# Convert dataset from float64 to float32 because PyTorch layers expect float32\n",
    "# 32-bit precision is sufficient for our signals and speeds up training\n",
    "print(X_g_train_wdw_flat.dtype)   \n",
    "print(y_g_train_wdw_flat.dtype)   \n",
    "\n",
    "x = X_g_train_wdw_flat.astype('float32')\n",
    "y = y_g_train_wdw_flat.astype('float32')\n",
    "\n",
    "x_f = X_f_train_wdw_flat.astype('float32')\n",
    "y_f = y_f_train_wdw_flat.astype('float32')\n",
    "\n",
    "\n",
    "net = NeuralNetRegressor(\n",
    "    module=NeuralNetwork,                 # PyTorch model \n",
    "    max_epochs=100,                 \n",
    "    lr=1e-3,                       \n",
    "    batch_size=64,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    callbacks=[('earlystop', EarlyStopping('valid_loss', patience=10)), # Stop if validation loss doesn't improve for 5 epochs \n",
    "                    ('lr_sched', LRScheduler(\n",
    "           policy=torch.optim.lr_scheduler.ReduceLROnPlateau, # Halve LR if validation loss stalls for 3 epochs\n",
    "           monitor='valid_loss',\n",
    "           patience=3, factor=0.5))]\n",
    ")\n",
    "\n",
    "\n",
    "pipe_cnn = Pipeline([\n",
    "    ('scale', StandardScaler()),  # Standardize inputs for a quick start, after the cnn will then normalize its data at each step\n",
    "    ('net',   net)                \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3771a06a-a336-45a1-af7d-146c519b0c45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m183.7466\u001b[0m      \u001b[32m117.0032\u001b[0m  0.0010  12.0949\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m189.3073\u001b[0m      \u001b[32m126.2191\u001b[0m  0.0010  12.0673\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m180.7975\u001b[0m      \u001b[32m117.4314\u001b[0m  0.0010  12.1996\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m197.3731\u001b[0m      \u001b[32m121.0930\u001b[0m  0.0010  12.2818\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m188.5981\u001b[0m      \u001b[32m127.7126\u001b[0m  0.0010  12.3568\n",
      "      2       \u001b[36m87.7310\u001b[0m      \u001b[32m109.3404\u001b[0m  0.0010  14.1686\n",
      "      2       \u001b[36m83.0573\u001b[0m       \u001b[32m79.4835\u001b[0m  0.0010  14.5236\n",
      "      2       \u001b[36m86.1038\u001b[0m      \u001b[32m101.4511\u001b[0m  0.0010  14.5651\n",
      "      2       \u001b[36m92.7528\u001b[0m       \u001b[32m80.5137\u001b[0m  0.0010  14.4968\n",
      "      2       \u001b[36m88.0846\u001b[0m       \u001b[32m70.8250\u001b[0m  0.0010  14.6335\n",
      "      3       \u001b[36m71.7454\u001b[0m       \u001b[32m82.0072\u001b[0m  0.0010  14.1871\n",
      "      3       \u001b[36m71.3979\u001b[0m       \u001b[32m70.8756\u001b[0m  0.0010  14.4481\n",
      "      3       \u001b[36m73.8106\u001b[0m      104.9197  0.0010  14.5051\n",
      "      3       \u001b[36m73.3394\u001b[0m       \u001b[32m65.9932\u001b[0m  0.0010  14.5265\n",
      "      3       \u001b[36m74.4162\u001b[0m       \u001b[32m63.6646\u001b[0m  0.0010  14.6656\n",
      "      4       \u001b[36m65.3786\u001b[0m      109.8720  0.0010  13.6359\n",
      "      4       \u001b[36m66.1868\u001b[0m       \u001b[32m67.8483\u001b[0m  0.0010  13.9701\n",
      "      4       \u001b[36m67.9883\u001b[0m       \u001b[32m92.9873\u001b[0m  0.0010  13.8523\n",
      "      4       \u001b[36m65.2174\u001b[0m       70.3397  0.0010  14.1126\n",
      "      4       \u001b[36m66.6974\u001b[0m       \u001b[32m61.5003\u001b[0m  0.0010  13.8529\n",
      "      5       \u001b[36m57.1699\u001b[0m      121.2900  0.0010  13.9379\n",
      "      5       \u001b[36m61.9896\u001b[0m       \u001b[32m67.5517\u001b[0m  0.0010  14.2278\n",
      "      5       \u001b[36m63.3696\u001b[0m      112.2947  0.0010  14.1869\n",
      "      5       \u001b[36m57.4883\u001b[0m       \u001b[32m62.4535\u001b[0m  0.0010  14.0096\n",
      "      5       \u001b[36m61.2147\u001b[0m       66.8975  0.0010  14.4048\n",
      "      6       \u001b[36m49.4435\u001b[0m      110.6339  0.0010  14.3343\n",
      "      6       \u001b[36m48.9398\u001b[0m       71.6584  0.0010  14.6942\n",
      "      6       \u001b[36m59.4840\u001b[0m      157.9479  0.0010  14.9415\n",
      "      6       \u001b[36m57.9302\u001b[0m       74.2148  0.0010  15.0156\n",
      "      6       \u001b[36m55.6606\u001b[0m       68.8032  0.0010  14.7876\n",
      "      7       \u001b[36m44.3109\u001b[0m       85.4393  0.0010  14.6077\n",
      "      7       \u001b[36m52.2402\u001b[0m       68.8686  0.0010  15.0368\n",
      "      7       \u001b[36m54.5199\u001b[0m      105.4687  0.0010  15.1324\n",
      "      7       \u001b[36m44.4612\u001b[0m       99.0961  0.0010  15.3240\n",
      "      7       \u001b[36m49.1960\u001b[0m       90.8365  0.0010  15.3686\n",
      "      8       \u001b[36m39.3213\u001b[0m       \u001b[32m44.4001\u001b[0m  0.0005  15.9957\n",
      "      8       \u001b[36m48.1076\u001b[0m       \u001b[32m57.3495\u001b[0m  0.0010  16.1063\n",
      "      8       \u001b[36m40.2043\u001b[0m      103.9971  0.0010  16.1512\n",
      "      8       \u001b[36m47.1071\u001b[0m       \u001b[32m51.6719\u001b[0m  0.0010  16.2713\n",
      "      8       \u001b[36m44.7533\u001b[0m      115.2530  0.0010  16.5479\n",
      "      9       \u001b[36m35.5247\u001b[0m       50.5773  0.0005  15.6664\n",
      "      9       \u001b[36m44.8019\u001b[0m       \u001b[32m49.9908\u001b[0m  0.0010  15.6464\n",
      "      9       \u001b[36m33.9791\u001b[0m       75.1117  0.0010  15.5985\n",
      "      9       \u001b[36m40.4772\u001b[0m       67.6510  0.0010  15.6340\n",
      "      9       \u001b[36m42.6010\u001b[0m       \u001b[32m55.0832\u001b[0m  0.0005  15.5347\n",
      "     10       \u001b[36m31.1481\u001b[0m       50.5102  0.0005  14.4477\n",
      "     10       \u001b[36m39.3330\u001b[0m       57.4269  0.0010  14.6053\n",
      "     10       \u001b[36m27.6671\u001b[0m       \u001b[32m62.0904\u001b[0m  0.0005  14.6465\n",
      "     10       \u001b[36m33.3133\u001b[0m       68.6059  0.0010  14.8592\n",
      "     10       \u001b[36m37.3235\u001b[0m       \u001b[32m42.6188\u001b[0m  0.0005  14.6990\n",
      "     11       \u001b[36m26.4877\u001b[0m       44.7340  0.0005  14.0472\n",
      "     11       \u001b[36m32.6888\u001b[0m       53.7821  0.0010  14.2988\n",
      "     11       \u001b[36m23.5136\u001b[0m       \u001b[32m46.3112\u001b[0m  0.0005  14.3089\n",
      "     11       \u001b[36m24.3804\u001b[0m       70.2326  0.0010  14.2412\n",
      "     11       \u001b[36m32.5826\u001b[0m       \u001b[32m40.0812\u001b[0m  0.0005  14.6744\n",
      "     12       \u001b[36m21.0480\u001b[0m       \u001b[32m36.5697\u001b[0m  0.0005  15.2448\n",
      "     12       \u001b[36m21.7970\u001b[0m       \u001b[32m34.1127\u001b[0m  0.0010  15.7525\n",
      "     12       \u001b[36m17.2035\u001b[0m       56.0573  0.0010  15.4997\n",
      "     12       \u001b[36m19.9292\u001b[0m       46.5264  0.0005  15.8770\n",
      "     12       \u001b[36m28.7695\u001b[0m       \u001b[32m38.7565\u001b[0m  0.0005  15.7497\n",
      "     13       \u001b[36m16.7622\u001b[0m       \u001b[32m32.9763\u001b[0m  0.0005  14.6307\n",
      "     13       \u001b[36m15.7971\u001b[0m       48.2693  0.0010  14.5587\n",
      "     13       \u001b[36m13.9830\u001b[0m       \u001b[32m41.9673\u001b[0m  0.0005  14.6748\n",
      "     13       \u001b[36m16.7260\u001b[0m       47.9487  0.0005  14.7042\n",
      "     13       \u001b[36m23.9879\u001b[0m       \u001b[32m38.1265\u001b[0m  0.0005  14.6256\n",
      "     14       \u001b[36m15.0545\u001b[0m       \u001b[32m26.1093\u001b[0m  0.0005  14.3299\n",
      "     14       \u001b[36m13.3090\u001b[0m       36.3172  0.0010  14.2799\n",
      "     14       \u001b[36m14.4306\u001b[0m       49.8201  0.0005  14.1752\n",
      "     14       14.0210       \u001b[32m41.7501\u001b[0m  0.0005  14.4258\n",
      "     14       \u001b[36m17.6418\u001b[0m       \u001b[32m35.7833\u001b[0m  0.0005  14.5430\n",
      "     15       \u001b[36m14.0350\u001b[0m       \u001b[32m23.4821\u001b[0m  0.0005  14.7648\n",
      "     15       \u001b[36m12.1817\u001b[0m       37.6700  0.0010  15.3711\n",
      "     15       \u001b[36m12.9157\u001b[0m       52.3708  0.0005  15.3298\n",
      "     15       \u001b[36m12.5397\u001b[0m       \u001b[32m41.4435\u001b[0m  0.0005  15.4610\n",
      "     15       \u001b[36m13.9971\u001b[0m       \u001b[32m32.3430\u001b[0m  0.0005  15.4558\n",
      "     16       \u001b[36m12.7113\u001b[0m       \u001b[32m23.4646\u001b[0m  0.0005  16.5370\n",
      "     16       \u001b[36m12.1313\u001b[0m       44.4728  0.0010  16.1694\n",
      "     16       \u001b[36m11.7758\u001b[0m       \u001b[32m39.1254\u001b[0m  0.0005  16.3998\n",
      "     16       12.9642       \u001b[32m28.0579\u001b[0m  0.0003  16.7071\n",
      "     16       \u001b[36m12.4968\u001b[0m       \u001b[32m30.2941\u001b[0m  0.0005  16.4748\n",
      "     17       \u001b[36m11.3795\u001b[0m       25.9693  0.0005  16.4189\n",
      "     17       14.5502       45.2093  0.0005  16.3712\n",
      "     17       \u001b[36m11.1880\u001b[0m       \u001b[32m34.6563\u001b[0m  0.0005  16.2729\n",
      "     17       \u001b[36m12.9074\u001b[0m       31.7588  0.0003  16.5078\n",
      "     17       \u001b[36m11.7994\u001b[0m       30.7997  0.0005  16.2580\n",
      "     18       \u001b[36m10.4949\u001b[0m       30.7538  0.0005  16.5941\n",
      "     18       14.9241       35.9784  0.0005  16.4648\n",
      "     18       \u001b[36m10.6175\u001b[0m       \u001b[32m30.9170\u001b[0m  0.0005  16.5413\n",
      "     18       \u001b[36m11.3824\u001b[0m       31.9787  0.0003  16.6717\n",
      "     18       \u001b[36m11.5623\u001b[0m       \u001b[32m29.8159\u001b[0m  0.0005  16.7126\n",
      "     19        \u001b[36m9.9474\u001b[0m       35.5863  0.0005  16.8174\n",
      "     19       12.4474       \u001b[32m27.7684\u001b[0m  0.0005  16.8604\n",
      "     19       \u001b[36m10.1054\u001b[0m       \u001b[32m28.4498\u001b[0m  0.0005  17.1102\n",
      "     19       \u001b[36m10.4908\u001b[0m       31.1242  0.0003  17.2091\n",
      "     19       12.0168       \u001b[32m26.2790\u001b[0m  0.0005  17.1805\n",
      "     20        \u001b[36m9.4837\u001b[0m       42.8768  0.0005  16.2360\n",
      "     20       \u001b[36m10.3046\u001b[0m       \u001b[32m24.4845\u001b[0m  0.0005  16.5125\n",
      "     20        \u001b[36m9.6377\u001b[0m       \u001b[32m27.2457\u001b[0m  0.0005  16.3904\n",
      "     20       \u001b[36m10.0683\u001b[0m       31.5514  0.0003  16.4078\n",
      "     20       12.9143       \u001b[32m23.9491\u001b[0m  0.0005  16.3858\n",
      "     21       10.3092       47.7319  0.0003  16.1979\n",
      "     21        \u001b[36m8.9405\u001b[0m       \u001b[32m23.0144\u001b[0m  0.0005  16.4071\n",
      "     21        \u001b[36m9.1261\u001b[0m       27.3972  0.0005  16.2243\n",
      "     21       10.5835       34.8217  0.0001  16.5969\n",
      "     21       13.0173       32.9741  0.0005  16.5172\n",
      "     22       10.3175       40.6191  0.0003  16.3815\n",
      "     22        \u001b[36m8.2134\u001b[0m       \u001b[32m22.3835\u001b[0m  0.0005  17.0183\n",
      "     22        \u001b[36m8.5912\u001b[0m       27.3444  0.0005  16.8777\n",
      "     22       10.5427       36.3116  0.0001  17.0553\n",
      "     22       12.9799       40.4781  0.0005  17.0027\n",
      "     23        9.8421       31.5344  0.0003  17.5825\n",
      "     23        \u001b[36m7.7842\u001b[0m       \u001b[32m21.9219\u001b[0m  0.0005  18.0699\n",
      "     23        \u001b[36m8.2544\u001b[0m       27.2781  0.0005  17.8023\n",
      "     23        \u001b[36m9.8678\u001b[0m       37.0696  0.0001  18.1005\n",
      "     23       13.4291       71.6399  0.0005  17.6475\n",
      "     24        9.5244       26.9491  0.0003  16.5383\n",
      "     24        \u001b[36m7.4871\u001b[0m       \u001b[32m21.6995\u001b[0m  0.0005  16.3869\n",
      "     24        \u001b[36m8.0489\u001b[0m       27.5470  0.0005  16.2506\n",
      "     24        \u001b[36m9.4152\u001b[0m       36.8696  0.0001  16.8776\n",
      "     24       13.5813       65.7949  0.0005  16.6669\n",
      "     25       10.3718       24.7460  0.0001  16.2626\n",
      "     25        8.7005       37.4583  0.0003  16.1773\n",
      "     25        \u001b[36m7.2708\u001b[0m       \u001b[32m21.4301\u001b[0m  0.0005  16.3370\n",
      "     25        \u001b[36m8.6015\u001b[0m       30.4528  0.0001  15.9244\n",
      "     25       \u001b[36m10.5195\u001b[0m       33.2960  0.0003  16.1115\n",
      "     26        \u001b[36m7.1014\u001b[0m       \u001b[32m21.2599\u001b[0m  0.0005  15.1655\n",
      "     26        9.1942       34.2053  0.0003  15.4370\n",
      "     26        \u001b[36m9.2970\u001b[0m       32.3858  0.0003  15.0979\n",
      "     27        \u001b[36m6.9604\u001b[0m       \u001b[32m21.0745\u001b[0m  0.0005  12.0926\n",
      "     27        8.8059       36.2714  0.0003  11.9173\n",
      "     27        \u001b[36m8.9488\u001b[0m       30.8550  0.0003  11.6687\n",
      "     28        8.5696       36.9689  0.0003  11.2831\n",
      "     28        \u001b[36m6.8514\u001b[0m       \u001b[32m20.9646\u001b[0m  0.0005  11.5479\n",
      "     28        \u001b[36m8.6654\u001b[0m       29.2258  0.0003  11.2927\n",
      "     29        8.6220       34.5831  0.0001  11.9292\n",
      "     29        \u001b[36m6.7637\u001b[0m       \u001b[32m20.7902\u001b[0m  0.0005  11.9827\n",
      "     29        \u001b[36m8.4623\u001b[0m       \u001b[32m23.1965\u001b[0m  0.0001  12.2200\n",
      "     30        \u001b[36m6.6904\u001b[0m       \u001b[32m20.6686\u001b[0m  0.0005  13.5965\n",
      "     30        \u001b[36m8.3133\u001b[0m       \u001b[32m22.6891\u001b[0m  0.0001  13.2260\n",
      "     31        \u001b[36m6.6310\u001b[0m       20.6694  0.0005  9.8761\n",
      "     31        \u001b[36m8.0974\u001b[0m       \u001b[32m22.4580\u001b[0m  0.0001  9.7120\n",
      "     32        \u001b[36m6.5581\u001b[0m       20.7353  0.0005  10.0860\n",
      "     32        \u001b[36m7.9089\u001b[0m       \u001b[32m22.3074\u001b[0m  0.0001  10.2957\n",
      "     33        \u001b[36m6.5405\u001b[0m       20.8101  0.0005  10.3465\n",
      "     33        \u001b[36m7.7534\u001b[0m       \u001b[32m22.1970\u001b[0m  0.0001  10.2079\n",
      "     34        \u001b[36m6.5155\u001b[0m       21.0443  0.0005  9.8030\n",
      "     34        \u001b[36m7.6215\u001b[0m       \u001b[32m22.1002\u001b[0m  0.0001  9.9232\n",
      "     35        7.0904       25.2996  0.0003  10.2499\n",
      "     35        \u001b[36m7.5074\u001b[0m       22.1028  0.0001  10.2432\n",
      "     36        7.3523       24.4434  0.0003  9.7671\n",
      "     36        \u001b[36m7.4084\u001b[0m       \u001b[32m22.0664\u001b[0m  0.0001  9.7224\n",
      "     37        6.8777       24.6409  0.0003  9.6032\n",
      "     37        \u001b[36m7.3202\u001b[0m       \u001b[32m22.0404\u001b[0m  0.0001  9.6784\n",
      "     38        6.5724       24.3151  0.0003  9.7047\n",
      "     38        \u001b[36m7.2417\u001b[0m       \u001b[32m22.0247\u001b[0m  0.0001  9.6217\n",
      "     39        \u001b[36m6.2294\u001b[0m       22.3135  0.0001  9.4282\n",
      "     39        \u001b[36m7.1695\u001b[0m       \u001b[32m22.0160\u001b[0m  0.0001  9.5417\n",
      "     40        \u001b[36m7.1056\u001b[0m       22.0190  0.0001  9.3791\n",
      "     41        \u001b[36m7.0472\u001b[0m       22.0705  0.0001  7.8706\n",
      "     42        \u001b[36m6.9916\u001b[0m       22.1654  0.0001  7.8522\n",
      "     43        \u001b[36m6.9413\u001b[0m       22.2100  0.0001  7.6442\n",
      "     44        \u001b[36m6.8377\u001b[0m       22.5861  0.0001  7.7697\n",
      "     45        \u001b[36m6.7624\u001b[0m       22.9948  0.0001  7.6128\n",
      "     46        \u001b[36m6.7209\u001b[0m       22.9864  0.0001  7.8058\n",
      "     47        \u001b[36m6.6829\u001b[0m       23.0200  0.0001  7.8871\n",
      "     48        \u001b[36m6.5561\u001b[0m       23.4852  0.0000  7.8046\n",
      "RMSE for each folder: [6.15534556 4.68717426 4.93688072 4.41636697 3.63331335]\n",
      "RMSE mean: 4.765816174271922\n"
     ]
    }
   ],
   "source": [
    "#Guided\n",
    "cross_validation_with_scores(x,y,groups,pipe_cnn,logo,rmse_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "080fae37-8d5c-420c-ba16-00a2e9acaf4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m219.1000\u001b[0m      \u001b[32m293.1583\u001b[0m  0.0010  13.9384\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m224.9589\u001b[0m      \u001b[32m242.9936\u001b[0m  0.0010  14.0721\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m222.1733\u001b[0m      \u001b[32m205.4595\u001b[0m  0.0010  14.1591\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m231.9693\u001b[0m      \u001b[32m148.7291\u001b[0m  0.0010  14.1602\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m199.9677\u001b[0m      \u001b[32m221.3613\u001b[0m  0.0010  14.1809\n",
      "      2      \u001b[36m177.8046\u001b[0m      \u001b[32m210.1426\u001b[0m  0.0010  14.1591\n",
      "      2      \u001b[36m162.1591\u001b[0m      \u001b[32m184.0694\u001b[0m  0.0010  14.1777\n",
      "      2      \u001b[36m156.1242\u001b[0m      \u001b[32m254.2190\u001b[0m  0.0010  14.4730\n",
      "      2      \u001b[36m150.9769\u001b[0m      \u001b[32m141.5371\u001b[0m  0.0010  14.2469\n",
      "      2      \u001b[36m166.1331\u001b[0m      \u001b[32m144.2413\u001b[0m  0.0010  14.3413\n",
      "      3      \u001b[36m176.2048\u001b[0m      \u001b[32m197.3821\u001b[0m  0.0010  14.7587\n",
      "      3      \u001b[36m156.3163\u001b[0m      189.9777  0.0010  14.8073\n",
      "      3      \u001b[36m141.6545\u001b[0m      \u001b[32m135.2890\u001b[0m  0.0010  14.8922\n",
      "      3      \u001b[36m146.0714\u001b[0m      257.4220  0.0010  14.9178\n",
      "      3      \u001b[36m162.7391\u001b[0m      \u001b[32m137.0963\u001b[0m  0.0010  14.9614\n",
      "      4      \u001b[36m173.3958\u001b[0m      \u001b[32m193.4380\u001b[0m  0.0010  14.6346\n",
      "      4      \u001b[36m150.2952\u001b[0m      193.9317  0.0010  14.5813\n",
      "      4      \u001b[36m136.6154\u001b[0m      \u001b[32m134.3099\u001b[0m  0.0010  14.6420\n",
      "      4      \u001b[36m141.1076\u001b[0m      259.0159  0.0010  14.7415\n",
      "      4      \u001b[36m159.7538\u001b[0m      \u001b[32m130.4021\u001b[0m  0.0010  14.7152\n",
      "      5      \u001b[36m166.4826\u001b[0m      194.5589  0.0010  14.2701\n",
      "      5      \u001b[36m145.6193\u001b[0m      \u001b[32m183.0582\u001b[0m  0.0010  14.6145\n",
      "      5      \u001b[36m132.1799\u001b[0m      138.3429  0.0010  14.7039\n",
      "      5      \u001b[36m136.1898\u001b[0m      263.2105  0.0010  14.6232\n",
      "      5      \u001b[36m154.4065\u001b[0m      \u001b[32m126.1199\u001b[0m  0.0010  14.5586\n",
      "      6      \u001b[36m161.0219\u001b[0m      \u001b[32m182.2900\u001b[0m  0.0010  14.1554\n",
      "      6      \u001b[36m139.8228\u001b[0m      \u001b[32m181.8283\u001b[0m  0.0010  14.2213\n",
      "      6      \u001b[36m125.7068\u001b[0m      148.9457  0.0010  14.2847\n",
      "      6      \u001b[36m130.4065\u001b[0m      265.1301  0.0010  14.3160\n",
      "      6      \u001b[36m147.2465\u001b[0m      129.7780  0.0010  14.5071\n",
      "      7      \u001b[36m154.9386\u001b[0m      \u001b[32m170.1131\u001b[0m  0.0010  15.1274\n",
      "      7      \u001b[36m134.1331\u001b[0m      \u001b[32m175.1883\u001b[0m  0.0010  15.3805\n",
      "      7      \u001b[36m120.9409\u001b[0m      314.7051  0.0005  15.1766\n",
      "      7      \u001b[36m119.2627\u001b[0m      166.4424  0.0010  15.3496\n",
      "      7      \u001b[36m142.5872\u001b[0m      131.0715  0.0010  15.2449\n",
      "      8      \u001b[36m150.3713\u001b[0m      \u001b[32m165.5861\u001b[0m  0.0010  14.3191\n",
      "      8      136.4665      209.8483  0.0010  14.2748\n",
      "      8      \u001b[36m113.9828\u001b[0m      187.6476  0.0010  14.1833\n",
      "      8      \u001b[36m119.3766\u001b[0m      321.1656  0.0005  14.6094\n",
      "      8      \u001b[36m137.4574\u001b[0m      \u001b[32m118.1850\u001b[0m  0.0010  14.3662\n",
      "      9      \u001b[36m144.2386\u001b[0m      170.9478  0.0010  13.9271\n",
      "      9      \u001b[36m132.1786\u001b[0m      184.6560  0.0010  14.1166\n",
      "      9      \u001b[36m105.2157\u001b[0m      138.2855  0.0005  14.0553\n",
      "      9      \u001b[36m115.6730\u001b[0m      325.9579  0.0005  14.0569\n",
      "      9      \u001b[36m133.1512\u001b[0m      119.2354  0.0010  14.0263\n",
      "     10      \u001b[36m142.5886\u001b[0m      168.9612  0.0010  14.4149\n",
      "     10      133.5078      197.4873  0.0010  14.3903\n",
      "     10      \u001b[36m102.5010\u001b[0m      156.3772  0.0005  14.3345\n",
      "     10      \u001b[36m128.8063\u001b[0m      123.2942  0.0010  14.4275\n",
      "     10      \u001b[36m112.5890\u001b[0m      321.1211  0.0005  14.4851\n",
      "     11      \u001b[36m139.6621\u001b[0m      172.1907  0.0010  14.7536\n",
      "     11      \u001b[36m129.7231\u001b[0m      215.0835  0.0010  15.0305\n",
      "     11       \u001b[36m99.7653\u001b[0m      162.9699  0.0005  15.0673\n",
      "     11      \u001b[36m106.4404\u001b[0m      271.7501  0.0003  15.1068\n",
      "     11      \u001b[36m122.6393\u001b[0m      \u001b[32m117.5987\u001b[0m  0.0010  15.1684\n",
      "     12      141.1559      172.2909  0.0010  15.1755\n",
      "     12      141.5118      \u001b[32m128.6639\u001b[0m  0.0005  15.1984\n",
      "     12       \u001b[36m96.9903\u001b[0m      168.5721  0.0005  15.2853\n",
      "     12      \u001b[36m117.6419\u001b[0m      123.2884  0.0010  15.0792\n",
      "     13      155.6607      \u001b[32m120.6787\u001b[0m  0.0005  13.3346\n",
      "     13      131.2549      138.5970  0.0005  13.2813\n",
      "     13      \u001b[36m112.9057\u001b[0m      121.8150  0.0010  13.0410\n",
      "     13       \u001b[36m90.6999\u001b[0m      138.5236  0.0003  13.3709\n",
      "     14      141.8527      127.8630  0.0005  13.2585\n",
      "     14      \u001b[36m129.1711\u001b[0m      146.5287  0.0005  13.6274\n",
      "     14      \u001b[36m110.4878\u001b[0m      119.9527  0.0010  13.4522\n",
      "     15      \u001b[36m139.1556\u001b[0m      138.2316  0.0005  12.3570\n",
      "     15      \u001b[36m127.6987\u001b[0m      147.5550  0.0005  12.1750\n",
      "     15      \u001b[36m104.8699\u001b[0m      125.9769  0.0010  12.0128\n",
      "     16      \u001b[36m138.1160\u001b[0m      145.7338  0.0005  11.8671\n",
      "     16      116.3839      127.8737  0.0005  12.1551\n",
      "     16      \u001b[36m125.0564\u001b[0m      150.1513  0.0005  12.2477\n",
      "     17      \u001b[36m136.5468\u001b[0m      156.0105  0.0005  11.4539\n",
      "     17      124.6566      \u001b[32m117.4156\u001b[0m  0.0005  11.4534\n",
      "     17      129.1627      144.2184  0.0003  11.4332\n",
      "     18      142.1830      123.1642  0.0003  11.4286\n",
      "     18      116.1441      \u001b[32m115.7990\u001b[0m  0.0005  11.5841\n",
      "     18      131.4540      150.5934  0.0003  11.6348\n",
      "     19      141.9290      124.1816  0.0003  12.1781\n",
      "     19      113.9773      \u001b[32m113.6878\u001b[0m  0.0005  12.6884\n",
      "     19      130.7673      153.7514  0.0003  12.6719\n",
      "     20      140.3256      123.2150  0.0003  13.1349\n",
      "     20      111.6987      \u001b[32m113.3111\u001b[0m  0.0005  13.0033\n",
      "     20      129.6098      153.4353  0.0003  13.1373\n",
      "     21      138.5458      122.7319  0.0003  12.0674\n",
      "     21      109.2259      \u001b[32m110.7101\u001b[0m  0.0005  12.0660\n",
      "     21      132.9765      \u001b[32m124.8128\u001b[0m  0.0001  12.0171\n",
      "     22      140.2374      125.6407  0.0001  11.9406\n",
      "     22      106.0143      \u001b[32m110.3215\u001b[0m  0.0005  12.0971\n",
      "     22      \u001b[36m118.6468\u001b[0m      \u001b[32m123.0215\u001b[0m  0.0001  12.3353\n",
      "     23      \u001b[36m102.9409\u001b[0m      110.8808  0.0005  11.8255\n",
      "     23      \u001b[36m117.2531\u001b[0m      123.0795  0.0001  11.7085\n",
      "     24       \u001b[36m99.9609\u001b[0m      111.4918  0.0005  10.2844\n",
      "     24      \u001b[36m115.8471\u001b[0m      123.2778  0.0001  10.3897\n",
      "     25       \u001b[36m96.0932\u001b[0m      113.8886  0.0005  9.9214\n",
      "     25      \u001b[36m114.5489\u001b[0m      123.4739  0.0001  10.0378\n",
      "     26       \u001b[36m93.8248\u001b[0m      113.7180  0.0005  9.8933\n",
      "     26      \u001b[36m113.3610\u001b[0m      123.6717  0.0001  10.0478\n",
      "     27      106.2938      132.3175  0.0003  10.1849\n",
      "     27      \u001b[36m111.5903\u001b[0m      147.7345  0.0001  10.2469\n",
      "     28      103.4070      126.2022  0.0003  9.8933\n",
      "     28      \u001b[36m107.7993\u001b[0m      144.8521  0.0001  10.0370\n",
      "     29       99.5720      137.4297  0.0003  10.2635\n",
      "     29      \u001b[36m106.6416\u001b[0m      141.7951  0.0001  10.3277\n",
      "     30       99.3771      153.9985  0.0003  10.1346\n",
      "     30      \u001b[36m105.8769\u001b[0m      139.6274  0.0001  10.1470\n",
      "     31      106.9268      125.6265  0.0001  10.2223\n",
      "     31      \u001b[36m103.4313\u001b[0m      138.5633  0.0000  10.3102\n",
      "RMSE for each folder: [11.84212014 15.36254378 11.58963046  9.84877608  9.72626717]\n",
      "RMSE mean: 11.673867526592275\n"
     ]
    }
   ],
   "source": [
    "#Free\n",
    "cross_validation_with_scores(x_f,y_f,groups_f,pipe_cnn,logo,rmse_scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abdb180-6ce2-42f5-8745-04c47650ba73",
   "metadata": {},
   "source": [
    "The mean rmse for this cnn was better than that of the covariance pipeline, but it takes a really long time to execute compared to that method. If only I could have the benefits of both approaches…\n",
    "\n",
    "It was with this train of thought that I created a hybrid between the cnn and the covariance matrice method. For this hybrid, I didn’t need a CNN because the data after the covariance matrice step was  small, so I just built a two-layer nn and the results are very close to the cnn but it runs much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e5ee8f6-481f-4723-bd8c-96a023576f07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "#Small nn\n",
    "class Cov_nn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.s = nn.Sequential(\n",
    "          nn.Linear(36, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(128, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(64, 51)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.s(x)\n",
    "\n",
    "net_nn2 = NeuralNetRegressor(\n",
    "    module      = Cov_nn,\n",
    "    max_epochs  = 200,\n",
    "    lr          = 1e-3,\n",
    "    batch_size  = 64,\n",
    "    optimizer   = torch.optim.Adam,\n",
    "    callbacks=[('earlystop', EarlyStopping('valid_loss', patience=5)), # Stop if validation loss doesn't improve for 5 epochs \n",
    "                    ('lr_sched', LRScheduler(\n",
    "           policy=torch.optim.lr_scheduler.ReduceLROnPlateau, # Halve LR if validation loss stalls for 3 epochs\n",
    "           monitor='valid_loss',\n",
    "           patience=3, factor=0.5))]\n",
    ")\n",
    "\n",
    "\n",
    "pipe_fusion = Pipeline([\n",
    "    ('cov',   Covariances(estimator='oas')),   \n",
    "    ('ts',    TangentSpace()),                \n",
    "    ('scale', StandardScaler()),               \n",
    "    ('cast',  FunctionTransformer(lambda X: X.astype(np.float32), validate=False)), # cast to float32 for PyTorch compatibility\n",
    "    ('nn',   net_nn2)                          \n",
    "])\n",
    "\n",
    "x_cov_nn = X_g_train_reshape.astype('float32')\n",
    "x_cov_nn_f = X_f_train_reshape.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77ca50b6-460a-4868-a4a4-09c76ec4f6ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m466.5164\u001b[0m      \u001b[32m390.7233\u001b[0m  0.0010  0.1491\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m456.2672\u001b[0m      \u001b[32m371.3561\u001b[0m  0.0010  0.1398\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m467.6991\u001b[0m      \u001b[32m394.0695\u001b[0m  0.0010  0.1163\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m465.8284\u001b[0m      \u001b[32m398.2766\u001b[0m  0.0010  0.1318\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m465.8373\u001b[0m      \u001b[32m391.6765\u001b[0m  0.0010  0.1354\n",
      "      2      \u001b[36m279.8029\u001b[0m      \u001b[32m182.5658\u001b[0m  0.0010  0.1208\n",
      "      2      \u001b[36m255.5881\u001b[0m      \u001b[32m172.4555\u001b[0m  0.0010  0.1300\n",
      "      2      \u001b[36m285.8089\u001b[0m      \u001b[32m182.1975\u001b[0m  0.0010  0.1364\n",
      "      2      \u001b[36m286.5691\u001b[0m      \u001b[32m178.1627\u001b[0m  0.0010  0.1336\n",
      "      2      \u001b[36m274.4942\u001b[0m      \u001b[32m183.1510\u001b[0m  0.0010  0.1405\n",
      "      3      \u001b[36m171.8439\u001b[0m      \u001b[32m140.6396\u001b[0m  0.0010  0.1371\n",
      "      3      \u001b[36m163.6931\u001b[0m      \u001b[32m137.6804\u001b[0m  0.0010  0.1298\n",
      "      3      \u001b[36m167.1673\u001b[0m      \u001b[32m134.1875\u001b[0m  0.0010  0.1412\n",
      "      3      \u001b[36m166.5601\u001b[0m      \u001b[32m131.5781\u001b[0m  0.0010  0.1359\n",
      "      4      \u001b[36m145.7358\u001b[0m      \u001b[32m130.2366\u001b[0m  0.0010  0.1239\n",
      "      3      \u001b[36m167.7696\u001b[0m      \u001b[32m138.0527\u001b[0m  0.0010  0.1423\n",
      "      4      \u001b[36m140.0734\u001b[0m      \u001b[32m125.0197\u001b[0m  0.0010  0.1272\n",
      "      4      \u001b[36m137.7875\u001b[0m      \u001b[32m117.1788\u001b[0m  0.0010  0.1257\n",
      "      4      \u001b[36m139.7615\u001b[0m      \u001b[32m119.5270\u001b[0m  0.0010  0.1403\n",
      "      5      \u001b[36m129.6100\u001b[0m      \u001b[32m118.1689\u001b[0m  0.0010  0.1321\n",
      "      5      \u001b[36m122.9533\u001b[0m      \u001b[32m112.0168\u001b[0m  0.0010  0.1234\n",
      "      4      \u001b[36m142.5954\u001b[0m      \u001b[32m124.1183\u001b[0m  0.0010  0.1390\n",
      "      5      \u001b[36m119.1720\u001b[0m      \u001b[32m102.1076\u001b[0m  0.0010  0.1529\n",
      "      5      \u001b[36m127.8525\u001b[0m      \u001b[32m108.6521\u001b[0m  0.0010  0.1406\n",
      "      6      \u001b[36m107.5524\u001b[0m      \u001b[32m103.1266\u001b[0m  0.0010  0.1318\n",
      "      6      \u001b[36m114.5203\u001b[0m      \u001b[32m110.5554\u001b[0m  0.0010  0.1423\n",
      "      5      \u001b[36m125.6365\u001b[0m      \u001b[32m112.9581\u001b[0m  0.0010  0.1475\n",
      "      6      \u001b[36m116.6872\u001b[0m       \u001b[32m99.4301\u001b[0m  0.0010  0.1305\n",
      "      6      \u001b[36m105.6328\u001b[0m       \u001b[32m90.8202\u001b[0m  0.0010  0.1477\n",
      "      7      \u001b[36m103.9281\u001b[0m      \u001b[32m100.8271\u001b[0m  0.0010  0.1275\n",
      "      7       \u001b[36m99.4159\u001b[0m       \u001b[32m97.1148\u001b[0m  0.0010  0.1439\n",
      "      6      \u001b[36m112.6993\u001b[0m      \u001b[32m104.2749\u001b[0m  0.0010  0.1402\n",
      "      7      \u001b[36m105.6514\u001b[0m       \u001b[32m91.8867\u001b[0m  0.0010  0.1346\n",
      "      7       \u001b[36m96.6108\u001b[0m       \u001b[32m82.8194\u001b[0m  0.0010  0.1409\n",
      "      8       \u001b[36m97.0088\u001b[0m       \u001b[32m95.5637\u001b[0m  0.0010  0.1405\n",
      "      8       \u001b[36m92.7170\u001b[0m       \u001b[32m91.2554\u001b[0m  0.0010  0.1435\n",
      "      7      \u001b[36m101.6645\u001b[0m       \u001b[32m96.8175\u001b[0m  0.0010  0.1361\n",
      "      8       \u001b[36m98.3476\u001b[0m       \u001b[32m84.9794\u001b[0m  0.0010  0.1268\n",
      "      8       \u001b[36m89.6124\u001b[0m       \u001b[32m76.0321\u001b[0m  0.0010  0.1308\n",
      "      9       \u001b[36m91.1243\u001b[0m       \u001b[32m90.6318\u001b[0m  0.0010  0.1085\n",
      "      9       \u001b[36m88.8845\u001b[0m       \u001b[32m86.6973\u001b[0m  0.0010  0.1221\n",
      "      8       \u001b[36m94.5065\u001b[0m       \u001b[32m88.1330\u001b[0m  0.0010  0.1220\n",
      "      9       \u001b[36m93.6122\u001b[0m       \u001b[32m80.1232\u001b[0m  0.0010  0.1248\n",
      "      9       \u001b[36m83.6094\u001b[0m       \u001b[32m71.7771\u001b[0m  0.0010  0.1344\n",
      "     10       \u001b[36m86.9248\u001b[0m       \u001b[32m83.2175\u001b[0m  0.0010  0.1115\n",
      "     10       \u001b[36m84.0075\u001b[0m       \u001b[32m80.9731\u001b[0m  0.0010  0.1180\n",
      "      9       \u001b[36m88.9773\u001b[0m       \u001b[32m83.0887\u001b[0m  0.0010  0.1295\n",
      "     10       \u001b[36m89.8334\u001b[0m       \u001b[32m74.5951\u001b[0m  0.0010  0.1339\n",
      "     10       \u001b[36m79.2145\u001b[0m       \u001b[32m68.5040\u001b[0m  0.0010  0.1134\n",
      "     11       \u001b[36m83.1105\u001b[0m       \u001b[32m79.8113\u001b[0m  0.0010  0.1034\n",
      "     11       \u001b[36m82.7533\u001b[0m       \u001b[32m77.6397\u001b[0m  0.0010  0.1296\n",
      "     10       \u001b[36m83.7433\u001b[0m       \u001b[32m78.7274\u001b[0m  0.0010  0.1243\n",
      "     11       \u001b[36m76.7056\u001b[0m       \u001b[32m64.9097\u001b[0m  0.0010  0.1183\n",
      "     11       \u001b[36m85.2271\u001b[0m       \u001b[32m69.7288\u001b[0m  0.0010  0.1292\n",
      "     12       \u001b[36m78.5113\u001b[0m       \u001b[32m74.7660\u001b[0m  0.0010  0.1451\n",
      "     11       \u001b[36m80.9565\u001b[0m       \u001b[32m74.8061\u001b[0m  0.0010  0.1242\n",
      "     12       \u001b[36m77.7530\u001b[0m       \u001b[32m72.7453\u001b[0m  0.0010  0.1354\n",
      "     12       \u001b[36m79.9562\u001b[0m       \u001b[32m65.3478\u001b[0m  0.0010  0.1039\n",
      "     12       \u001b[36m71.2722\u001b[0m       \u001b[32m61.9949\u001b[0m  0.0010  0.1133\n",
      "     13       \u001b[36m76.9786\u001b[0m       \u001b[32m70.9217\u001b[0m  0.0010  0.1215\n",
      "     12       \u001b[36m76.4633\u001b[0m       \u001b[32m70.2127\u001b[0m  0.0010  0.1709\n",
      "     13       \u001b[36m72.5431\u001b[0m       \u001b[32m70.1280\u001b[0m  0.0010  0.1886\n",
      "     13       \u001b[36m68.3044\u001b[0m       \u001b[32m60.0411\u001b[0m  0.0010  0.1832\n",
      "     13       \u001b[36m74.5457\u001b[0m       \u001b[32m61.2177\u001b[0m  0.0010  0.1856\n",
      "     14       \u001b[36m73.8394\u001b[0m       \u001b[32m68.6271\u001b[0m  0.0010  0.1914\n",
      "     13       \u001b[36m72.3971\u001b[0m       \u001b[32m67.3689\u001b[0m  0.0010  0.1105\n",
      "     14       \u001b[36m69.6941\u001b[0m       \u001b[32m66.4475\u001b[0m  0.0010  0.1148\n",
      "     14       \u001b[36m70.7478\u001b[0m       \u001b[32m57.9507\u001b[0m  0.0010  0.0953\n",
      "     14       \u001b[36m67.2953\u001b[0m       \u001b[32m57.2477\u001b[0m  0.0010  0.1233\n",
      "     15       \u001b[36m70.4065\u001b[0m       \u001b[32m65.8814\u001b[0m  0.0010  0.1185\n",
      "     14       \u001b[36m69.4786\u001b[0m       \u001b[32m62.3641\u001b[0m  0.0010  0.1144\n",
      "     15       \u001b[36m67.4774\u001b[0m       \u001b[32m54.6275\u001b[0m  0.0010  0.1189\n",
      "     15       \u001b[36m66.2293\u001b[0m       \u001b[32m63.9902\u001b[0m  0.0010  0.1288\n",
      "     15       \u001b[36m63.8021\u001b[0m       \u001b[32m55.0015\u001b[0m  0.0010  0.1304\n",
      "     15       \u001b[36m66.5703\u001b[0m       \u001b[32m61.4485\u001b[0m  0.0010  0.0928\n",
      "     16       \u001b[36m68.5533\u001b[0m       \u001b[32m61.2289\u001b[0m  0.0010  0.1267\n",
      "     16       \u001b[36m64.3248\u001b[0m       \u001b[32m51.8575\u001b[0m  0.0010  0.1314\n",
      "     16       \u001b[36m64.3881\u001b[0m       \u001b[32m61.5520\u001b[0m  0.0010  0.1378\n",
      "     16       \u001b[36m62.6618\u001b[0m       \u001b[32m52.9276\u001b[0m  0.0010  0.1117\n",
      "     16       \u001b[36m63.2682\u001b[0m       \u001b[32m58.4496\u001b[0m  0.0010  0.1060\n",
      "     17       \u001b[36m63.9772\u001b[0m       \u001b[32m60.0176\u001b[0m  0.0010  0.1181\n",
      "     17       \u001b[36m62.4258\u001b[0m       \u001b[32m58.4451\u001b[0m  0.0010  0.1034\n",
      "     17       \u001b[36m60.9823\u001b[0m       \u001b[32m49.0023\u001b[0m  0.0010  0.1193\n",
      "     17       \u001b[36m60.5403\u001b[0m       \u001b[32m51.1298\u001b[0m  0.0010  0.1100\n",
      "     17       \u001b[36m62.6651\u001b[0m       \u001b[32m56.4771\u001b[0m  0.0010  0.1027\n",
      "     18       64.5188       \u001b[32m58.2678\u001b[0m  0.0010  0.1163\n",
      "     18       \u001b[36m60.4703\u001b[0m       \u001b[32m57.7619\u001b[0m  0.0010  0.1015\n",
      "     18       \u001b[36m57.8528\u001b[0m       \u001b[32m49.7122\u001b[0m  0.0010  0.1064\n",
      "     18       \u001b[36m60.4335\u001b[0m       \u001b[32m54.8785\u001b[0m  0.0010  0.1048\n",
      "     18       \u001b[36m60.9190\u001b[0m       \u001b[32m47.2742\u001b[0m  0.0010  0.1279\n",
      "     19       \u001b[36m59.3567\u001b[0m       \u001b[32m55.5926\u001b[0m  0.0010  0.0927\n",
      "     19       \u001b[36m63.4488\u001b[0m       \u001b[32m56.1731\u001b[0m  0.0010  0.1362\n",
      "     19       \u001b[36m57.7306\u001b[0m       \u001b[32m53.1869\u001b[0m  0.0010  0.1089\n",
      "     19       \u001b[36m55.3087\u001b[0m       \u001b[32m48.2508\u001b[0m  0.0010  0.1161\n",
      "     19       \u001b[36m58.2245\u001b[0m       \u001b[32m46.4074\u001b[0m  0.0010  0.1276\n",
      "     20       \u001b[36m59.5184\u001b[0m       56.4998  0.0010  0.1031\n",
      "     20       \u001b[36m57.0558\u001b[0m       \u001b[32m54.0971\u001b[0m  0.0010  0.1295\n",
      "     20       58.8100       \u001b[32m51.0561\u001b[0m  0.0010  0.1165\n",
      "     20       \u001b[36m53.4256\u001b[0m       \u001b[32m45.7453\u001b[0m  0.0010  0.1156\n",
      "     20       \u001b[36m57.1792\u001b[0m       \u001b[32m45.0715\u001b[0m  0.0010  0.1102\n",
      "     21       \u001b[36m59.0503\u001b[0m       \u001b[32m52.5540\u001b[0m  0.0010  0.1019\n",
      "     21       \u001b[36m56.1370\u001b[0m       54.2688  0.0010  0.1081\n",
      "     21       \u001b[36m53.0142\u001b[0m       46.0677  0.0010  0.1099\n",
      "     21       \u001b[36m57.6159\u001b[0m       \u001b[32m50.0500\u001b[0m  0.0010  0.1175\n",
      "     21       \u001b[36m57.0474\u001b[0m       \u001b[32m43.5555\u001b[0m  0.0010  0.1275\n",
      "     22       \u001b[36m58.7250\u001b[0m       \u001b[32m50.5665\u001b[0m  0.0010  0.1263\n",
      "     22       \u001b[36m54.0021\u001b[0m       \u001b[32m52.1105\u001b[0m  0.0010  0.1223\n",
      "     22       \u001b[36m50.9726\u001b[0m       \u001b[32m43.8362\u001b[0m  0.0010  0.1201\n",
      "     22       \u001b[36m55.9698\u001b[0m       \u001b[32m49.0122\u001b[0m  0.0010  0.1244\n",
      "     22       \u001b[36m55.5513\u001b[0m       \u001b[32m41.6409\u001b[0m  0.0010  0.1215\n",
      "     23       \u001b[36m56.2505\u001b[0m       \u001b[32m50.5085\u001b[0m  0.0010  0.1221\n",
      "     23       54.5593       \u001b[32m51.8748\u001b[0m  0.0010  0.1289\n",
      "     23       51.3353       \u001b[32m41.1637\u001b[0m  0.0010  0.1371\n",
      "     23       \u001b[36m55.0953\u001b[0m       \u001b[32m48.5124\u001b[0m  0.0010  0.1371\n",
      "     23       \u001b[36m52.4932\u001b[0m       \u001b[32m40.4988\u001b[0m  0.0010  0.1456\n",
      "     24       \u001b[36m55.1359\u001b[0m       \u001b[32m48.9076\u001b[0m  0.0010  0.1295\n",
      "     24       \u001b[36m53.7146\u001b[0m       \u001b[32m50.5256\u001b[0m  0.0010  0.1361\n",
      "     24       \u001b[36m48.6872\u001b[0m       41.6209  0.0010  0.1281\n",
      "     24       \u001b[36m52.5252\u001b[0m       \u001b[32m46.5245\u001b[0m  0.0010  0.1371\n",
      "     24       \u001b[36m51.7905\u001b[0m       \u001b[32m39.2521\u001b[0m  0.0010  0.1250\n",
      "     25       \u001b[36m54.6790\u001b[0m       \u001b[32m47.5799\u001b[0m  0.0010  0.1336\n",
      "     25       \u001b[36m52.3431\u001b[0m       \u001b[32m49.7286\u001b[0m  0.0010  0.1228\n",
      "     25       \u001b[36m47.9446\u001b[0m       \u001b[32m41.0252\u001b[0m  0.0010  0.1389\n",
      "     25       \u001b[36m51.7358\u001b[0m       \u001b[32m45.1293\u001b[0m  0.0010  0.1453\n",
      "     25       \u001b[36m51.6150\u001b[0m       \u001b[32m38.1502\u001b[0m  0.0010  0.1296\n",
      "     26       \u001b[36m52.7794\u001b[0m       \u001b[32m46.2492\u001b[0m  0.0010  0.1303\n",
      "     26       \u001b[36m50.7352\u001b[0m       \u001b[32m48.7548\u001b[0m  0.0010  0.1218\n",
      "     26       \u001b[36m46.4937\u001b[0m       \u001b[32m39.6201\u001b[0m  0.0010  0.1392\n",
      "     26       \u001b[36m49.8892\u001b[0m       \u001b[32m36.6392\u001b[0m  0.0010  0.1241\n",
      "     26       \u001b[36m50.6300\u001b[0m       \u001b[32m43.3563\u001b[0m  0.0010  0.1324\n",
      "     27       \u001b[36m50.7991\u001b[0m       \u001b[32m45.2482\u001b[0m  0.0010  0.1455\n",
      "     27       \u001b[36m50.2670\u001b[0m       \u001b[32m47.8350\u001b[0m  0.0010  0.1331\n",
      "     27       47.1253       \u001b[32m37.2385\u001b[0m  0.0010  0.1325\n",
      "     27       \u001b[36m50.3597\u001b[0m       \u001b[32m42.8808\u001b[0m  0.0010  0.1179\n",
      "     27       \u001b[36m49.1064\u001b[0m       \u001b[32m35.2053\u001b[0m  0.0010  0.1260\n",
      "     28       \u001b[36m47.7103\u001b[0m       \u001b[32m46.9395\u001b[0m  0.0010  0.1024\n",
      "     28       50.8048       \u001b[32m43.3260\u001b[0m  0.0010  0.1177\n",
      "     28       \u001b[36m48.2624\u001b[0m       \u001b[32m41.7375\u001b[0m  0.0010  0.1307\n",
      "     28       \u001b[36m44.4229\u001b[0m       \u001b[32m36.6279\u001b[0m  0.0010  0.1501\n",
      "     28       \u001b[36m47.9506\u001b[0m       \u001b[32m34.1044\u001b[0m  0.0010  0.1429\n",
      "     29       48.2462       \u001b[32m46.6077\u001b[0m  0.0010  0.1296\n",
      "     29       \u001b[36m50.6139\u001b[0m       \u001b[32m41.7437\u001b[0m  0.0010  0.1253\n",
      "     29       \u001b[36m46.7313\u001b[0m       \u001b[32m40.4428\u001b[0m  0.0010  0.1746\n",
      "     29       \u001b[36m43.9022\u001b[0m       37.2581  0.0010  0.1908\n",
      "     29       \u001b[36m45.4040\u001b[0m       \u001b[32m33.0731\u001b[0m  0.0010  0.2179\n",
      "     30       \u001b[36m49.4120\u001b[0m       \u001b[32m41.4122\u001b[0m  0.0010  0.1960\n",
      "     30       \u001b[36m45.6761\u001b[0m       \u001b[32m44.2651\u001b[0m  0.0010  0.2185\n",
      "     30       47.8207       \u001b[32m39.1885\u001b[0m  0.0010  0.1432\n",
      "     30       \u001b[36m43.7557\u001b[0m       36.6712  0.0010  0.1412\n",
      "     30       46.1447       \u001b[32m32.1354\u001b[0m  0.0010  0.1279\n",
      "     31       \u001b[36m48.5199\u001b[0m       \u001b[32m40.8428\u001b[0m  0.0010  0.1335\n",
      "     31       \u001b[36m44.5513\u001b[0m       44.3703  0.0010  0.1240\n",
      "     31       \u001b[36m46.3277\u001b[0m       \u001b[32m38.0809\u001b[0m  0.0010  0.1353\n",
      "     31       \u001b[36m44.4368\u001b[0m       \u001b[32m31.5873\u001b[0m  0.0010  0.1113\n",
      "     31       \u001b[36m42.4654\u001b[0m       \u001b[32m34.6186\u001b[0m  0.0010  0.1404\n",
      "     32       \u001b[36m48.4810\u001b[0m       41.6653  0.0010  0.1199\n",
      "     32       \u001b[36m44.5410\u001b[0m       \u001b[32m43.1039\u001b[0m  0.0010  0.1205\n",
      "     32       \u001b[36m46.2640\u001b[0m       \u001b[32m37.4054\u001b[0m  0.0010  0.1086\n",
      "     33       \u001b[36m47.8007\u001b[0m       \u001b[32m38.3887\u001b[0m  0.0010  0.0920\n",
      "     32       \u001b[36m43.9000\u001b[0m       \u001b[32m30.9285\u001b[0m  0.0010  0.1088\n",
      "     33       45.4501       43.2576  0.0010  0.0921\n",
      "     32       \u001b[36m41.0542\u001b[0m       \u001b[32m33.7215\u001b[0m  0.0010  0.1318\n",
      "     33       \u001b[36m43.7909\u001b[0m       \u001b[32m29.7058\u001b[0m  0.0010  0.0906\n",
      "     33       \u001b[36m44.9669\u001b[0m       \u001b[32m36.2610\u001b[0m  0.0010  0.1131\n",
      "     34       \u001b[36m44.1638\u001b[0m       \u001b[32m41.8842\u001b[0m  0.0010  0.1066\n",
      "     34       \u001b[36m46.6576\u001b[0m       \u001b[32m37.0836\u001b[0m  0.0010  0.1236\n",
      "     33       42.4615       34.3993  0.0010  0.1119\n",
      "     34       \u001b[36m42.9182\u001b[0m       36.3456  0.0010  0.0972\n",
      "     34       \u001b[36m42.2526\u001b[0m       \u001b[32m29.4974\u001b[0m  0.0010  0.1215\n",
      "     35       47.9986       \u001b[32m36.2564\u001b[0m  0.0010  0.1011\n",
      "     35       \u001b[36m43.1209\u001b[0m       42.0059  0.0010  0.1156\n",
      "     34       41.2077       \u001b[32m32.3911\u001b[0m  0.0010  0.1207\n",
      "     35       \u001b[36m42.8452\u001b[0m       \u001b[32m35.5791\u001b[0m  0.0010  0.1052\n",
      "     36       \u001b[36m45.0592\u001b[0m       \u001b[32m35.4049\u001b[0m  0.0010  0.0980\n",
      "     36       \u001b[36m41.9421\u001b[0m       \u001b[32m41.2891\u001b[0m  0.0010  0.0981\n",
      "     35       \u001b[36m42.0138\u001b[0m       \u001b[32m28.9524\u001b[0m  0.0010  0.1193\n",
      "     35       \u001b[36m40.7812\u001b[0m       33.0754  0.0010  0.1225\n",
      "     36       43.0310       \u001b[32m34.8702\u001b[0m  0.0010  0.0989\n",
      "     37       45.8224       35.6503  0.0010  0.1142\n",
      "     36       \u001b[36m40.8023\u001b[0m       \u001b[32m28.6535\u001b[0m  0.0010  0.1112\n",
      "     37       \u001b[36m41.3899\u001b[0m       \u001b[32m40.5932\u001b[0m  0.0010  0.1230\n",
      "     36       40.8313       \u001b[32m31.7889\u001b[0m  0.0010  0.1168\n",
      "     37       43.5246       35.0556  0.0010  0.1317\n",
      "     38       \u001b[36m44.2561\u001b[0m       \u001b[32m34.7821\u001b[0m  0.0010  0.1237\n",
      "     38       \u001b[36m40.2196\u001b[0m       \u001b[32m39.9560\u001b[0m  0.0010  0.1123\n",
      "     37       41.8300       \u001b[32m28.0176\u001b[0m  0.0010  0.1230\n",
      "     37       \u001b[36m39.6269\u001b[0m       32.4242  0.0010  0.1031\n",
      "     38       \u001b[36m41.7055\u001b[0m       \u001b[32m33.0008\u001b[0m  0.0010  0.0947\n",
      "     39       44.5191       \u001b[32m33.5478\u001b[0m  0.0010  0.1040\n",
      "     39       40.3396       \u001b[32m39.4602\u001b[0m  0.0010  0.1093\n",
      "     38       \u001b[36m40.4097\u001b[0m       \u001b[32m27.0873\u001b[0m  0.0010  0.1111\n",
      "     38       \u001b[36m38.8954\u001b[0m       \u001b[32m31.2305\u001b[0m  0.0010  0.1089\n",
      "     39       42.3921       33.0627  0.0010  0.1266\n",
      "     40       \u001b[36m42.5680\u001b[0m       33.7183  0.0010  0.1049\n",
      "     40       \u001b[36m39.9639\u001b[0m       \u001b[32m38.5678\u001b[0m  0.0010  0.1083\n",
      "     39       \u001b[36m39.0978\u001b[0m       27.4024  0.0010  0.1148\n",
      "     39       \u001b[36m38.3659\u001b[0m       \u001b[32m30.4201\u001b[0m  0.0010  0.1279\n",
      "     40       \u001b[36m40.2141\u001b[0m       \u001b[32m32.7985\u001b[0m  0.0010  0.1264\n",
      "     41       \u001b[36m41.5311\u001b[0m       34.0721  0.0010  0.1256\n",
      "     41       40.1427       \u001b[32m38.4909\u001b[0m  0.0010  0.1542\n",
      "     40       \u001b[36m38.7845\u001b[0m       \u001b[32m25.9683\u001b[0m  0.0010  0.1399\n",
      "     40       \u001b[36m37.4355\u001b[0m       \u001b[32m29.9624\u001b[0m  0.0010  0.1322\n",
      "     41       40.7718       \u001b[32m32.1066\u001b[0m  0.0010  0.1374\n",
      "     42       \u001b[36m41.4344\u001b[0m       \u001b[32m31.8432\u001b[0m  0.0010  0.1268\n",
      "     42       \u001b[36m37.8961\u001b[0m       \u001b[32m37.2336\u001b[0m  0.0010  0.1035\n",
      "     41       39.2771       26.2789  0.0010  0.1274\n",
      "     41       \u001b[36m37.1861\u001b[0m       \u001b[32m29.8393\u001b[0m  0.0010  0.0920\n",
      "     42       40.4576       \u001b[32m31.7417\u001b[0m  0.0010  0.0951\n",
      "     43       41.8161       32.8109  0.0010  0.1309\n",
      "     43       39.6794       \u001b[32m36.8464\u001b[0m  0.0010  0.1267\n",
      "     42       \u001b[36m38.4001\u001b[0m       \u001b[32m25.9070\u001b[0m  0.0010  0.1238\n",
      "     42       37.7574       30.1900  0.0010  0.1391\n",
      "     43       \u001b[36m39.7920\u001b[0m       32.0625  0.0010  0.1320\n",
      "     44       42.3070       \u001b[32m31.0176\u001b[0m  0.0010  0.1268\n",
      "     44       \u001b[36m37.3729\u001b[0m       37.8972  0.0010  0.1238\n",
      "     43       \u001b[36m37.4042\u001b[0m       \u001b[32m25.6335\u001b[0m  0.0010  0.1034\n",
      "     43       \u001b[36m37.0110\u001b[0m       \u001b[32m29.2108\u001b[0m  0.0010  0.1208\n",
      "     44       \u001b[36m39.4482\u001b[0m       \u001b[32m31.3393\u001b[0m  0.0010  0.0985\n",
      "     44       \u001b[36m36.7082\u001b[0m       26.8340  0.0010  0.0924\n",
      "     45       37.8831       36.9061  0.0010  0.1174\n",
      "     45       \u001b[36m40.8324\u001b[0m       \u001b[32m29.2968\u001b[0m  0.0010  0.1274\n",
      "     45       40.2182       \u001b[32m30.7458\u001b[0m  0.0010  0.1143\n",
      "     44       \u001b[36m35.6150\u001b[0m       \u001b[32m28.6902\u001b[0m  0.0010  0.1283\n",
      "     45       37.1368       \u001b[32m25.5936\u001b[0m  0.0010  0.1142\n",
      "     46       \u001b[36m38.8639\u001b[0m       30.0740  0.0010  0.1148\n",
      "     46       38.2205       \u001b[32m36.2461\u001b[0m  0.0010  0.1216\n",
      "     45       \u001b[36m35.4046\u001b[0m       28.8613  0.0010  0.1174\n",
      "     46       \u001b[36m36.6725\u001b[0m       \u001b[32m29.3140\u001b[0m  0.0010  0.1420\n",
      "     46       36.8277       \u001b[32m24.8692\u001b[0m  0.0010  0.1401\n",
      "     47       38.9401       30.0362  0.0010  0.1255\n",
      "     47       37.6254       37.0258  0.0010  0.1386\n",
      "     46       35.5934       \u001b[32m28.4203\u001b[0m  0.0010  0.1267\n",
      "     47       37.4733       29.8366  0.0010  0.1256\n",
      "     47       \u001b[36m36.4751\u001b[0m       25.2140  0.0010  0.1184\n",
      "     48       \u001b[36m38.6303\u001b[0m       30.3991  0.0010  0.1207\n",
      "     48       \u001b[36m36.1972\u001b[0m       \u001b[32m34.9180\u001b[0m  0.0010  0.1230\n",
      "     47       35.7487       \u001b[32m28.3574\u001b[0m  0.0010  0.1381\n",
      "     48       36.8106       29.4502  0.0010  0.1347\n",
      "     48       \u001b[36m36.1508\u001b[0m       \u001b[32m23.9926\u001b[0m  0.0010  0.1135\n",
      "     49       38.9025       \u001b[32m27.6629\u001b[0m  0.0010  0.1366\n",
      "     49       \u001b[36m36.0736\u001b[0m       35.7288  0.0010  0.1392\n",
      "     49       36.7488       \u001b[32m29.1661\u001b[0m  0.0010  0.0982\n",
      "     48       35.6517       \u001b[32m27.3201\u001b[0m  0.0010  0.1163\n",
      "     49       \u001b[36m35.4664\u001b[0m       \u001b[32m23.6677\u001b[0m  0.0010  0.1189\n",
      "     50       \u001b[36m38.3567\u001b[0m       \u001b[32m27.6167\u001b[0m  0.0010  0.1421\n",
      "     50       \u001b[36m35.0639\u001b[0m       \u001b[32m34.4344\u001b[0m  0.0010  0.1201\n",
      "     50       \u001b[36m36.6534\u001b[0m       \u001b[32m28.8185\u001b[0m  0.0010  0.1272\n",
      "     49       35.5676       \u001b[32m27.0704\u001b[0m  0.0010  0.1240\n",
      "     50       35.8553       24.6639  0.0010  0.1345\n",
      "     51       38.4424       \u001b[32m27.5850\u001b[0m  0.0010  0.1148\n",
      "     51       36.2066       34.9891  0.0010  0.1361\n",
      "     51       \u001b[36m36.0929\u001b[0m       \u001b[32m28.4307\u001b[0m  0.0010  0.1345\n",
      "     50       \u001b[36m34.6405\u001b[0m       \u001b[32m26.6178\u001b[0m  0.0010  0.1365\n",
      "     51       \u001b[36m34.8952\u001b[0m       23.9010  0.0010  0.1456\n",
      "     52       \u001b[36m36.3098\u001b[0m       \u001b[32m26.8938\u001b[0m  0.0010  0.1299\n",
      "     52       \u001b[36m34.6875\u001b[0m       34.5933  0.0010  0.1314\n",
      "     52       36.3624       \u001b[32m28.2805\u001b[0m  0.0010  0.1191\n",
      "     51       35.5609       26.7047  0.0010  0.1213\n",
      "     52       35.0182       24.3429  0.0010  0.1319\n",
      "     53       \u001b[36m36.0096\u001b[0m       \u001b[32m26.6337\u001b[0m  0.0010  0.1176\n",
      "     53       34.8070       \u001b[32m34.1939\u001b[0m  0.0010  0.1104\n",
      "     53       36.3981       \u001b[32m27.8265\u001b[0m  0.0010  0.1270\n",
      "     52       34.6478       \u001b[32m26.0291\u001b[0m  0.0010  0.1401\n",
      "     53       \u001b[36m34.4330\u001b[0m       \u001b[32m23.0010\u001b[0m  0.0010  0.1161\n",
      "     54       35.2252       34.6380  0.0010  0.1333\n",
      "     54       36.7850       26.6881  0.0010  0.1509\n",
      "     54       \u001b[36m35.1861\u001b[0m       \u001b[32m27.7096\u001b[0m  0.0010  0.1043\n",
      "     53       \u001b[36m33.5002\u001b[0m       27.2472  0.0010  0.1134\n",
      "     54       34.8782       \u001b[32m22.1859\u001b[0m  0.0010  0.1320\n",
      "     55       \u001b[36m33.7058\u001b[0m       \u001b[32m27.2348\u001b[0m  0.0010  0.0987\n",
      "     55       36.1716       27.7121  0.0010  0.1077\n",
      "     55       34.7960       \u001b[32m33.9486\u001b[0m  0.0010  0.1279\n",
      "     54       33.9540       \u001b[32m25.4695\u001b[0m  0.0010  0.1195\n",
      "     55       \u001b[36m34.0057\u001b[0m       22.5687  0.0010  0.1645\n",
      "     56       37.6304       \u001b[32m25.6854\u001b[0m  0.0010  0.1775\n",
      "     56       34.3479       \u001b[32m27.1874\u001b[0m  0.0010  0.1932\n",
      "     56       \u001b[36m34.1915\u001b[0m       \u001b[32m32.7145\u001b[0m  0.0010  0.1758\n",
      "     55       \u001b[36m33.2236\u001b[0m       26.3003  0.0010  0.1926\n",
      "     56       34.0979       22.5564  0.0010  0.1526\n",
      "     57       \u001b[36m35.1272\u001b[0m       26.0208  0.0010  0.1175\n",
      "     57       34.3243       33.5918  0.0010  0.1252\n",
      "     57       34.1579       \u001b[32m27.0898\u001b[0m  0.0010  0.1326\n",
      "     56       \u001b[36m31.9503\u001b[0m       \u001b[32m25.0367\u001b[0m  0.0010  0.1230\n",
      "     57       \u001b[36m33.8113\u001b[0m       \u001b[32m21.9258\u001b[0m  0.0010  0.1018\n",
      "     58       36.0525       26.3557  0.0010  0.1162\n",
      "     58       \u001b[36m33.4246\u001b[0m       33.1851  0.0010  0.1006\n",
      "     58       35.6335       \u001b[32m26.8371\u001b[0m  0.0010  0.1184\n",
      "     57       33.8817       25.2183  0.0010  0.1687\n",
      "     58       \u001b[36m33.6607\u001b[0m       \u001b[32m21.7726\u001b[0m  0.0010  0.1766\n",
      "     59       36.0612       \u001b[32m24.6848\u001b[0m  0.0010  0.1700\n",
      "     59       34.0437       32.8613  0.0010  0.1720\n",
      "     59       33.9906       27.4689  0.0010  0.1733\n",
      "     58       33.5088       \u001b[32m24.8705\u001b[0m  0.0010  0.1362\n",
      "     59       \u001b[36m32.3974\u001b[0m       21.9883  0.0010  0.1201\n",
      "     60       \u001b[36m32.1681\u001b[0m       32.9941  0.0010  0.1163\n",
      "     60       \u001b[36m34.3897\u001b[0m       \u001b[32m24.0432\u001b[0m  0.0010  0.1371\n",
      "     60       \u001b[36m33.5279\u001b[0m       26.9335  0.0010  0.1197\n",
      "     59       32.5439       \u001b[32m24.6809\u001b[0m  0.0010  0.1081\n",
      "     60       33.1595       \u001b[32m21.5873\u001b[0m  0.0010  0.1170\n",
      "     61       33.1106       \u001b[32m31.7174\u001b[0m  0.0005  0.1050\n",
      "     61       34.8196       24.6146  0.0010  0.0986\n",
      "     61       \u001b[36m32.3072\u001b[0m       \u001b[32m26.4558\u001b[0m  0.0010  0.1244\n",
      "     60       32.5763       \u001b[32m24.4800\u001b[0m  0.0010  0.1301\n",
      "     62       34.4650       25.2859  0.0010  0.0978\n",
      "     61       33.5067       21.7444  0.0010  0.1114\n",
      "     62       \u001b[36m31.7280\u001b[0m       32.8129  0.0005  0.1266\n",
      "     62       33.8715       \u001b[32m25.7996\u001b[0m  0.0010  0.1203\n",
      "     61       32.2392       \u001b[32m24.3173\u001b[0m  0.0010  0.1083\n",
      "     62       32.5170       \u001b[32m20.9479\u001b[0m  0.0010  0.1135\n",
      "     63       \u001b[36m34.3268\u001b[0m       25.1545  0.0010  0.1206\n",
      "     63       32.5814       \u001b[32m31.1907\u001b[0m  0.0005  0.1133\n",
      "     63       32.4884       26.1667  0.0010  0.1284\n",
      "     62       33.2675       24.6701  0.0010  0.1278\n",
      "     63       33.7665       21.3999  0.0010  0.1136\n",
      "     64       35.4902       24.5819  0.0010  0.1357\n",
      "     64       32.5129       \u001b[32m30.7420\u001b[0m  0.0005  0.1301\n",
      "     64       32.6064       \u001b[32m25.1815\u001b[0m  0.0010  0.1834\n",
      "     63       32.9669       \u001b[32m24.1422\u001b[0m  0.0010  0.1674\n",
      "     64       32.5644       21.8971  0.0010  0.1684\n",
      "     65       31.8467       31.1407  0.0005  0.1763\n",
      "     65       32.6244       26.1736  0.0010  0.1482\n",
      "     64       \u001b[36m31.2517\u001b[0m       \u001b[32m23.7772\u001b[0m  0.0010  0.1732\n",
      "     65       32.9847       21.3394  0.0010  0.1771\n",
      "     66       32.4921       31.3007  0.0005  0.1755\n",
      "     66       \u001b[36m31.8875\u001b[0m       25.2018  0.0010  0.1705\n",
      "     65       32.9136       23.8989  0.0010  0.1758\n",
      "     66       \u001b[36m32.2659\u001b[0m       \u001b[32m20.9351\u001b[0m  0.0010  0.1744\n",
      "     67       \u001b[36m30.4896\u001b[0m       31.3032  0.0005  0.1622\n",
      "     67       \u001b[36m31.4842\u001b[0m       \u001b[32m24.9934\u001b[0m  0.0010  0.1399\n",
      "     66       32.3383       \u001b[32m23.4084\u001b[0m  0.0010  0.1159\n",
      "     67       \u001b[36m32.1650\u001b[0m       \u001b[32m20.8417\u001b[0m  0.0010  0.1375\n",
      "     68       31.4117       30.8848  0.0005  0.1328\n",
      "     68       32.1153       26.1104  0.0010  0.1127\n",
      "     67       31.5337       24.4100  0.0010  0.1056\n",
      "     68       \u001b[36m31.6898\u001b[0m       \u001b[32m20.3876\u001b[0m  0.0010  0.1229\n",
      "     69       31.5426       \u001b[32m30.6293\u001b[0m  0.0003  0.1080\n",
      "     69       \u001b[36m31.0674\u001b[0m       \u001b[32m24.7139\u001b[0m  0.0010  0.1215\n",
      "     68       \u001b[36m30.5368\u001b[0m       \u001b[32m22.4428\u001b[0m  0.0010  0.1177\n",
      "     69       31.7574       20.7619  0.0010  0.1276\n",
      "     70       30.9530       \u001b[32m30.5892\u001b[0m  0.0003  0.1344\n",
      "     70       31.4800       24.9514  0.0010  0.0967\n",
      "     69       31.3823       23.3241  0.0010  0.1086\n",
      "     70       \u001b[36m31.2933\u001b[0m       \u001b[32m20.3808\u001b[0m  0.0010  0.0898\n",
      "     71       31.3427       30.8185  0.0003  0.0896\n",
      "     71       31.3346       \u001b[32m24.1822\u001b[0m  0.0010  0.1143\n",
      "     70       \u001b[36m30.5090\u001b[0m       23.0879  0.0010  0.1018\n",
      "     71       \u001b[36m31.2151\u001b[0m       20.5118  0.0010  0.0953\n",
      "     72       30.6481       \u001b[32m30.4918\u001b[0m  0.0003  0.0923\n",
      "     72       31.7002       24.3895  0.0010  0.0878\n",
      "     71       \u001b[36m30.3564\u001b[0m       23.7691  0.0010  0.0998\n",
      "     73       31.6152       30.6012  0.0003  0.0872\n",
      "     72       31.4741       \u001b[32m19.4258\u001b[0m  0.0010  0.1050\n",
      "     73       \u001b[36m30.5841\u001b[0m       \u001b[32m23.8080\u001b[0m  0.0010  0.0955\n",
      "     72       \u001b[36m30.0008\u001b[0m       22.7821  0.0010  0.1137\n",
      "     74       30.5425       31.3164  0.0003  0.0957\n",
      "     73       31.6597       19.6330  0.0010  0.1115\n",
      "     74       \u001b[36m30.2255\u001b[0m       24.2693  0.0010  0.0997\n",
      "     73       \u001b[36m29.7344\u001b[0m       \u001b[32m22.4387\u001b[0m  0.0005  0.1035\n",
      "     75       30.7542       \u001b[32m30.3946\u001b[0m  0.0003  0.1324\n",
      "     74       \u001b[36m30.5767\u001b[0m       \u001b[32m19.2294\u001b[0m  0.0010  0.1177\n",
      "     75       30.3549       24.2715  0.0010  0.1225\n",
      "     74       \u001b[36m29.2940\u001b[0m       22.8812  0.0005  0.1145\n",
      "     76       \u001b[36m30.3517\u001b[0m       \u001b[32m30.2786\u001b[0m  0.0003  0.0942\n",
      "     75       30.9472       20.2446  0.0010  0.1120\n",
      "     76       30.3099       23.9199  0.0010  0.1041\n",
      "     75       29.4383       22.6714  0.0005  0.1014\n",
      "     76       \u001b[36m29.9689\u001b[0m       19.8615  0.0010  0.0902\n",
      "     77       \u001b[36m30.2435\u001b[0m       \u001b[32m30.1410\u001b[0m  0.0003  0.1125\n",
      "     77       30.2642       23.9114  0.0010  0.1027\n",
      "     76       29.4874       \u001b[32m21.8141\u001b[0m  0.0005  0.0858\n",
      "     77       31.5994       19.4703  0.0010  0.0930\n",
      "     78       30.8028       30.2023  0.0003  0.1008\n",
      "     78       \u001b[36m30.0667\u001b[0m       \u001b[32m23.5166\u001b[0m  0.0005  0.1111\n",
      "     77       30.4486       22.6298  0.0005  0.0962\n",
      "     79       30.7873       30.6429  0.0003  0.0850\n",
      "     78       30.4027       \u001b[32m19.1259\u001b[0m  0.0010  0.1098\n",
      "     79       30.8468       23.5453  0.0005  0.0902\n",
      "     78       \u001b[36m28.7098\u001b[0m       \u001b[32m21.7869\u001b[0m  0.0005  0.0840\n",
      "     80       \u001b[36m28.7919\u001b[0m       30.3281  0.0003  0.0983\n",
      "     79       \u001b[36m29.6082\u001b[0m       19.3925  0.0010  0.0961\n",
      "     80       \u001b[36m28.1939\u001b[0m       \u001b[32m23.3912\u001b[0m  0.0005  0.1006\n",
      "     79       29.2940       21.8847  0.0005  0.1054\n",
      "     81       29.7585       30.4101  0.0003  0.0974\n",
      "     80       29.6278       \u001b[32m18.8882\u001b[0m  0.0010  0.1035\n",
      "     81       29.8393       \u001b[32m23.3832\u001b[0m  0.0005  0.0957\n",
      "     80       28.8122       22.1547  0.0005  0.1063\n",
      "     81       \u001b[36m29.1013\u001b[0m       \u001b[32m18.7697\u001b[0m  0.0010  0.0926\n",
      "     81       \u001b[36m28.6067\u001b[0m       \u001b[32m21.6985\u001b[0m  0.0005  0.0835\n",
      "     82       30.2796       23.5939  0.0005  0.1081\n",
      "     82       29.3520       \u001b[32m18.7269\u001b[0m  0.0010  0.1000\n",
      "     83       29.4015       \u001b[32m23.1364\u001b[0m  0.0005  0.0828\n",
      "     82       28.6079       \u001b[32m21.4079\u001b[0m  0.0005  0.1000\n",
      "     83       29.9527       19.0579  0.0010  0.1173\n",
      "     84       29.5426       \u001b[32m23.0680\u001b[0m  0.0005  0.1105\n",
      "     83       \u001b[36m28.4004\u001b[0m       21.5536  0.0005  0.1384\n",
      "     84       30.3332       19.0303  0.0010  0.1052\n",
      "     85       28.8380       23.2777  0.0005  0.1108\n",
      "     84       29.8727       21.6485  0.0005  0.1258\n",
      "     85       \u001b[36m28.7195\u001b[0m       18.7778  0.0010  0.0878\n",
      "     86       29.6374       \u001b[32m22.7571\u001b[0m  0.0005  0.1129\n",
      "     85       28.6667       21.4457  0.0005  0.1529\n",
      "     86       \u001b[36m28.6536\u001b[0m       \u001b[32m18.6995\u001b[0m  0.0010  0.1718\n",
      "     87       28.3129       \u001b[32m22.7317\u001b[0m  0.0005  0.1564\n",
      "     86       28.4052       \u001b[32m21.1217\u001b[0m  0.0005  0.0907\n",
      "     87       29.4063       \u001b[32m18.6694\u001b[0m  0.0010  0.1058\n",
      "     88       29.3558       \u001b[32m22.5343\u001b[0m  0.0005  0.0880\n",
      "     87       28.7683       \u001b[32m21.0762\u001b[0m  0.0005  0.1006\n",
      "     88       29.2670       18.8512  0.0010  0.0975\n",
      "     89       29.4462       \u001b[32m22.4191\u001b[0m  0.0005  0.0863\n",
      "     88       \u001b[36m27.6709\u001b[0m       \u001b[32m20.9885\u001b[0m  0.0005  0.0701\n",
      "     89       29.0478       19.4018  0.0010  0.0774\n",
      "     90       29.4807       22.7277  0.0005  0.0757\n",
      "     89       27.9170       \u001b[32m20.6567\u001b[0m  0.0005  0.0714\n",
      "     90       29.1487       \u001b[32m18.5526\u001b[0m  0.0010  0.0831\n",
      "     91       28.4369       \u001b[32m22.2924\u001b[0m  0.0005  0.0916\n",
      "     90       28.2344       21.0133  0.0005  0.0873\n",
      "     92       28.8875       22.6828  0.0005  0.0693\n",
      "     91       28.8761       \u001b[32m18.0518\u001b[0m  0.0010  0.0786\n",
      "     91       28.6313       20.9471  0.0005  0.0767\n",
      "     93       \u001b[36m27.8606\u001b[0m       \u001b[32m22.2920\u001b[0m  0.0005  0.0727\n",
      "     92       28.9098       18.2016  0.0010  0.0803\n",
      "     92       28.5672       20.9949  0.0005  0.0901\n",
      "     94       28.2182       \u001b[32m22.0636\u001b[0m  0.0005  0.0771\n",
      "     93       \u001b[36m28.4167\u001b[0m       18.1756  0.0010  0.0955\n",
      "     93       28.3931       21.3114  0.0005  0.0865\n",
      "     95       28.5489       22.6008  0.0005  0.0846\n",
      "     94       29.2728       \u001b[32m18.0006\u001b[0m  0.0010  0.0864\n",
      "     96       28.6128       22.0652  0.0005  0.0833\n",
      "     95       \u001b[36m28.1838\u001b[0m       \u001b[32m17.6752\u001b[0m  0.0010  0.0868\n",
      "     97       29.4020       22.1941  0.0005  0.1065\n",
      "     96       28.6467       18.5328  0.0010  0.0898\n",
      "     98       \u001b[36m27.6436\u001b[0m       22.3264  0.0005  0.0863\n",
      "     97       29.2398       18.1891  0.0010  0.0793\n",
      "     98       \u001b[36m26.9645\u001b[0m       17.9528  0.0010  0.0983\n",
      "     99       27.6152       17.7154  0.0010  0.1000\n",
      "RMSE for each folder: [5.92115703 5.07397028 5.89334359 4.60218856 4.31174496]\n",
      "RMSE mean: 5.160480883728715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m695.3553\u001b[0m      \u001b[32m455.6610\u001b[0m  0.0010  0.1177\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m684.8175\u001b[0m      \u001b[32m470.9519\u001b[0m  0.0010  0.1276\n",
      "      2      \u001b[36m373.7792\u001b[0m      \u001b[32m183.0332\u001b[0m  0.0010  0.1520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3      \u001b[36m183.2548\u001b[0m      \u001b[32m142.1072\u001b[0m  0.0010  0.1221\n",
      "      2      \u001b[36m390.4179\u001b[0m      \u001b[32m192.8287\u001b[0m  0.0010  0.1474\n",
      "      4      \u001b[36m162.4048\u001b[0m      \u001b[32m133.0991\u001b[0m  0.0010  0.1208\n",
      "      3      \u001b[36m182.7841\u001b[0m      \u001b[32m149.2148\u001b[0m  0.0010  0.1382\n",
      "      5      \u001b[36m154.4654\u001b[0m      \u001b[32m129.1941\u001b[0m  0.0010  0.1305\n",
      "      4      \u001b[36m159.5441\u001b[0m      \u001b[32m138.7199\u001b[0m  0.0010  0.1349\n",
      "      6      \u001b[36m151.6519\u001b[0m      \u001b[32m127.0925\u001b[0m  0.0010  0.1344\n",
      "      5      \u001b[36m150.8548\u001b[0m      \u001b[32m134.0723\u001b[0m  0.0010  0.1226\n",
      "      7      \u001b[36m145.9781\u001b[0m      \u001b[32m124.7601\u001b[0m  0.0010  0.1295\n",
      "      6      \u001b[36m144.1507\u001b[0m      \u001b[32m131.3970\u001b[0m  0.0010  0.1266\n",
      "      7      \u001b[36m138.9715\u001b[0m      \u001b[32m129.2382\u001b[0m  0.0010  0.1054\n",
      "      8      \u001b[36m142.8638\u001b[0m      \u001b[32m123.0975\u001b[0m  0.0010  0.1430\n",
      "      8      \u001b[36m133.9411\u001b[0m      \u001b[32m126.9947\u001b[0m  0.0010  0.1303\n",
      "      9      \u001b[36m138.3238\u001b[0m      \u001b[32m121.2700\u001b[0m  0.0010  0.1341\n",
      "      9      \u001b[36m130.5862\u001b[0m      \u001b[32m125.1670\u001b[0m  0.0010  0.1042\n",
      "     10      \u001b[36m136.2048\u001b[0m      \u001b[32m119.5996\u001b[0m  0.0010  0.1428\n",
      "     10      \u001b[36m128.8398\u001b[0m      \u001b[32m125.0085\u001b[0m  0.0010  0.1308\n",
      "     11      \u001b[36m132.9836\u001b[0m      \u001b[32m118.3666\u001b[0m  0.0010  0.1229\n",
      "     11      \u001b[36m124.9102\u001b[0m      \u001b[32m123.3154\u001b[0m  0.0010  0.1466\n",
      "     12      \u001b[36m129.5954\u001b[0m      \u001b[32m117.5713\u001b[0m  0.0010  0.1257\n",
      "     12      \u001b[36m123.1145\u001b[0m      \u001b[32m122.1970\u001b[0m  0.0010  0.1256\n",
      "     13      \u001b[36m127.9718\u001b[0m      \u001b[32m116.8419\u001b[0m  0.0010  0.1363\n",
      "     13      \u001b[36m120.7698\u001b[0m      \u001b[32m121.7765\u001b[0m  0.0010  0.1282\n",
      "     14      \u001b[36m125.1774\u001b[0m      \u001b[32m116.7821\u001b[0m  0.0010  0.1041\n",
      "     14      \u001b[36m118.2325\u001b[0m      \u001b[32m121.0047\u001b[0m  0.0010  0.1289\n",
      "     15      \u001b[36m124.2991\u001b[0m      \u001b[32m115.9251\u001b[0m  0.0010  0.1181\n",
      "     16      \u001b[36m124.2689\u001b[0m      116.3526  0.0010  0.0983\n",
      "     15      \u001b[36m117.6701\u001b[0m      \u001b[32m120.4441\u001b[0m  0.0010  0.1288\n",
      "     17      \u001b[36m120.9256\u001b[0m      116.0423  0.0010  0.1161\n",
      "     16      117.8667      \u001b[32m120.2450\u001b[0m  0.0010  0.1171\n",
      "     18      \u001b[36m119.2319\u001b[0m      \u001b[32m114.6265\u001b[0m  0.0010  0.1139\n",
      "     17      \u001b[36m113.9110\u001b[0m      \u001b[32m120.2325\u001b[0m  0.0010  0.1166\n",
      "     19      \u001b[36m118.9068\u001b[0m      \u001b[32m114.5783\u001b[0m  0.0010  0.1081\n",
      "     18      115.3026      \u001b[32m120.1118\u001b[0m  0.0010  0.1304\n",
      "     20      \u001b[36m118.0354\u001b[0m      114.7675  0.0010  0.1219\n",
      "     19      115.9878      \u001b[32m120.0338\u001b[0m  0.0010  0.1143\n",
      "     21      \u001b[36m117.2690\u001b[0m      \u001b[32m113.7270\u001b[0m  0.0010  0.1296\n",
      "     20      \u001b[36m112.9562\u001b[0m      \u001b[32m119.1541\u001b[0m  0.0010  0.1241\n",
      "     22      \u001b[36m115.9235\u001b[0m      \u001b[32m113.4029\u001b[0m  0.0010  0.1318\n",
      "     21      \u001b[36m110.4939\u001b[0m      119.6455  0.0010  0.1291\n",
      "     23      \u001b[36m115.5497\u001b[0m      \u001b[32m113.3875\u001b[0m  0.0010  0.1442\n",
      "     22      \u001b[36m109.8206\u001b[0m      \u001b[32m119.0885\u001b[0m  0.0010  0.1557\n",
      "     24      \u001b[36m114.7341\u001b[0m      113.5387  0.0010  0.1588\n",
      "     23      \u001b[36m109.0888\u001b[0m      \u001b[32m118.3219\u001b[0m  0.0010  0.1326\n",
      "     25      \u001b[36m114.5725\u001b[0m      \u001b[32m113.2822\u001b[0m  0.0010  0.1277\n",
      "     24      \u001b[36m107.5339\u001b[0m      118.5615  0.0010  0.1287\n",
      "     26      \u001b[36m112.3250\u001b[0m      \u001b[32m113.1267\u001b[0m  0.0010  0.1444\n",
      "     25      108.7957      \u001b[32m118.2325\u001b[0m  0.0010  0.1521\n",
      "     27      114.2907      \u001b[32m112.9511\u001b[0m  0.0010  0.1477\n",
      "     26      109.2553      \u001b[32m117.5562\u001b[0m  0.0010  0.1310\n",
      "     27      \u001b[36m106.2735\u001b[0m      \u001b[32m117.4095\u001b[0m  0.0010  0.1131\n",
      "     28      113.8163      \u001b[32m112.7021\u001b[0m  0.0010  0.1461\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m634.2134\u001b[0m      \u001b[32m450.6800\u001b[0m  0.0010  0.1402\n",
      "     28      \u001b[36m105.2007\u001b[0m      117.5435  0.0010  0.1400\n",
      "     29      113.1989      \u001b[32m112.7018\u001b[0m  0.0010  0.1257\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m559.6300\u001b[0m      \u001b[32m799.8089\u001b[0m  0.0010  0.1640\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m539.8159\u001b[0m      \u001b[32m453.9748\u001b[0m  0.0010  0.1463\n",
      "      2      \u001b[36m322.8853\u001b[0m      \u001b[32m187.5205\u001b[0m  0.0010  0.1365\n",
      "      2      \u001b[36m295.6630\u001b[0m      \u001b[32m289.3259\u001b[0m  0.0010  0.1223\n",
      "      2      \u001b[36m270.9221\u001b[0m      \u001b[32m188.2236\u001b[0m  0.0010  0.1239\n",
      "     30      \u001b[36m111.6098\u001b[0m      \u001b[32m111.7097\u001b[0m  0.0010  0.1411\n",
      "     29      \u001b[36m105.0961\u001b[0m      \u001b[32m117.1564\u001b[0m  0.0010  0.1569\n",
      "      3      \u001b[36m163.2707\u001b[0m      \u001b[32m150.2231\u001b[0m  0.0010  0.1360\n",
      "     30      105.4238      \u001b[32m116.5668\u001b[0m  0.0010  0.1214\n",
      "      3      \u001b[36m155.3051\u001b[0m      \u001b[32m211.3348\u001b[0m  0.0010  0.1494\n",
      "      3      \u001b[36m149.1997\u001b[0m      \u001b[32m153.2900\u001b[0m  0.0010  0.1472\n",
      "     31      111.6979      112.1227  0.0010  0.1432\n",
      "      4      \u001b[36m145.5554\u001b[0m      \u001b[32m140.3891\u001b[0m  0.0010  0.1227\n",
      "      4      \u001b[36m136.6061\u001b[0m      \u001b[32m140.2646\u001b[0m  0.0010  0.1177\n",
      "     31      \u001b[36m104.3931\u001b[0m      \u001b[32m115.9838\u001b[0m  0.0010  0.1396\n",
      "      4      \u001b[36m142.4962\u001b[0m      \u001b[32m196.5511\u001b[0m  0.0010  0.1350\n",
      "     32      111.7619      111.7853  0.0010  0.1447\n",
      "      5      \u001b[36m141.3932\u001b[0m      \u001b[32m136.2472\u001b[0m  0.0010  0.1330\n",
      "      5      \u001b[36m130.3598\u001b[0m      \u001b[32m134.5886\u001b[0m  0.0010  0.1352\n",
      "     32      \u001b[36m104.3284\u001b[0m      116.3807  0.0010  0.1429\n",
      "      5      \u001b[36m137.0424\u001b[0m      \u001b[32m188.7813\u001b[0m  0.0010  0.1431\n",
      "     33      111.9210      \u001b[32m111.3629\u001b[0m  0.0010  0.1622\n",
      "      6      \u001b[36m137.1667\u001b[0m      \u001b[32m133.1365\u001b[0m  0.0010  0.1298\n",
      "      6      \u001b[36m125.4834\u001b[0m      \u001b[32m130.6452\u001b[0m  0.0010  0.1448\n",
      "      6      \u001b[36m132.4003\u001b[0m      \u001b[32m182.3363\u001b[0m  0.0010  0.1333\n",
      "     33      \u001b[36m101.6569\u001b[0m      116.5541  0.0010  0.1442\n",
      "     34      \u001b[36m110.9118\u001b[0m      111.6199  0.0010  0.1249\n",
      "      7      \u001b[36m132.7648\u001b[0m      \u001b[32m130.9187\u001b[0m  0.0010  0.1579\n",
      "      7      \u001b[36m128.6357\u001b[0m      \u001b[32m176.7995\u001b[0m  0.0010  0.1142\n",
      "      7      \u001b[36m122.2071\u001b[0m      \u001b[32m128.7088\u001b[0m  0.0010  0.1334\n",
      "     34      101.9234      \u001b[32m115.8776\u001b[0m  0.0010  0.1382\n",
      "     35      \u001b[36m110.4397\u001b[0m      111.6024  0.0010  0.1487\n",
      "      8      \u001b[36m129.7996\u001b[0m      \u001b[32m128.6797\u001b[0m  0.0010  0.1266\n",
      "      8      \u001b[36m119.4942\u001b[0m      \u001b[32m126.5104\u001b[0m  0.0010  0.1302\n",
      "      8      \u001b[36m125.2890\u001b[0m      \u001b[32m171.7223\u001b[0m  0.0010  0.1420\n",
      "     35      103.2485      \u001b[32m115.4623\u001b[0m  0.0010  0.1334\n",
      "     36      \u001b[36m109.1878\u001b[0m      111.4327  0.0010  0.1317\n",
      "      9      \u001b[36m123.8760\u001b[0m      \u001b[32m126.8743\u001b[0m  0.0010  0.1421\n",
      "      9      \u001b[36m114.4302\u001b[0m      \u001b[32m124.9915\u001b[0m  0.0010  0.1375\n",
      "      9      \u001b[36m122.8954\u001b[0m      \u001b[32m168.5623\u001b[0m  0.0010  0.1403\n",
      "     36      102.6868      115.9985  0.0010  0.1250\n",
      "     37      110.5325      111.4131  0.0010  0.1447\n",
      "     10      \u001b[36m119.0497\u001b[0m      \u001b[32m125.4418\u001b[0m  0.0010  0.1408\n",
      "     10      \u001b[36m113.7219\u001b[0m      \u001b[32m124.2465\u001b[0m  0.0010  0.1375\n",
      "     10      \u001b[36m118.6927\u001b[0m      \u001b[32m161.8054\u001b[0m  0.0010  0.1392\n",
      "     37      102.6792      116.3594  0.0010  0.1486\n",
      "     38      \u001b[36m108.0306\u001b[0m      \u001b[32m110.5683\u001b[0m  0.0005  0.1139\n",
      "     11      \u001b[36m116.2470\u001b[0m      \u001b[32m125.3634\u001b[0m  0.0010  0.1230\n",
      "     38      \u001b[36m101.0308\u001b[0m      116.3582  0.0010  0.1236\n",
      "     11      \u001b[36m111.9290\u001b[0m      \u001b[32m123.1281\u001b[0m  0.0010  0.1604\n",
      "     11      \u001b[36m117.2200\u001b[0m      \u001b[32m157.5150\u001b[0m  0.0010  0.1550\n",
      "     39      \u001b[36m107.1653\u001b[0m      110.7731  0.0005  0.1273\n",
      "     12      \u001b[36m113.2593\u001b[0m      \u001b[32m124.4939\u001b[0m  0.0010  0.1230\n",
      "     39      101.9465      115.4986  0.0010  0.1468\n",
      "     12      \u001b[36m111.4910\u001b[0m      \u001b[32m122.0926\u001b[0m  0.0010  0.1346\n",
      "     12      \u001b[36m113.7982\u001b[0m      \u001b[32m153.1504\u001b[0m  0.0010  0.1422\n",
      "     40      \u001b[36m106.0057\u001b[0m      111.4371  0.0005  0.1558\n",
      "     13      \u001b[36m110.7467\u001b[0m      124.6746  0.0010  0.1474\n",
      "     40       \u001b[36m98.5522\u001b[0m      \u001b[32m115.4128\u001b[0m  0.0005  0.1410\n",
      "     13      \u001b[36m113.7791\u001b[0m      \u001b[32m151.1354\u001b[0m  0.0010  0.1423\n",
      "     13      \u001b[36m108.4202\u001b[0m      \u001b[32m121.4083\u001b[0m  0.0010  0.1647\n",
      "     41      \u001b[36m105.6677\u001b[0m      \u001b[32m110.3486\u001b[0m  0.0005  0.1437\n",
      "     14      \u001b[36m109.8203\u001b[0m      124.7619  0.0010  0.1256\n",
      "     41      100.0499      115.5825  0.0005  0.1356\n",
      "     14      \u001b[36m111.4281\u001b[0m      \u001b[32m150.8281\u001b[0m  0.0010  0.1413\n",
      "     42      106.8071      \u001b[32m110.3047\u001b[0m  0.0005  0.1288\n",
      "     14      \u001b[36m108.2129\u001b[0m      \u001b[32m121.1006\u001b[0m  0.0010  0.1562\n",
      "     15      \u001b[36m107.4798\u001b[0m      \u001b[32m123.7559\u001b[0m  0.0010  0.1414\n",
      "     42       99.6480      \u001b[32m115.1536\u001b[0m  0.0005  0.1186\n",
      "     15      \u001b[36m110.6160\u001b[0m      \u001b[32m147.9182\u001b[0m  0.0010  0.1246\n",
      "     15      \u001b[36m107.3101\u001b[0m      \u001b[32m120.3427\u001b[0m  0.0010  0.1332\n",
      "     43      106.7551      110.3532  0.0005  0.1642\n",
      "     16      \u001b[36m106.7548\u001b[0m      \u001b[32m122.8848\u001b[0m  0.0010  0.1390\n",
      "     43       99.2157      115.2002  0.0005  0.1330\n",
      "     16      \u001b[36m109.4348\u001b[0m      \u001b[32m146.6500\u001b[0m  0.0010  0.1476\n",
      "     16      \u001b[36m105.9865\u001b[0m      \u001b[32m118.9216\u001b[0m  0.0010  0.1160\n",
      "     17      107.2964      123.0387  0.0010  0.1350\n",
      "     44      106.9094      110.7691  0.0005  0.1582\n",
      "     44      101.1221      \u001b[32m114.5780\u001b[0m  0.0005  0.1428\n",
      "     17      \u001b[36m108.9958\u001b[0m      \u001b[32m143.5639\u001b[0m  0.0010  0.1155\n",
      "     17      \u001b[36m105.2712\u001b[0m      119.2016  0.0010  0.1336\n",
      "     18      \u001b[36m104.8916\u001b[0m      122.9835  0.0010  0.1502\n",
      "     45      107.1259      110.3295  0.0005  0.1584\n",
      "     45      100.0156      \u001b[32m114.5694\u001b[0m  0.0005  0.1314\n",
      "     18      \u001b[36m106.8744\u001b[0m      \u001b[32m140.7302\u001b[0m  0.0010  0.1327\n",
      "     18      \u001b[36m103.7832\u001b[0m      118.9230  0.0010  0.1388\n",
      "     19      105.8765      \u001b[32m122.0975\u001b[0m  0.0010  0.1315\n",
      "     19      \u001b[36m106.5339\u001b[0m      141.2189  0.0010  0.1187\n",
      "     46       98.9267      115.1427  0.0005  0.1320\n",
      "     46      106.0094      \u001b[32m110.0740\u001b[0m  0.0005  0.1597\n",
      "     19      \u001b[36m102.3649\u001b[0m      \u001b[32m118.1294\u001b[0m  0.0010  0.1465\n",
      "     20      \u001b[36m103.5970\u001b[0m      122.7770  0.0010  0.1549\n",
      "     20      \u001b[36m105.3037\u001b[0m      \u001b[32m140.3967\u001b[0m  0.0010  0.1192\n",
      "     47       \u001b[36m98.0608\u001b[0m      114.8970  0.0005  0.1414\n",
      "     47      106.3558      \u001b[32m109.9142\u001b[0m  0.0005  0.1405\n",
      "     20      \u001b[36m102.1677\u001b[0m      \u001b[32m117.8570\u001b[0m  0.0010  0.1326\n",
      "     21      \u001b[36m103.1991\u001b[0m      \u001b[32m121.8708\u001b[0m  0.0010  0.1230\n",
      "     21      \u001b[36m103.8441\u001b[0m      \u001b[32m138.6660\u001b[0m  0.0010  0.1296\n",
      "     48       \u001b[36m97.5634\u001b[0m      115.4691  0.0005  0.1325\n",
      "     48      106.5237      \u001b[32m109.8384\u001b[0m  0.0005  0.1332\n",
      "     21      \u001b[36m101.4257\u001b[0m      \u001b[32m117.2008\u001b[0m  0.0010  0.1516\n",
      "     22      \u001b[36m101.9912\u001b[0m      122.4903  0.0010  0.1272\n",
      "     22      \u001b[36m103.7709\u001b[0m      \u001b[32m138.2136\u001b[0m  0.0010  0.1375\n",
      "     49      \u001b[36m105.1407\u001b[0m      \u001b[32m109.6583\u001b[0m  0.0005  0.1203\n",
      "     22      \u001b[36m100.1446\u001b[0m      \u001b[32m117.0486\u001b[0m  0.0010  0.1328\n",
      "     23      \u001b[36m101.1312\u001b[0m      \u001b[32m121.4125\u001b[0m  0.0010  0.1363\n",
      "     23      \u001b[36m102.4065\u001b[0m      138.7091  0.0010  0.1474\n",
      "     50      \u001b[36m104.5577\u001b[0m      109.9118  0.0005  0.1463\n",
      "     23       \u001b[36m99.3559\u001b[0m      117.2396  0.0010  0.1383\n",
      "     24      \u001b[36m101.1047\u001b[0m      \u001b[32m120.9600\u001b[0m  0.0010  0.1498\n",
      "     24      103.3470      \u001b[32m137.1598\u001b[0m  0.0010  0.1377\n",
      "     51      106.3939      109.8633  0.0005  0.1270\n",
      "     24       \u001b[36m99.2917\u001b[0m      \u001b[32m116.8113\u001b[0m  0.0010  0.1408\n",
      "     25      \u001b[36m100.1046\u001b[0m      \u001b[32m120.4719\u001b[0m  0.0010  0.1374\n",
      "     25      \u001b[36m102.2552\u001b[0m      \u001b[32m136.7837\u001b[0m  0.0010  0.1403\n",
      "     52      \u001b[36m103.8155\u001b[0m      \u001b[32m109.5419\u001b[0m  0.0005  0.1541\n",
      "     25       \u001b[36m98.6434\u001b[0m      \u001b[32m116.0974\u001b[0m  0.0010  0.1232\n",
      "     26       \u001b[36m99.5648\u001b[0m      121.0302  0.0010  0.1460\n",
      "     26      \u001b[36m101.2401\u001b[0m      \u001b[32m134.7415\u001b[0m  0.0010  0.1465\n",
      "     53      105.6737      \u001b[32m109.4157\u001b[0m  0.0005  0.1360\n",
      "     26       99.2504      116.2003  0.0010  0.1432\n",
      "     27       \u001b[36m98.5365\u001b[0m      121.1932  0.0010  0.1169\n",
      "     27       \u001b[36m99.7127\u001b[0m      135.2847  0.0010  0.1118\n",
      "     54      104.9089      109.7949  0.0005  0.1251\n",
      "     27       \u001b[36m98.0854\u001b[0m      \u001b[32m115.9098\u001b[0m  0.0010  0.1167\n",
      "     28       99.8467      120.8927  0.0010  0.1061\n",
      "     28       99.9804      \u001b[32m134.3447\u001b[0m  0.0010  0.1356\n",
      "     55      \u001b[36m103.7042\u001b[0m      109.8119  0.0005  0.1348\n",
      "     28       \u001b[36m97.7675\u001b[0m      \u001b[32m115.7562\u001b[0m  0.0010  0.1283\n",
      "     29       98.6877      121.4241  0.0010  0.1218\n",
      "     29       \u001b[36m99.3041\u001b[0m      \u001b[32m134.3220\u001b[0m  0.0010  0.1319\n",
      "     56      \u001b[36m103.2389\u001b[0m      \u001b[32m109.1137\u001b[0m  0.0005  0.1275\n",
      "     29       \u001b[36m96.4326\u001b[0m      \u001b[32m115.5153\u001b[0m  0.0010  0.1238\n",
      "     30       \u001b[36m96.6316\u001b[0m      \u001b[32m119.1052\u001b[0m  0.0005  0.1136\n",
      "     30       99.8281      \u001b[32m133.3158\u001b[0m  0.0010  0.1269\n",
      "     57      104.7329      \u001b[32m108.7615\u001b[0m  0.0005  0.1187\n",
      "     30       96.4656      \u001b[32m115.3078\u001b[0m  0.0010  0.1159\n",
      "     31       \u001b[36m96.0770\u001b[0m      \u001b[32m118.8644\u001b[0m  0.0005  0.1207\n",
      "     58      104.4734      108.9639  0.0005  0.1019\n",
      "     31       \u001b[36m97.5386\u001b[0m      \u001b[32m132.7098\u001b[0m  0.0010  0.1311\n",
      "     31       97.2356      115.3146  0.0010  0.1201\n",
      "     32       96.2583      \u001b[32m118.5614\u001b[0m  0.0005  0.1200\n",
      "     59      103.5599      \u001b[32m108.6434\u001b[0m  0.0005  0.1780\n",
      "     32       98.6957      133.8302  0.0010  0.1836\n",
      "     32       \u001b[36m94.7940\u001b[0m      \u001b[32m115.1097\u001b[0m  0.0010  0.1982\n",
      "     33       \u001b[36m94.4860\u001b[0m      118.7045  0.0005  0.2204\n",
      "     60      103.9093      109.1402  0.0005  0.1641\n",
      "     33       97.6977      133.5621  0.0010  0.1478\n",
      "     33       95.9681      \u001b[32m114.4705\u001b[0m  0.0010  0.1447\n",
      "     34       94.5200      118.7437  0.0005  0.1323\n",
      "     61      103.5685      108.7286  0.0005  0.1344\n",
      "     34       98.0166      \u001b[32m130.3818\u001b[0m  0.0010  0.1396\n",
      "     34       \u001b[36m94.7925\u001b[0m      \u001b[32m114.3963\u001b[0m  0.0010  0.1244\n",
      "     35       96.2774      118.8318  0.0005  0.1335\n",
      "     62      103.5885      108.7562  0.0005  0.1512\n",
      "     35       \u001b[36m96.2031\u001b[0m      131.3937  0.0010  0.1335\n",
      "     35       \u001b[36m94.3727\u001b[0m      114.8212  0.0010  0.1263\n",
      "     36       94.9155      118.5912  0.0005  0.1367\n",
      "     36       97.3446      132.6318  0.0010  0.1125\n",
      "     63      104.0081      \u001b[32m108.4410\u001b[0m  0.0005  0.1316\n",
      "     36       94.9384      114.9172  0.0010  0.1331\n",
      "     37       \u001b[36m94.1582\u001b[0m      \u001b[32m118.4194\u001b[0m  0.0003  0.1439\n",
      "     37       96.7750      132.5743  0.0010  0.1121\n",
      "     64      103.2664      \u001b[32m108.1019\u001b[0m  0.0005  0.1263\n",
      "     37       94.4266      114.5141  0.0010  0.1264\n",
      "     38       \u001b[36m92.7586\u001b[0m      \u001b[32m118.2873\u001b[0m  0.0003  0.1274\n",
      "     38       96.4483      132.3756  0.0010  0.1401\n",
      "     65      104.2414      108.5355  0.0005  0.1167\n",
      "     38       94.3828      114.6803  0.0010  0.1076\n",
      "     39       93.4431      \u001b[32m118.1681\u001b[0m  0.0003  0.1169\n",
      "     39       \u001b[36m94.9427\u001b[0m      \u001b[32m126.6457\u001b[0m  0.0005  0.1175\n",
      "     39       \u001b[36m93.5248\u001b[0m      \u001b[32m114.1749\u001b[0m  0.0005  0.1258\n",
      "     66      103.8792      108.3308  0.0005  0.1505\n",
      "     40       93.5143      \u001b[32m118.1137\u001b[0m  0.0003  0.1095\n",
      "     40       95.2019      127.6901  0.0005  0.1152\n",
      "     40       \u001b[36m91.0803\u001b[0m      114.3429  0.0005  0.1208\n",
      "     67      103.6315      108.6122  0.0005  0.1162\n",
      "     41       94.4613      \u001b[32m118.0684\u001b[0m  0.0003  0.1253\n",
      "     41       \u001b[36m94.2267\u001b[0m      126.9288  0.0005  0.1106\n",
      "     41       93.2750      \u001b[32m113.9714\u001b[0m  0.0005  0.1255\n",
      "     68      \u001b[36m102.9757\u001b[0m      108.4200  0.0005  0.1300\n",
      "     42       93.1223      118.2068  0.0003  0.0991\n",
      "     42       \u001b[36m93.5232\u001b[0m      127.2511  0.0005  0.1184\n",
      "     42       92.2833      114.0588  0.0005  0.1187\n",
      "     43       94.3625      \u001b[32m117.8923\u001b[0m  0.0003  0.1314\n",
      "     43       94.1446      127.5759  0.0005  0.1132\n",
      "     43       92.1201      113.9831  0.0005  0.1207\n",
      "     44       93.4171      118.2093  0.0003  0.1098\n",
      "     44       94.0931      \u001b[32m126.3633\u001b[0m  0.0003  0.1023\n",
      "     44       92.2045      114.1367  0.0005  0.1523\n",
      "     45       93.7807      \u001b[32m124.8636\u001b[0m  0.0003  0.1155\n",
      "     45       93.2006      118.7030  0.0003  0.1481\n",
      "     46       \u001b[36m92.4069\u001b[0m      125.3458  0.0003  0.1331\n",
      "     46       \u001b[36m91.5808\u001b[0m      118.6000  0.0003  0.1330\n",
      "     45       92.3278      113.9882  0.0005  0.1616\n",
      "     47       92.5932      125.5575  0.0003  0.1034\n",
      "     47       93.2266      118.3497  0.0003  0.1166\n",
      "     48       \u001b[36m91.7163\u001b[0m      \u001b[32m124.4525\u001b[0m  0.0003  0.1034\n",
      "     49       91.7226      \u001b[32m124.4077\u001b[0m  0.0003  0.1049\n",
      "     50       \u001b[36m91.5848\u001b[0m      \u001b[32m124.0708\u001b[0m  0.0003  0.1206\n",
      "     51       92.0513      124.2705  0.0003  0.1033\n",
      "     52       91.9152      125.0287  0.0003  0.0937\n",
      "     53       91.7573      124.7235  0.0003  0.0908\n",
      "     54       92.7123      124.7268  0.0003  0.0746\n",
      "RMSE for each folder: [10.71007736 11.8871154  11.30105568 10.0725438   8.95185521]\n",
      "RMSE mean: 10.584529491525903\n"
     ]
    }
   ],
   "source": [
    "#Guided\n",
    "cross_validation_with_scores(x_cov_nn,y,groups,pipe_fusion,logo,rmse_scorer)\n",
    "\n",
    "#Free \n",
    "cross_validation_with_scores(x_cov_nn_f,y_f,groups_f,pipe_fusion,logo,rmse_scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5127f6c1-c9b2-47df-878b-aa7e2cfa63d6",
   "metadata": {},
   "source": [
    "![](./images/5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a2f89932-867f-4c54-9a38-d64b949fe003",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m440.9353\u001b[0m      \u001b[32m321.2495\u001b[0m  0.0010  0.0966\n",
      "      2      \u001b[36m217.0207\u001b[0m      \u001b[32m147.0345\u001b[0m  0.0010  0.0863\n",
      "      3      \u001b[36m150.9787\u001b[0m      \u001b[32m126.6180\u001b[0m  0.0010  0.0839\n",
      "      4      \u001b[36m128.9234\u001b[0m      \u001b[32m113.5365\u001b[0m  0.0010  0.0789\n",
      "      5      \u001b[36m112.8172\u001b[0m      \u001b[32m102.9149\u001b[0m  0.0010  0.0844\n",
      "      6      \u001b[36m101.2874\u001b[0m       \u001b[32m94.7909\u001b[0m  0.0010  0.0836\n",
      "      7       \u001b[36m93.6632\u001b[0m       \u001b[32m87.5280\u001b[0m  0.0010  0.0794\n",
      "      8       \u001b[36m87.4154\u001b[0m       \u001b[32m81.7757\u001b[0m  0.0010  0.0878\n",
      "      9       \u001b[36m83.5102\u001b[0m       \u001b[32m77.0341\u001b[0m  0.0010  0.0810\n",
      "     10       \u001b[36m79.0211\u001b[0m       \u001b[32m72.6960\u001b[0m  0.0010  0.0789\n",
      "     11       \u001b[36m75.2462\u001b[0m       \u001b[32m68.3102\u001b[0m  0.0010  0.0781\n",
      "     12       \u001b[36m70.8998\u001b[0m       \u001b[32m65.9711\u001b[0m  0.0010  0.0883\n",
      "     13       \u001b[36m67.6282\u001b[0m       \u001b[32m61.6043\u001b[0m  0.0010  0.0868\n",
      "     14       \u001b[36m65.3987\u001b[0m       \u001b[32m59.3400\u001b[0m  0.0010  0.0918\n",
      "     15       \u001b[36m63.0777\u001b[0m       \u001b[32m58.0101\u001b[0m  0.0010  0.0833\n",
      "     16       \u001b[36m60.6039\u001b[0m       \u001b[32m54.9997\u001b[0m  0.0010  0.0928\n",
      "     17       \u001b[36m57.8418\u001b[0m       \u001b[32m53.6849\u001b[0m  0.0010  0.0797\n",
      "     18       \u001b[36m56.5628\u001b[0m       \u001b[32m52.0810\u001b[0m  0.0010  0.0743\n",
      "     19       \u001b[36m55.6242\u001b[0m       \u001b[32m49.8632\u001b[0m  0.0010  0.0819\n",
      "     20       \u001b[36m53.7839\u001b[0m       \u001b[32m48.7777\u001b[0m  0.0010  0.0878\n",
      "     21       \u001b[36m53.2406\u001b[0m       \u001b[32m46.0751\u001b[0m  0.0010  0.0787\n",
      "     22       \u001b[36m49.5066\u001b[0m       46.0936  0.0010  0.0920\n",
      "     23       49.9333       \u001b[32m42.8342\u001b[0m  0.0010  0.0771\n",
      "     24       \u001b[36m48.1561\u001b[0m       43.6134  0.0010  0.0949\n",
      "     25       \u001b[36m46.7011\u001b[0m       \u001b[32m42.0426\u001b[0m  0.0010  0.0907\n",
      "     26       47.0397       \u001b[32m40.2726\u001b[0m  0.0010  0.0870\n",
      "     27       \u001b[36m45.9716\u001b[0m       \u001b[32m38.7841\u001b[0m  0.0010  0.0901\n",
      "     28       \u001b[36m43.8132\u001b[0m       \u001b[32m38.5716\u001b[0m  0.0010  0.0750\n",
      "     29       \u001b[36m43.7827\u001b[0m       \u001b[32m36.9755\u001b[0m  0.0010  0.0870\n",
      "     30       \u001b[36m43.2125\u001b[0m       37.5883  0.0010  0.0843\n",
      "     31       \u001b[36m41.3787\u001b[0m       \u001b[32m36.5505\u001b[0m  0.0010  0.0794\n",
      "     32       41.7147       \u001b[32m35.6906\u001b[0m  0.0010  0.0775\n",
      "     33       42.0776       \u001b[32m35.2654\u001b[0m  0.0010  0.0938\n",
      "     34       \u001b[36m40.6362\u001b[0m       \u001b[32m34.1729\u001b[0m  0.0010  0.0987\n",
      "     35       \u001b[36m39.0906\u001b[0m       \u001b[32m33.2292\u001b[0m  0.0010  0.0919\n",
      "     36       \u001b[36m39.0273\u001b[0m       \u001b[32m32.9284\u001b[0m  0.0010  0.0861\n",
      "     37       \u001b[36m38.5559\u001b[0m       \u001b[32m32.3237\u001b[0m  0.0010  0.0823\n",
      "     38       39.2386       32.5695  0.0010  0.0882\n",
      "     39       \u001b[36m38.3109\u001b[0m       \u001b[32m31.6148\u001b[0m  0.0010  0.0790\n",
      "     40       \u001b[36m37.7407\u001b[0m       31.9860  0.0010  0.0822\n",
      "     41       \u001b[36m36.9149\u001b[0m       \u001b[32m30.0468\u001b[0m  0.0010  0.0819\n",
      "     42       \u001b[36m36.6151\u001b[0m       30.0563  0.0010  0.0842\n",
      "     43       37.3279       \u001b[32m29.9757\u001b[0m  0.0010  0.0785\n",
      "     44       \u001b[36m36.4630\u001b[0m       \u001b[32m29.9541\u001b[0m  0.0010  0.0877\n",
      "     45       \u001b[36m35.4014\u001b[0m       \u001b[32m29.2054\u001b[0m  0.0010  0.1306\n",
      "     46       \u001b[36m35.0509\u001b[0m       \u001b[32m28.4452\u001b[0m  0.0010  0.0822\n",
      "     47       35.5461       \u001b[32m27.4027\u001b[0m  0.0010  0.0818\n",
      "     48       \u001b[36m35.0116\u001b[0m       27.5753  0.0010  0.0796\n",
      "     49       \u001b[36m34.4466\u001b[0m       \u001b[32m26.9079\u001b[0m  0.0010  0.0849\n",
      "     50       34.6369       \u001b[32m26.5661\u001b[0m  0.0010  0.0774\n",
      "     51       \u001b[36m33.8350\u001b[0m       27.0967  0.0010  0.0842\n",
      "     52       \u001b[36m33.1480\u001b[0m       \u001b[32m26.4088\u001b[0m  0.0010  0.0768\n",
      "     53       \u001b[36m32.9024\u001b[0m       \u001b[32m25.3684\u001b[0m  0.0010  0.0834\n",
      "     54       33.5331       26.4776  0.0010  0.0826\n",
      "     55       33.2957       25.4381  0.0010  0.0785\n",
      "     56       33.4557       25.6028  0.0010  0.0885\n",
      "     57       33.2455       \u001b[32m24.4325\u001b[0m  0.0010  0.0854\n",
      "     58       \u001b[36m32.5942\u001b[0m       25.3279  0.0010  0.0770\n",
      "     59       33.2536       25.2204  0.0010  0.0821\n",
      "     60       \u001b[36m31.5246\u001b[0m       \u001b[32m24.3119\u001b[0m  0.0010  0.0910\n",
      "     61       31.6059       24.5982  0.0010  0.0748\n",
      "     62       32.5043       24.8134  0.0010  0.0756\n",
      "     63       \u001b[36m31.2484\u001b[0m       \u001b[32m22.7708\u001b[0m  0.0010  0.0901\n",
      "     64       \u001b[36m29.9109\u001b[0m       23.4672  0.0010  0.0751\n",
      "     65       31.0488       23.6455  0.0010  0.0749\n",
      "     66       32.2985       23.5670  0.0010  0.0822\n",
      "     67       31.3520       22.9561  0.0010  0.0766\n",
      "     68       30.1231       \u001b[32m22.0886\u001b[0m  0.0005  0.0839\n",
      "     69       \u001b[36m29.6570\u001b[0m       22.8461  0.0005  0.0776\n",
      "     70       30.4230       22.7972  0.0005  0.0815\n",
      "     71       30.3282       22.7098  0.0005  0.0846\n",
      "     72       30.1843       22.5951  0.0005  0.0815\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m620.4384\u001b[0m      \u001b[32m382.0728\u001b[0m  0.0010  0.0926\n",
      "      2      \u001b[36m270.6949\u001b[0m      \u001b[32m160.4806\u001b[0m  0.0010  0.0883\n",
      "      3      \u001b[36m159.3039\u001b[0m      \u001b[32m141.4028\u001b[0m  0.0010  0.1016\n",
      "      4      \u001b[36m147.8866\u001b[0m      \u001b[32m135.0690\u001b[0m  0.0010  0.0867\n",
      "      5      \u001b[36m141.7053\u001b[0m      \u001b[32m130.9750\u001b[0m  0.0010  0.0879\n",
      "      6      \u001b[36m137.1370\u001b[0m      \u001b[32m128.6216\u001b[0m  0.0010  0.0977\n",
      "      7      \u001b[36m132.9875\u001b[0m      \u001b[32m127.1554\u001b[0m  0.0010  0.0947\n",
      "      8      \u001b[36m129.0842\u001b[0m      \u001b[32m126.0535\u001b[0m  0.0010  0.0989\n",
      "      9      \u001b[36m125.7572\u001b[0m      \u001b[32m125.0518\u001b[0m  0.0010  0.1004\n",
      "     10      \u001b[36m122.5381\u001b[0m      \u001b[32m122.8381\u001b[0m  0.0010  0.1030\n",
      "     11      \u001b[36m121.2748\u001b[0m      \u001b[32m122.6606\u001b[0m  0.0010  0.1070\n",
      "     12      \u001b[36m118.2585\u001b[0m      \u001b[32m122.0661\u001b[0m  0.0010  0.1103\n",
      "     13      \u001b[36m116.8067\u001b[0m      \u001b[32m121.1448\u001b[0m  0.0010  0.1003\n",
      "     14      \u001b[36m114.8546\u001b[0m      \u001b[32m120.8670\u001b[0m  0.0010  0.1014\n",
      "     15      \u001b[36m113.6295\u001b[0m      \u001b[32m119.8187\u001b[0m  0.0010  0.0901\n",
      "     16      \u001b[36m111.9878\u001b[0m      \u001b[32m119.1762\u001b[0m  0.0010  0.0873\n",
      "     17      \u001b[36m109.7922\u001b[0m      \u001b[32m118.9213\u001b[0m  0.0010  0.0981\n",
      "     18      109.9275      \u001b[32m118.4307\u001b[0m  0.0010  0.0854\n",
      "     19      \u001b[36m108.4722\u001b[0m      \u001b[32m117.5087\u001b[0m  0.0010  0.0900\n",
      "     20      \u001b[36m107.7092\u001b[0m      \u001b[32m117.4994\u001b[0m  0.0010  0.1031\n",
      "     21      \u001b[36m106.3049\u001b[0m      \u001b[32m117.0531\u001b[0m  0.0010  0.0932\n",
      "     22      108.6672      117.7611  0.0010  0.1018\n",
      "     23      \u001b[36m105.2838\u001b[0m      \u001b[32m116.6600\u001b[0m  0.0010  0.1009\n",
      "     24      \u001b[36m104.5858\u001b[0m      117.1554  0.0010  0.1027\n",
      "     25      106.6438      116.7557  0.0010  0.1053\n",
      "     26      \u001b[36m104.0233\u001b[0m      \u001b[32m115.8803\u001b[0m  0.0010  0.1014\n",
      "     27      \u001b[36m103.1336\u001b[0m      116.0546  0.0010  0.0949\n",
      "     28      \u001b[36m102.3531\u001b[0m      116.4495  0.0010  0.0860\n",
      "     29      102.4619      \u001b[32m115.8286\u001b[0m  0.0010  0.1118\n",
      "     30      \u001b[36m102.2263\u001b[0m      \u001b[32m115.0719\u001b[0m  0.0010  0.1142\n",
      "     31      \u001b[36m102.1617\u001b[0m      115.3190  0.0010  0.0891\n",
      "     32      102.6799      116.2675  0.0010  0.0936\n",
      "     33       \u001b[36m99.8395\u001b[0m      115.2359  0.0010  0.0920\n",
      "     34      101.9521      \u001b[32m114.5382\u001b[0m  0.0010  0.0949\n",
      "     35       99.9146      115.4401  0.0010  0.1567\n",
      "     36       \u001b[36m98.8096\u001b[0m      \u001b[32m114.1255\u001b[0m  0.0010  0.0973\n",
      "     37       99.3415      114.9211  0.0010  0.0935\n",
      "     38      100.5616      114.7043  0.0010  0.1000\n",
      "     39       99.7880      114.8439  0.0010  0.0898\n",
      "     40       99.1731      114.7869  0.0010  0.0985\n",
      "     41       \u001b[36m98.0872\u001b[0m      \u001b[32m113.3648\u001b[0m  0.0005  0.0941\n",
      "     42       98.1712      \u001b[32m113.0123\u001b[0m  0.0005  0.0915\n",
      "     43       \u001b[36m96.8556\u001b[0m      113.2048  0.0005  0.0943\n",
      "     44       \u001b[36m96.6298\u001b[0m      113.2834  0.0005  0.1115\n",
      "     45       \u001b[36m96.3159\u001b[0m      \u001b[32m112.9071\u001b[0m  0.0005  0.1091\n",
      "     46       \u001b[36m95.8443\u001b[0m      \u001b[32m112.5336\u001b[0m  0.0005  0.0942\n",
      "     47       \u001b[36m95.3122\u001b[0m      112.6587  0.0005  0.0954\n",
      "     48       96.1766      \u001b[32m112.4534\u001b[0m  0.0005  0.0971\n",
      "     49       96.3519      112.4995  0.0005  0.0987\n",
      "     50       95.9150      \u001b[32m112.4493\u001b[0m  0.0005  0.0956\n",
      "     51       96.2884      112.6814  0.0005  0.0949\n",
      "     52       96.4631      \u001b[32m112.4267\u001b[0m  0.0005  0.0970\n",
      "     53       96.3025      \u001b[32m112.3215\u001b[0m  0.0005  0.0963\n",
      "     54       95.9275      \u001b[32m111.9275\u001b[0m  0.0005  0.0964\n",
      "     55       \u001b[36m95.2876\u001b[0m      112.1129  0.0005  0.0959\n",
      "     56       95.6318      111.9564  0.0005  0.0918\n",
      "     57       95.8401      112.2952  0.0005  0.0988\n",
      "     58       96.6051      112.0851  0.0005  0.0981\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def plot(pipeline):\n",
    "    history = pipeline.named_steps['nn'].history\n",
    "    \n",
    "    epochs = history[:, 'epoch']\n",
    "    train_loss = history[:, 'train_loss']\n",
    "    valid_loss = history[:, 'valid_loss']\n",
    "    \n",
    "    plt.plot(epochs, train_loss, label='Train loss')\n",
    "    plt.plot(epochs, valid_loss, label='Valid loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.title('Train/Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "pipe_fusion.fit(x_cov_nn, y)\n",
    "plot(pipe_fusion)\n",
    "pipe_fusion.fit(x_cov_nn_f, y_f)\n",
    "plot(pipe_fusion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ad26a5e-f3e6-4a07-aae2-23535b5a84a0",
   "metadata": {},
   "source": [
    "| Method                                   | RMSE Guided | RMSE  Free |\n",
    "|------------------------------------------|-------------------|------------------:|\n",
    "| Basic Lasso                              |      18.73 | 15.78 |\n",
    "| Baseline approach                        |        –   | |\n",
    "| Covariance matrices + Lasso              |       7.36 | 10.45 |\n",
    "| Covariance matrices + Neural Network     |       5.02 | 10.61|\n",
    "| Convolutional Neural Network             |       4.54 | 11.51|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "When we compare the progression of our models, we see that incorporating covariance matrices into a simple Lasso regressor brings the rmse down about 62 % over the basic Lasso. \n",
    "\n",
    "...\n",
    "\n",
    "Moving on to a small neural network trained on those 36 tangent‐space features further reduces the error to 5.02, which represents a 39 % drop. Finally, the cnn achieves the lowest rmse of 4.87, a modest 3 % gain over the covariance nn hybrid.\n",
    "\n",
    "That last 3 % improvement, however, comes at a cost, the cnn is significantly more complex to train and deploy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416e1878-049b-4342-ad98-3c20afa1110e",
   "metadata": {},
   "source": [
    "#### Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ca1952-a0f3-45b6-8c78-0fae7b055672",
   "metadata": {},
   "source": [
    "For this question, we have ... regression models trained on different feature representations:\n",
    "\n",
    "- ...\n",
    "- Tangentes spaces of the signal using the NN + Covariance Matrice\n",
    "- Filtered raw signal data using a CNN\n",
    "\n",
    "\n",
    "To obtain each models predictions, we use cross_val_predict, which is essentially the same as cross_val_score but returns the predictions of each fold instead of the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4538322-bae9-4ba8-9e60-3a0001b3f189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m186.7674\u001b[0m      \u001b[32m119.0253\u001b[0m  0.0010  12.7094\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m200.5874\u001b[0m      \u001b[32m122.0080\u001b[0m  0.0010  12.8256\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m192.0104\u001b[0m      \u001b[32m114.4852\u001b[0m  0.0010  12.8553\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m180.7467\u001b[0m       \u001b[32m97.3401\u001b[0m  0.0010  12.9011\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m191.9468\u001b[0m      \u001b[32m123.3527\u001b[0m  0.0010  12.9838\n",
      "      2       \u001b[36m89.9195\u001b[0m      164.3289  0.0010  11.7223\n",
      "      2       \u001b[36m94.7736\u001b[0m       \u001b[32m75.4957\u001b[0m  0.0010  11.8235\n",
      "      2       \u001b[36m83.6776\u001b[0m       \u001b[32m72.0571\u001b[0m  0.0010  11.9660\n",
      "      2       \u001b[36m84.1798\u001b[0m       \u001b[32m68.0591\u001b[0m  0.0010  11.8563\n",
      "      2       \u001b[36m90.8490\u001b[0m      \u001b[32m119.5945\u001b[0m  0.0010  11.8032\n",
      "      3       \u001b[36m74.1286\u001b[0m       \u001b[32m88.8302\u001b[0m  0.0010  12.7188\n",
      "      3       \u001b[36m73.0610\u001b[0m       \u001b[32m66.1038\u001b[0m  0.0010  12.7124\n",
      "      3       \u001b[36m72.1327\u001b[0m       \u001b[32m67.8434\u001b[0m  0.0010  12.7563\n",
      "      3       \u001b[36m74.3822\u001b[0m       \u001b[32m78.1353\u001b[0m  0.0010  12.8302\n",
      "      3       \u001b[36m71.2184\u001b[0m       \u001b[32m64.2184\u001b[0m  0.0010  12.9473\n",
      "      4       \u001b[36m67.5273\u001b[0m       91.7342  0.0010  12.1782\n",
      "      4       \u001b[36m65.5899\u001b[0m       67.0692  0.0010  12.1839\n",
      "      4       \u001b[36m65.4402\u001b[0m       \u001b[32m65.3434\u001b[0m  0.0010  12.1949\n",
      "      4       \u001b[36m67.8235\u001b[0m       \u001b[32m77.0454\u001b[0m  0.0010  12.1598\n",
      "      4       \u001b[36m64.9956\u001b[0m       \u001b[32m59.8770\u001b[0m  0.0010  12.2241\n",
      "      5       \u001b[36m61.8830\u001b[0m      106.4194  0.0010  12.6466\n",
      "      5       \u001b[36m59.8635\u001b[0m       69.2298  0.0010  12.5395\n",
      "      5       \u001b[36m59.3167\u001b[0m       67.9728  0.0010  12.6342\n",
      "      5       \u001b[36m61.7057\u001b[0m       90.9417  0.0010  12.6554\n",
      "      5       \u001b[36m58.4550\u001b[0m       \u001b[32m57.6218\u001b[0m  0.0010  12.7962\n",
      "      6       \u001b[36m56.8615\u001b[0m      108.5354  0.0010  12.3407\n",
      "      6       \u001b[36m52.4767\u001b[0m       \u001b[32m64.7832\u001b[0m  0.0010  12.3416\n",
      "      6       \u001b[36m52.9416\u001b[0m       \u001b[32m65.3113\u001b[0m  0.0010  12.3728\n",
      "      6       \u001b[36m54.6174\u001b[0m       98.5847  0.0010  12.4068\n",
      "      6       \u001b[36m50.3031\u001b[0m       \u001b[32m57.3034\u001b[0m  0.0010  12.4856\n",
      "      7       \u001b[36m50.2660\u001b[0m       92.4164  0.0010  12.1494\n",
      "      7       \u001b[36m46.3722\u001b[0m       \u001b[32m64.7323\u001b[0m  0.0010  12.2220\n",
      "      7       \u001b[36m45.4265\u001b[0m       67.9224  0.0010  12.5368\n",
      "      7       \u001b[36m46.2275\u001b[0m       89.4807  0.0010  12.3808\n",
      "      7       \u001b[36m44.7485\u001b[0m       \u001b[32m50.3095\u001b[0m  0.0010  12.4652\n",
      "      8       \u001b[36m44.4373\u001b[0m       \u001b[32m59.1930\u001b[0m  0.0005  12.8915\n",
      "      8       \u001b[36m41.1955\u001b[0m       \u001b[32m57.4448\u001b[0m  0.0010  12.8723\n",
      "      8       \u001b[36m39.4115\u001b[0m       \u001b[32m47.6179\u001b[0m  0.0010  12.8304\n",
      "      8       \u001b[36m40.8475\u001b[0m      107.6202  0.0010  13.0284\n",
      "      8       \u001b[36m39.7100\u001b[0m       \u001b[32m39.4224\u001b[0m  0.0010  12.7588\n",
      "      9       \u001b[36m43.5032\u001b[0m       \u001b[32m51.3779\u001b[0m  0.0005  13.4248\n",
      "      9       \u001b[36m34.6341\u001b[0m       65.9202  0.0010  13.4548\n",
      "      9       \u001b[36m34.0393\u001b[0m       \u001b[32m42.4501\u001b[0m  0.0010  13.3712\n",
      "      9       \u001b[36m36.0958\u001b[0m       98.7080  0.0010  13.5495\n",
      "      9       \u001b[36m33.0891\u001b[0m       40.5030  0.0010  13.2761\n",
      "     10       \u001b[36m42.2352\u001b[0m       \u001b[32m50.8499\u001b[0m  0.0005  12.7007\n",
      "     10       \u001b[36m24.7378\u001b[0m       \u001b[32m34.8606\u001b[0m  0.0010  12.6913\n",
      "     10       \u001b[36m29.0640\u001b[0m       65.1381  0.0010  12.8547\n",
      "     10       \u001b[36m28.5104\u001b[0m       79.3507  0.0010  12.8641\n",
      "     10       \u001b[36m26.0072\u001b[0m       \u001b[32m28.1536\u001b[0m  0.0010  12.8147\n",
      "     11       \u001b[36m41.2698\u001b[0m       53.9609  0.0005  12.0615\n",
      "     11       \u001b[36m17.7185\u001b[0m       51.8112  0.0010  12.4361\n",
      "     11       \u001b[36m20.8134\u001b[0m       53.9683  0.0010  12.2232\n",
      "     11       \u001b[36m17.7397\u001b[0m       \u001b[32m47.9271\u001b[0m  0.0005  12.0474\n",
      "     11       \u001b[36m20.8889\u001b[0m       32.8387  0.0010  12.4167\n",
      "     12       \u001b[36m39.6794\u001b[0m       59.4686  0.0005  12.7136\n",
      "     12       \u001b[36m16.3411\u001b[0m       34.9732  0.0010  12.7763\n",
      "     12       \u001b[36m18.4733\u001b[0m       \u001b[32m40.3432\u001b[0m  0.0010  12.5766\n",
      "     12       \u001b[36m14.2860\u001b[0m       \u001b[32m37.2515\u001b[0m  0.0005  12.8351\n",
      "     12       \u001b[36m19.3525\u001b[0m       72.1224  0.0010  12.9326\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "     13       \u001b[36m37.1210\u001b[0m       61.8509  0.0005  12.4591\n",
      "     13       \u001b[36m13.9281\u001b[0m       \u001b[32m28.5795\u001b[0m  0.0010  12.5822\n",
      "     13       \u001b[36m15.5477\u001b[0m       \u001b[32m26.8369\u001b[0m  0.0010  12.5452\n",
      "     13       \u001b[36m12.5737\u001b[0m       40.4890  0.0005  12.6213\n",
      "     13       19.9554       28.7642  0.0010  12.6211\n",
      "     14       \u001b[36m33.6936\u001b[0m       56.5983  0.0005  12.9846\n",
      "     14       \u001b[36m13.5538\u001b[0m       \u001b[32m26.2763\u001b[0m  0.0010  12.8330\n",
      "     14       \u001b[36m12.3008\u001b[0m       33.2701  0.0010  13.0112\n",
      "     14       \u001b[36m11.4225\u001b[0m       41.1816  0.0005  12.7858\n",
      "     14       \u001b[36m16.6204\u001b[0m       45.2619  0.0010  12.9007\n",
      "     15       \u001b[36m29.0027\u001b[0m       51.1281  0.0003  12.5178\n",
      "     15       \u001b[36m12.9642\u001b[0m       33.6025  0.0010  12.3107\n",
      "     15       \u001b[36m11.5528\u001b[0m       35.8852  0.0010  12.5908\n",
      "     15       \u001b[36m10.6223\u001b[0m       42.5224  0.0005  12.7104\n",
      "     15       \u001b[36m14.6788\u001b[0m       \u001b[32m21.1797\u001b[0m  0.0005  12.6922\n",
      "     16       \u001b[36m23.2356\u001b[0m       \u001b[32m48.3011\u001b[0m  0.0003  14.0726\n",
      "     16       13.4362       37.0153  0.0010  13.8618\n",
      "     16       11.7223       40.7809  0.0010  14.0174\n",
      "     16       \u001b[36m10.0170\u001b[0m       42.7992  0.0005  14.4603\n",
      "     16       \u001b[36m11.7529\u001b[0m       \u001b[32m19.9361\u001b[0m  0.0005  14.2289\n",
      "     17       \u001b[36m18.9532\u001b[0m       \u001b[32m44.9171\u001b[0m  0.0003  14.5203\n",
      "     17       13.8516       40.0929  0.0010  14.4623\n",
      "     17       12.8269       48.1219  0.0010  14.5813\n",
      "     17       10.8047       41.4058  0.0003  14.5722\n",
      "     17       \u001b[36m10.7505\u001b[0m       20.1544  0.0005  14.2091\n",
      "     18       15.3749       46.7374  0.0010  13.0964\n",
      "     18       \u001b[36m15.6748\u001b[0m       \u001b[32m44.3360\u001b[0m  0.0003  13.4080\n",
      "     18       15.1031       45.0544  0.0005  13.2851\n",
      "     18       \u001b[36m10.1714\u001b[0m       20.0410  0.0005  13.1602\n",
      "     18       10.4923       \u001b[32m35.3022\u001b[0m  0.0003  13.4621\n",
      "     19       16.9743      100.2376  0.0005  12.9655\n",
      "     19       \u001b[36m13.7405\u001b[0m       \u001b[32m43.4871\u001b[0m  0.0003  13.0476\n",
      "     19       15.2137       38.8573  0.0005  13.2025\n",
      "     19        \u001b[36m9.8374\u001b[0m       21.3340  0.0005  13.4365\n",
      "     19       10.0987       \u001b[32m34.0232\u001b[0m  0.0003  13.2246\n",
      "     20       13.8564       45.2096  0.0005  13.1488\n",
      "     20       \u001b[36m12.4757\u001b[0m       \u001b[32m41.7133\u001b[0m  0.0003  13.2105\n",
      "     20       13.3867       33.1076  0.0005  13.1096\n",
      "     20        \u001b[36m9.3873\u001b[0m       22.1475  0.0005  12.9312\n",
      "     20        \u001b[36m9.7794\u001b[0m       \u001b[32m33.9992\u001b[0m  0.0003  13.2867\n",
      "     21       \u001b[36m12.3426\u001b[0m       40.4246  0.0005  12.7248\n",
      "     21       \u001b[36m11.5231\u001b[0m       \u001b[32m40.5006\u001b[0m  0.0003  12.9940\n",
      "     21       11.7868       \u001b[32m27.0396\u001b[0m  0.0005  12.8823\n",
      "     21        \u001b[36m9.1202\u001b[0m       23.6145  0.0003  12.9317\n",
      "     21        \u001b[36m9.5831\u001b[0m       35.3551  0.0003  12.9810\n",
      "     22       \u001b[36m10.9567\u001b[0m       34.8483  0.0005  12.7393\n",
      "     22       \u001b[36m10.7855\u001b[0m       \u001b[32m38.7069\u001b[0m  0.0003  13.1986\n",
      "     22        \u001b[36m9.6440\u001b[0m       \u001b[32m24.1274\u001b[0m  0.0005  12.9346\n",
      "     22        \u001b[36m8.8561\u001b[0m       23.2591  0.0003  12.8870\n",
      "     22        \u001b[36m9.4280\u001b[0m       37.1050  0.0003  13.2153\n",
      "     23       \u001b[36m10.3697\u001b[0m       27.3312  0.0003  13.2498\n",
      "     23       \u001b[36m10.2233\u001b[0m       \u001b[32m37.9494\u001b[0m  0.0003  13.2040\n",
      "     23        \u001b[36m8.2566\u001b[0m       \u001b[32m22.9615\u001b[0m  0.0005  13.2762\n",
      "     23        \u001b[36m8.6576\u001b[0m       22.9351  0.0003  13.1425\n",
      "     23        \u001b[36m9.2859\u001b[0m       38.4472  0.0003  13.4355\n",
      "     24       10.7079       \u001b[32m24.9671\u001b[0m  0.0003  12.9105\n",
      "     24        \u001b[36m9.7594\u001b[0m       \u001b[32m37.4367\u001b[0m  0.0003  13.0890\n",
      "     24        \u001b[36m7.5349\u001b[0m       \u001b[32m22.4374\u001b[0m  0.0005  12.9148\n",
      "     24        \u001b[36m8.5080\u001b[0m       22.5514  0.0003  12.9973\n",
      "     24        \u001b[36m9.1349\u001b[0m       38.8397  0.0003  13.1283\n",
      "     25       \u001b[36m10.2443\u001b[0m       \u001b[32m23.8673\u001b[0m  0.0003  13.2061\n",
      "     25        \u001b[36m9.3586\u001b[0m       \u001b[32m37.0714\u001b[0m  0.0003  13.4076\n",
      "     25        \u001b[36m7.1402\u001b[0m       \u001b[32m22.3126\u001b[0m  0.0005  13.3316\n",
      "     25        \u001b[36m8.4639\u001b[0m       \u001b[32m19.4176\u001b[0m  0.0001  13.4266\n",
      "     25       10.0616       34.0096  0.0001  13.5006\n",
      "     26        \u001b[36m9.6785\u001b[0m       \u001b[32m22.9160\u001b[0m  0.0003  13.1933\n",
      "     26        \u001b[36m9.0172\u001b[0m       \u001b[32m36.9437\u001b[0m  0.0003  13.3319\n",
      "     26        \u001b[36m6.9011\u001b[0m       22.5285  0.0005  13.2548\n",
      "     26        \u001b[36m8.2186\u001b[0m       \u001b[32m19.0505\u001b[0m  0.0001  13.4503\n",
      "     26        9.5392       \u001b[32m32.4592\u001b[0m  0.0001  13.7450\n",
      "     27        \u001b[36m9.1733\u001b[0m       \u001b[32m22.3588\u001b[0m  0.0003  13.4314\n",
      "     27        \u001b[36m6.7315\u001b[0m       22.9746  0.0005  13.2876\n",
      "     27        \u001b[36m8.7039\u001b[0m       \u001b[32m36.9324\u001b[0m  0.0003  13.5328\n",
      "     27        \u001b[36m7.9947\u001b[0m       \u001b[32m18.9729\u001b[0m  0.0001  13.3475\n",
      "     27        \u001b[36m8.7518\u001b[0m       \u001b[32m31.0119\u001b[0m  0.0001  13.6548\n",
      "     28        \u001b[36m8.7483\u001b[0m       \u001b[32m22.0951\u001b[0m  0.0003  13.4640\n",
      "     28        \u001b[36m6.6117\u001b[0m       23.3424  0.0005  13.7415\n",
      "     28        \u001b[36m8.4299\u001b[0m       36.9894  0.0003  13.8832\n",
      "     28        \u001b[36m7.8334\u001b[0m       \u001b[32m18.8987\u001b[0m  0.0001  13.8070\n",
      "     28        \u001b[36m8.3247\u001b[0m       \u001b[32m30.3827\u001b[0m  0.0001  14.4412\n",
      "     29        \u001b[36m8.3913\u001b[0m       \u001b[32m21.8876\u001b[0m  0.0003  14.8319\n",
      "     29        \u001b[36m6.5177\u001b[0m       23.6714  0.0005  15.0713\n",
      "     29        \u001b[36m8.1872\u001b[0m       36.9725  0.0003  15.0502\n",
      "     29        \u001b[36m7.7048\u001b[0m       \u001b[32m18.8250\u001b[0m  0.0001  15.1517\n",
      "     29        \u001b[36m8.0270\u001b[0m       \u001b[32m30.2814\u001b[0m  0.0001  15.0059\n",
      "     30        \u001b[36m8.0962\u001b[0m       \u001b[32m21.8340\u001b[0m  0.0003  14.4999\n",
      "     30        6.8445       24.5197  0.0003  14.5180\n",
      "     30        \u001b[36m7.9682\u001b[0m       \u001b[32m36.6988\u001b[0m  0.0003  14.3509\n",
      "     30        \u001b[36m7.5960\u001b[0m       \u001b[32m18.7211\u001b[0m  0.0001  14.3622\n",
      "     30        \u001b[36m7.7938\u001b[0m       \u001b[32m30.2137\u001b[0m  0.0001  14.0745\n",
      "     31        \u001b[36m7.8415\u001b[0m       22.0316  0.0003  13.2950\n",
      "     31        7.0363       23.7530  0.0003  13.5850\n",
      "     31        \u001b[36m7.7670\u001b[0m       36.7685  0.0003  13.6605\n",
      "     31        \u001b[36m7.5025\u001b[0m       \u001b[32m18.6925\u001b[0m  0.0001  13.6580\n",
      "     31        \u001b[36m7.6027\u001b[0m       \u001b[32m29.9150\u001b[0m  0.0001  13.9660\n",
      "     32        \u001b[36m7.6265\u001b[0m       22.3768  0.0003  14.1312\n",
      "     32        6.7710       23.0550  0.0003  14.2372\n",
      "     32        \u001b[36m7.5936\u001b[0m       36.8091  0.0003  14.3959\n",
      "     32        \u001b[36m7.4187\u001b[0m       \u001b[32m18.5884\u001b[0m  0.0001  14.1811\n",
      "     32        \u001b[36m7.4395\u001b[0m       29.9314  0.0001  14.4320\n",
      "     33        \u001b[36m7.4393\u001b[0m       22.3920  0.0003  14.3815\n",
      "     33        6.5844       22.5572  0.0003  14.3956\n",
      "     33        \u001b[36m7.4347\u001b[0m       36.7749  0.0003  14.6230\n",
      "     33        \u001b[36m7.3451\u001b[0m       \u001b[32m18.5653\u001b[0m  0.0001  14.2463\n",
      "     33        \u001b[36m7.2981\u001b[0m       29.9212  0.0001  14.5932\n",
      "     34        \u001b[36m7.2716\u001b[0m       22.6506  0.0003  14.2324\n",
      "     34        \u001b[36m6.3633\u001b[0m       \u001b[32m21.0732\u001b[0m  0.0001  14.3585\n",
      "     34        \u001b[36m7.2844\u001b[0m       37.0964  0.0003  14.4605\n",
      "     34        \u001b[36m7.2781\u001b[0m       \u001b[32m18.5293\u001b[0m  0.0001  14.3153\n",
      "     34        \u001b[36m7.1722\u001b[0m       \u001b[32m29.7672\u001b[0m  0.0001  14.5329\n",
      "     35        \u001b[36m6.9800\u001b[0m       22.8190  0.0001  14.1174\n",
      "     35        \u001b[36m6.2002\u001b[0m       21.1929  0.0001  14.3995\n",
      "     35        \u001b[36m7.2149\u001b[0m       \u001b[32m18.4867\u001b[0m  0.0001  14.2221\n",
      "     35        7.4606       \u001b[32m35.6852\u001b[0m  0.0001  14.5002\n",
      "     35        \u001b[36m7.0582\u001b[0m       \u001b[32m29.7558\u001b[0m  0.0001  14.5712\n",
      "     36        \u001b[36m6.7244\u001b[0m       23.0887  0.0001  15.1351\n",
      "     36        \u001b[36m6.0687\u001b[0m       21.1001  0.0001  15.4543\n",
      "     36        \u001b[36m7.1574\u001b[0m       \u001b[32m18.4458\u001b[0m  0.0001  15.5459\n",
      "     36        7.6817       36.8117  0.0001  15.5197\n",
      "     36        \u001b[36m6.9560\u001b[0m       \u001b[32m29.6720\u001b[0m  0.0001  15.5569\n",
      "     37        \u001b[36m6.6421\u001b[0m       22.9795  0.0001  14.3694\n",
      "     37        \u001b[36m5.9689\u001b[0m       21.0821  0.0001  14.3296\n",
      "     37        \u001b[36m7.1047\u001b[0m       \u001b[32m18.3951\u001b[0m  0.0001  14.2782\n",
      "     37        7.4243       37.8004  0.0001  14.3189\n",
      "     37        \u001b[36m6.8621\u001b[0m       \u001b[32m29.5223\u001b[0m  0.0001  14.3536\n",
      "     38        \u001b[36m6.5664\u001b[0m       23.0144  0.0001  14.0771\n",
      "     38        \u001b[36m5.8835\u001b[0m       \u001b[32m21.0434\u001b[0m  0.0001  14.5345\n",
      "     38        \u001b[36m7.0545\u001b[0m       \u001b[32m18.3426\u001b[0m  0.0001  14.5430\n",
      "     38        \u001b[36m7.1880\u001b[0m       38.4020  0.0001  14.4782\n",
      "     38        \u001b[36m6.7756\u001b[0m       29.6734  0.0001  14.7001\n",
      "     39        \u001b[36m6.3621\u001b[0m       23.1576  0.0001  14.3163\n",
      "     39        \u001b[36m5.8096\u001b[0m       \u001b[32m21.0357\u001b[0m  0.0001  14.5277\n",
      "     39        \u001b[36m7.0079\u001b[0m       \u001b[32m18.3060\u001b[0m  0.0001  14.5720\n",
      "     39        \u001b[36m6.9887\u001b[0m       38.0229  0.0001  14.6926\n",
      "     39        \u001b[36m6.6959\u001b[0m       29.6142  0.0001  14.9531\n",
      "     40        \u001b[36m5.7452\u001b[0m       21.0457  0.0001  15.5591\n",
      "     40        \u001b[36m6.9634\u001b[0m       18.3273  0.0001  15.5756\n",
      "     40        \u001b[36m6.7121\u001b[0m       \u001b[32m32.0497\u001b[0m  0.0001  15.8189\n",
      "     40        \u001b[36m6.6218\u001b[0m       29.7138  0.0001  15.5237\n",
      "     41        \u001b[36m5.6877\u001b[0m       21.0456  0.0001  13.6161\n",
      "     41        \u001b[36m6.9206\u001b[0m       \u001b[32m18.2787\u001b[0m  0.0001  13.3487\n",
      "     41        \u001b[36m6.5969\u001b[0m       32.2225  0.0001  13.6246\n",
      "     41        \u001b[36m6.5515\u001b[0m       29.5399  0.0001  13.9098\n",
      "     42        \u001b[36m5.6362\u001b[0m       21.0762  0.0001  13.7434\n",
      "     42        \u001b[36m6.8805\u001b[0m       \u001b[32m18.2489\u001b[0m  0.0001  13.6286\n",
      "     42        \u001b[36m6.5190\u001b[0m       \u001b[32m32.0296\u001b[0m  0.0001  13.6872\n",
      "     42        \u001b[36m6.3630\u001b[0m       31.3387  0.0001  13.8809\n",
      "     43        \u001b[36m5.5891\u001b[0m       21.0656  0.0001  13.8628\n",
      "     43        \u001b[36m6.8406\u001b[0m       18.2563  0.0001  13.7893\n",
      "     43        \u001b[36m6.4511\u001b[0m       \u001b[32m31.7397\u001b[0m  0.0001  13.9554\n",
      "     43        \u001b[36m6.2149\u001b[0m       32.4094  0.0001  13.5301\n",
      "     44        \u001b[36m5.4725\u001b[0m       21.7615  0.0001  13.2504\n",
      "     44        \u001b[36m6.8025\u001b[0m       18.2627  0.0001  13.3819\n",
      "     44        \u001b[36m6.3917\u001b[0m       \u001b[32m31.5858\u001b[0m  0.0001  13.3531\n",
      "     44        \u001b[36m6.1678\u001b[0m       32.7226  0.0001  13.7671\n",
      "     45        \u001b[36m5.3907\u001b[0m       21.7943  0.0001  13.3423\n",
      "     45        \u001b[36m6.7670\u001b[0m       \u001b[32m18.2289\u001b[0m  0.0001  13.3742\n",
      "     45        \u001b[36m6.3371\u001b[0m       \u001b[32m31.4327\u001b[0m  0.0001  13.6264\n",
      "     45        \u001b[36m6.1265\u001b[0m       33.0147  0.0001  13.3195\n",
      "     46        \u001b[36m5.3634\u001b[0m       21.8255  0.0001  13.0501\n",
      "     46        \u001b[36m6.7303\u001b[0m       \u001b[32m18.2267\u001b[0m  0.0001  13.0783\n",
      "     46        \u001b[36m6.2867\u001b[0m       \u001b[32m31.2232\u001b[0m  0.0001  13.4415\n",
      "     46        \u001b[36m5.9632\u001b[0m       33.1334  0.0000  13.9495\n",
      "     47        \u001b[36m5.3390\u001b[0m       21.8688  0.0001  13.9310\n",
      "     47        \u001b[36m6.6956\u001b[0m       \u001b[32m18.2105\u001b[0m  0.0001  13.8535\n",
      "     47        \u001b[36m6.2400\u001b[0m       \u001b[32m31.0667\u001b[0m  0.0001  13.7422\n",
      "     48        \u001b[36m5.2398\u001b[0m       21.6425  0.0000  13.8642\n",
      "     48        \u001b[36m6.6622\u001b[0m       18.2279  0.0001  13.7081\n",
      "     48        \u001b[36m6.1948\u001b[0m       \u001b[32m30.9382\u001b[0m  0.0001  13.7196\n",
      "     49        \u001b[36m6.6289\u001b[0m       \u001b[32m18.1808\u001b[0m  0.0001  13.6714\n",
      "     49        \u001b[36m6.1532\u001b[0m       \u001b[32m30.8185\u001b[0m  0.0001  13.7529\n",
      "     50        \u001b[36m6.5959\u001b[0m       \u001b[32m18.1464\u001b[0m  0.0001  10.2530\n",
      "     50        \u001b[36m6.1126\u001b[0m       \u001b[32m30.7501\u001b[0m  0.0001  10.0520\n",
      "     51        \u001b[36m6.5644\u001b[0m       \u001b[32m18.1288\u001b[0m  0.0001  9.6251\n",
      "     51        \u001b[36m6.0739\u001b[0m       \u001b[32m30.6895\u001b[0m  0.0001  10.0231\n",
      "     52        \u001b[36m6.5343\u001b[0m       \u001b[32m18.1206\u001b[0m  0.0001  10.2571\n",
      "     52        \u001b[36m6.0368\u001b[0m       \u001b[32m30.6032\u001b[0m  0.0001  10.1024\n",
      "     53        \u001b[36m6.5027\u001b[0m       \u001b[32m18.0976\u001b[0m  0.0001  9.6914\n",
      "     53        \u001b[36m6.0012\u001b[0m       \u001b[32m30.4908\u001b[0m  0.0001  9.7669\n",
      "     54        \u001b[36m6.4736\u001b[0m       18.1063  0.0001  9.9763\n",
      "     54        \u001b[36m5.9668\u001b[0m       \u001b[32m30.4189\u001b[0m  0.0001  10.4302\n",
      "     55        \u001b[36m6.4432\u001b[0m       \u001b[32m18.0727\u001b[0m  0.0001  11.4824\n",
      "     55        \u001b[36m5.9332\u001b[0m       \u001b[32m30.3102\u001b[0m  0.0001  11.3831\n",
      "     56        \u001b[36m6.4142\u001b[0m       \u001b[32m18.0400\u001b[0m  0.0001  9.5819\n",
      "     56        \u001b[36m5.9013\u001b[0m       \u001b[32m30.2085\u001b[0m  0.0001  9.4709\n",
      "     57        \u001b[36m6.3851\u001b[0m       \u001b[32m18.0124\u001b[0m  0.0001  10.1353\n",
      "     57        \u001b[36m5.8693\u001b[0m       \u001b[32m30.0975\u001b[0m  0.0001  10.2687\n",
      "     58        \u001b[36m6.3565\u001b[0m       18.0268  0.0001  9.9822\n",
      "     58        \u001b[36m5.8386\u001b[0m       \u001b[32m30.0536\u001b[0m  0.0001  10.2135\n",
      "     59        \u001b[36m6.3290\u001b[0m       \u001b[32m18.0026\u001b[0m  0.0001  11.4248\n",
      "     59        \u001b[36m5.8086\u001b[0m       \u001b[32m29.9382\u001b[0m  0.0001  11.4834\n",
      "     60        \u001b[36m6.3006\u001b[0m       \u001b[32m17.9809\u001b[0m  0.0001  9.9766\n",
      "     60        \u001b[36m5.7798\u001b[0m       \u001b[32m29.8180\u001b[0m  0.0001  11.2072\n",
      "     61        \u001b[36m6.2740\u001b[0m       \u001b[32m17.9670\u001b[0m  0.0001  11.1677\n",
      "     61        \u001b[36m5.7505\u001b[0m       \u001b[32m29.7202\u001b[0m  0.0001  10.1497\n",
      "     62        \u001b[36m6.2451\u001b[0m       17.9785  0.0001  11.6688\n",
      "     62        \u001b[36m5.7231\u001b[0m       \u001b[32m29.6137\u001b[0m  0.0001  11.8073\n",
      "     63        \u001b[36m6.2172\u001b[0m       17.9902  0.0001  10.7640\n",
      "     63        \u001b[36m5.6951\u001b[0m       \u001b[32m29.5260\u001b[0m  0.0001  10.7783\n",
      "     64        \u001b[36m6.1889\u001b[0m       \u001b[32m17.9533\u001b[0m  0.0001  10.2251\n",
      "     64        \u001b[36m5.6679\u001b[0m       \u001b[32m29.5101\u001b[0m  0.0001  10.4823\n",
      "     65        \u001b[36m6.1626\u001b[0m       \u001b[32m17.9369\u001b[0m  0.0001  10.2821\n",
      "     65        \u001b[36m5.6413\u001b[0m       \u001b[32m29.3819\u001b[0m  0.0001  10.2689\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "     66        \u001b[36m6.1353\u001b[0m       17.9839  0.0001  9.9946\n",
      "     66        \u001b[36m5.6144\u001b[0m       \u001b[32m29.2638\u001b[0m  0.0001  10.0126\n",
      "     67        \u001b[36m6.1085\u001b[0m       17.9608  0.0001  10.5975\n",
      "     67        \u001b[36m5.5884\u001b[0m       \u001b[32m29.1855\u001b[0m  0.0001  13.0204\n",
      "     68        \u001b[36m6.0822\u001b[0m       17.9560  0.0001  12.1941\n",
      "     68        \u001b[36m5.5626\u001b[0m       \u001b[32m29.0824\u001b[0m  0.0001  9.8016\n",
      "     69        \u001b[36m6.0542\u001b[0m       17.9569  0.0001  9.4728\n",
      "     69        \u001b[36m5.5375\u001b[0m       \u001b[32m29.0555\u001b[0m  0.0001  9.5218\n",
      "     70        \u001b[36m5.8749\u001b[0m       18.4210  0.0001  10.4846\n",
      "     70        \u001b[36m5.5128\u001b[0m       \u001b[32m28.9176\u001b[0m  0.0001  10.7155\n",
      "     71        \u001b[36m5.8220\u001b[0m       18.2724  0.0001  9.6219\n",
      "     71        \u001b[36m5.4883\u001b[0m       \u001b[32m28.7902\u001b[0m  0.0001  9.5366\n",
      "     72        \u001b[36m5.8067\u001b[0m       18.3262  0.0001  9.4886\n",
      "     72        \u001b[36m5.4641\u001b[0m       \u001b[32m28.7066\u001b[0m  0.0001  9.6845\n",
      "     73        \u001b[36m5.7908\u001b[0m       18.3930  0.0001  9.5229\n",
      "     73        \u001b[36m5.4409\u001b[0m       \u001b[32m28.6239\u001b[0m  0.0001  9.5371\n",
      "     74        \u001b[36m5.6916\u001b[0m       19.1016  0.0000  9.5675\n",
      "     74        \u001b[36m5.4168\u001b[0m       \u001b[32m28.5161\u001b[0m  0.0001  9.6594\n",
      "     75        \u001b[36m5.3936\u001b[0m       \u001b[32m28.4358\u001b[0m  0.0001  9.1335\n",
      "     76        \u001b[36m5.3703\u001b[0m       \u001b[32m28.3129\u001b[0m  0.0001  7.3238\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "     77        \u001b[36m5.3475\u001b[0m       \u001b[32m28.2017\u001b[0m  0.0001  7.6347\n",
      "     78        \u001b[36m5.3246\u001b[0m       \u001b[32m28.1754\u001b[0m  0.0001  7.1852\n",
      "     79        \u001b[36m5.3022\u001b[0m       \u001b[32m28.0551\u001b[0m  0.0001  7.6726\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "     80        \u001b[36m5.2796\u001b[0m       \u001b[32m27.9445\u001b[0m  0.0001  7.6300\n",
      "     81        \u001b[36m5.2576\u001b[0m       27.9457  0.0001  7.4116\n",
      "     82        \u001b[36m5.2349\u001b[0m       \u001b[32m27.9210\u001b[0m  0.0001  7.2682\n",
      "     83        \u001b[36m5.2130\u001b[0m       \u001b[32m27.8172\u001b[0m  0.0001  7.3842\n",
      "     84        \u001b[36m5.1907\u001b[0m       \u001b[32m27.7074\u001b[0m  0.0001  7.5259\n",
      "     85        \u001b[36m5.1697\u001b[0m       \u001b[32m27.5153\u001b[0m  0.0001  7.2197\n",
      "     86        \u001b[36m5.1474\u001b[0m       27.6173  0.0001  7.6374\n",
      "     87        \u001b[36m5.1258\u001b[0m       \u001b[32m27.4265\u001b[0m  0.0001  7.3844\n",
      "     88        \u001b[36m5.1036\u001b[0m       \u001b[32m27.4127\u001b[0m  0.0001  7.9262\n",
      "     89        \u001b[36m5.0813\u001b[0m       \u001b[32m27.2923\u001b[0m  0.0001  7.0366\n",
      "     90        \u001b[36m5.0590\u001b[0m       27.4183  0.0001  7.1412\n",
      "     91        \u001b[36m5.0368\u001b[0m       \u001b[32m27.2612\u001b[0m  0.0001  7.0050\n",
      "     92        \u001b[36m5.0144\u001b[0m       27.3443  0.0001  7.0039\n",
      "     93        \u001b[36m4.9926\u001b[0m       \u001b[32m27.2215\u001b[0m  0.0001  6.9832\n",
      "     94        \u001b[36m4.9713\u001b[0m       \u001b[32m27.0902\u001b[0m  0.0001  7.0448\n",
      "     95        \u001b[36m4.9499\u001b[0m       \u001b[32m27.0579\u001b[0m  0.0001  7.0162\n",
      "     96        \u001b[36m4.9282\u001b[0m       \u001b[32m27.0312\u001b[0m  0.0001  6.9584\n",
      "     97        \u001b[36m4.9071\u001b[0m       \u001b[32m26.9155\u001b[0m  0.0001  6.9694\n",
      "     98        \u001b[36m4.8859\u001b[0m       \u001b[32m26.9068\u001b[0m  0.0001  6.9214\n",
      "     99        \u001b[36m4.8646\u001b[0m       26.9205  0.0001  7.0467\n",
      "    100        \u001b[36m4.8437\u001b[0m       \u001b[32m26.8046\u001b[0m  0.0001  7.0372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m217.7911\u001b[0m      \u001b[32m307.9967\u001b[0m  0.0010  15.9992\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m202.6911\u001b[0m      \u001b[32m235.5637\u001b[0m  0.0010  16.4177\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m229.1994\u001b[0m      \u001b[32m240.4752\u001b[0m  0.0010  17.3499\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m220.2596\u001b[0m      \u001b[32m237.3494\u001b[0m  0.0010  17.4009\n",
      "  epoch    train_loss    valid_loss      lr      dur\n",
      "-------  ------------  ------------  ------  -------\n",
      "      1      \u001b[36m233.7318\u001b[0m      \u001b[32m145.7822\u001b[0m  0.0010  17.3581\n",
      "      2      \u001b[36m155.0827\u001b[0m      \u001b[32m247.6790\u001b[0m  0.0010  16.7890\n",
      "      2      \u001b[36m149.7581\u001b[0m      \u001b[32m134.6022\u001b[0m  0.0010  17.1008\n",
      "      2      \u001b[36m174.2996\u001b[0m      \u001b[32m193.9991\u001b[0m  0.0010  17.1581\n",
      "      2      \u001b[36m162.8379\u001b[0m      \u001b[32m159.2743\u001b[0m  0.0010  17.2443\n",
      "      2      \u001b[36m167.4768\u001b[0m      \u001b[32m138.7075\u001b[0m  0.0010  17.4401\n",
      "      3      \u001b[36m147.4087\u001b[0m      \u001b[32m240.6949\u001b[0m  0.0010  16.9558\n",
      "      3      \u001b[36m141.2650\u001b[0m      144.6005  0.0010  17.0300\n",
      "      3      \u001b[36m157.1180\u001b[0m      161.9443  0.0010  16.7003\n",
      "      3      \u001b[36m171.4072\u001b[0m      \u001b[32m188.8503\u001b[0m  0.0010  16.9408\n",
      "      3      \u001b[36m162.4483\u001b[0m      \u001b[32m134.1495\u001b[0m  0.0010  16.7792\n",
      "      4      \u001b[36m142.2427\u001b[0m      244.9372  0.0010  16.6932\n",
      "      4      \u001b[36m135.5729\u001b[0m      139.6286  0.0010  17.1318\n",
      "      4      \u001b[36m152.8279\u001b[0m      165.9792  0.0010  17.3332\n",
      "      4      \u001b[36m169.0650\u001b[0m      \u001b[32m187.8542\u001b[0m  0.0010  17.3168\n",
      "      4      \u001b[36m158.7193\u001b[0m      \u001b[32m127.8177\u001b[0m  0.0010  17.5153\n",
      "      5      \u001b[36m138.0220\u001b[0m      245.7353  0.0010  17.3059\n",
      "      5      \u001b[36m129.2462\u001b[0m      150.4052  0.0010  17.5427\n",
      "      5      \u001b[36m165.9103\u001b[0m      \u001b[32m181.6186\u001b[0m  0.0010  17.2718\n",
      "      5      \u001b[36m148.7995\u001b[0m      166.1479  0.0010  17.4775\n",
      "      5      \u001b[36m153.1942\u001b[0m      \u001b[32m123.3407\u001b[0m  0.0010  17.3601\n",
      "      6      \u001b[36m132.3890\u001b[0m      \u001b[32m240.6209\u001b[0m  0.0010  16.2468\n",
      "      6      \u001b[36m123.1220\u001b[0m      143.7801  0.0010  16.4749\n",
      "      6      \u001b[36m158.9438\u001b[0m      \u001b[32m178.0417\u001b[0m  0.0010  16.5564\n",
      "      6      \u001b[36m143.7151\u001b[0m      171.4318  0.0010  16.8868\n",
      "      6      \u001b[36m146.4917\u001b[0m      124.0477  0.0010  16.7016\n",
      "      7      \u001b[36m125.5803\u001b[0m      275.4295  0.0010  16.8742\n",
      "      7      \u001b[36m110.1285\u001b[0m      135.6944  0.0005  16.9566\n",
      "      7      \u001b[36m152.2514\u001b[0m      181.1863  0.0010  16.9740\n",
      "      7      152.4219      167.4703  0.0005  16.6967\n",
      "      7      \u001b[36m139.1632\u001b[0m      \u001b[32m117.4334\u001b[0m  0.0010  16.7776\n",
      "      8      \u001b[36m120.0781\u001b[0m      289.5502  0.0010  16.6357\n",
      "      8      \u001b[36m107.4771\u001b[0m      143.8702  0.0005  16.8674\n",
      "      8      153.4663      \u001b[32m155.0629\u001b[0m  0.0005  16.9978\n",
      "      8      \u001b[36m144.4773\u001b[0m      191.4396  0.0010  17.2407\n",
      "      8      \u001b[36m133.8912\u001b[0m      \u001b[32m116.7374\u001b[0m  0.0010  17.0888\n",
      "      9      \u001b[36m116.4602\u001b[0m      287.7888  0.0010  17.1187\n",
      "      9      \u001b[36m104.6695\u001b[0m      149.9024  0.0005  17.3560\n",
      "      9      149.1179      \u001b[32m149.0124\u001b[0m  0.0005  18.1866\n",
      "      9      \u001b[36m142.0652\u001b[0m      191.3763  0.0010  18.1632\n",
      "      9      \u001b[36m129.4326\u001b[0m      120.0077  0.0010  17.9885\n",
      "     10      \u001b[36m112.7861\u001b[0m      292.2317  0.0010  17.7220\n",
      "     10      \u001b[36m101.9602\u001b[0m      157.0362  0.0005  17.9541\n",
      "     10      \u001b[36m140.6777\u001b[0m      \u001b[32m177.6175\u001b[0m  0.0010  17.2615\n",
      "     10      \u001b[36m121.3688\u001b[0m      117.7273  0.0010  17.2467\n",
      "     10      145.0251      152.3880  0.0005  17.5272\n",
      "     11      \u001b[36m105.2265\u001b[0m      260.8146  0.0005  16.8088\n",
      "     11       \u001b[36m95.5271\u001b[0m      \u001b[32m132.3444\u001b[0m  0.0003  16.9855\n",
      "     11      \u001b[36m140.5931\u001b[0m      184.0932  0.0010  16.3678\n",
      "     11      \u001b[36m141.2127\u001b[0m      157.1053  0.0005  16.2658\n",
      "     11      \u001b[36m120.6222\u001b[0m      \u001b[32m116.6550\u001b[0m  0.0010  16.6153\n",
      "     12      \u001b[36m102.2114\u001b[0m      285.4290  0.0005  16.3508\n",
      "     12       \u001b[36m93.4188\u001b[0m      \u001b[32m131.1072\u001b[0m  0.0003  16.6927\n",
      "     12      \u001b[36m138.0628\u001b[0m      162.6505  0.0005  16.6290\n",
      "     12      \u001b[36m139.5187\u001b[0m      \u001b[32m170.4049\u001b[0m  0.0010  16.7908\n",
      "     12      \u001b[36m116.3343\u001b[0m      120.6772  0.0010  16.5167\n",
      "     13       \u001b[36m99.2949\u001b[0m      302.3869  0.0005  16.8305\n",
      "     13       \u001b[36m91.3106\u001b[0m      131.2607  0.0003  17.3632\n",
      "     13      \u001b[36m139.1718\u001b[0m      \u001b[32m163.1410\u001b[0m  0.0010  17.3484\n",
      "     13      \u001b[36m136.8384\u001b[0m      158.6643  0.0005  17.4839\n",
      "     13      \u001b[36m112.7826\u001b[0m      118.2419  0.0010  17.3674\n",
      "     14       \u001b[36m96.6340\u001b[0m      309.9844  0.0005  18.4712\n",
      "     14       \u001b[36m89.4728\u001b[0m      \u001b[32m130.0352\u001b[0m  0.0003  18.5209\n",
      "     14      \u001b[36m108.5862\u001b[0m      \u001b[32m113.3832\u001b[0m  0.0010  18.2491\n",
      "     14      \u001b[36m137.8925\u001b[0m      \u001b[32m150.7381\u001b[0m  0.0010  18.3428\n",
      "     14      137.5189      \u001b[32m132.7789\u001b[0m  0.0003  18.4832\n",
      "     15       \u001b[36m91.3199\u001b[0m      253.5638  0.0003  18.7019\n",
      "     15       \u001b[36m87.6264\u001b[0m      \u001b[32m129.8371\u001b[0m  0.0003  19.2513\n",
      "     15      \u001b[36m104.1001\u001b[0m      116.0306  0.0010  18.8056\n",
      "     15      138.0906      \u001b[32m139.5243\u001b[0m  0.0010  19.1687\n",
      "     15      139.5504      134.0155  0.0003  19.5298\n",
      "     16       \u001b[36m85.7664\u001b[0m      \u001b[32m129.3392\u001b[0m  0.0003  18.4750\n",
      "     16       \u001b[36m96.5994\u001b[0m      120.2355  0.0010  17.7322\n",
      "     16      \u001b[36m136.4253\u001b[0m      \u001b[32m136.1883\u001b[0m  0.0010  17.8898\n",
      "     16      139.6710      133.5076  0.0003  17.7174\n",
      "     17       \u001b[36m83.8682\u001b[0m      \u001b[32m129.1567\u001b[0m  0.0003  15.6511\n",
      "     17       \u001b[36m95.0927\u001b[0m      122.4584  0.0010  15.7427\n",
      "     17      \u001b[36m136.1565\u001b[0m      147.1323  0.0010  15.9830\n",
      "     17      138.3011      133.6344  0.0003  15.8504\n",
      "     18       \u001b[36m81.9130\u001b[0m      \u001b[32m128.9490\u001b[0m  0.0003  15.4477\n",
      "     18       \u001b[36m90.0946\u001b[0m      121.9314  0.0010  15.5249\n",
      "     18      \u001b[36m134.2779\u001b[0m      137.4381  0.0010  15.6420\n",
      "     18      137.0104      134.3170  0.0003  16.0476\n",
      "     19       \u001b[36m79.8740\u001b[0m      129.9577  0.0003  15.4193\n",
      "     19      105.4383      147.7905  0.0005  15.5679\n",
      "     19      \u001b[36m132.8030\u001b[0m      \u001b[32m131.0919\u001b[0m  0.0010  15.7902\n",
      "     19      140.6814      \u001b[32m130.5265\u001b[0m  0.0001  15.8220\n",
      "     20       \u001b[36m77.7540\u001b[0m      131.1049  0.0003  15.4624\n",
      "     20      123.2502      145.2830  0.0005  16.0298\n",
      "     20      \u001b[36m129.2732\u001b[0m      \u001b[32m129.9624\u001b[0m  0.0010  16.1838\n",
      "     20      \u001b[36m132.1733\u001b[0m      \u001b[32m128.2803\u001b[0m  0.0001  16.5174\n",
      "     21       \u001b[36m75.5854\u001b[0m      132.7283  0.0003  16.7033\n",
      "     21      120.3792      152.0790  0.0005  16.0527\n",
      "     21      \u001b[36m126.3917\u001b[0m      \u001b[32m129.5249\u001b[0m  0.0010  16.2930\n",
      "     21      \u001b[36m130.1853\u001b[0m      \u001b[32m126.5849\u001b[0m  0.0001  16.2699\n",
      "     22       \u001b[36m73.3015\u001b[0m      137.6783  0.0003  16.2859\n",
      "     22      116.7335      140.5597  0.0005  16.0731\n",
      "     22      \u001b[36m124.9463\u001b[0m      130.6438  0.0010  16.4186\n",
      "     22      \u001b[36m128.6901\u001b[0m      \u001b[32m125.5136\u001b[0m  0.0001  16.6546\n",
      "     23       \u001b[36m70.3863\u001b[0m      \u001b[32m123.3214\u001b[0m  0.0001  16.2718\n",
      "     23      122.7167      121.1138  0.0003  15.9210\n",
      "     23      \u001b[36m120.1903\u001b[0m      131.2808  0.0010  16.8695\n",
      "     23      \u001b[36m127.4248\u001b[0m      \u001b[32m125.0892\u001b[0m  0.0001  16.9021\n",
      "     24       \u001b[36m68.0504\u001b[0m      126.3894  0.0001  16.9046\n",
      "     24      109.7374      \u001b[32m112.1949\u001b[0m  0.0003  16.7513\n",
      "     24      122.1011      \u001b[32m128.5069\u001b[0m  0.0010  17.3168\n",
      "     24      \u001b[36m126.2544\u001b[0m      \u001b[32m124.8454\u001b[0m  0.0001  17.3277\n",
      "     25       \u001b[36m66.7023\u001b[0m      127.5624  0.0001  16.3393\n",
      "     25      107.2965      \u001b[32m108.8176\u001b[0m  0.0003  15.8328\n",
      "     25      \u001b[36m118.7959\u001b[0m      \u001b[32m122.6320\u001b[0m  0.0010  15.9332\n",
      "     25      \u001b[36m125.1650\u001b[0m      \u001b[32m124.6607\u001b[0m  0.0001  15.7835\n",
      "     26       \u001b[36m65.5276\u001b[0m      128.1886  0.0001  15.7572\n",
      "     26      105.6087      109.4354  0.0003  15.4620\n",
      "     26      120.9534      122.8877  0.0010  16.0338\n",
      "     26      \u001b[36m124.1068\u001b[0m      \u001b[32m124.5360\u001b[0m  0.0001  15.9599\n",
      "     27       \u001b[36m64.3651\u001b[0m      129.4753  0.0001  15.5651\n",
      "     27      103.7596      109.7795  0.0003  16.1145\n",
      "     27      121.8796      135.6050  0.0010  16.5121\n",
      "     27      \u001b[36m123.0758\u001b[0m      \u001b[32m124.4999\u001b[0m  0.0001  16.5310\n",
      "     28       \u001b[36m63.8662\u001b[0m      140.1751  0.0001  16.7056\n",
      "     28      102.1409      110.5634  0.0003  16.1417\n",
      "     28      120.7381      139.0700  0.0010  16.4067\n",
      "     28      \u001b[36m122.0805\u001b[0m      124.5287  0.0001  16.2494\n",
      "     29       \u001b[36m63.4336\u001b[0m      140.4992  0.0001  15.5332\n",
      "     29      100.2300      111.2276  0.0003  15.5478\n",
      "     29      \u001b[36m117.7481\u001b[0m      150.7753  0.0010  15.8248\n",
      "     29      \u001b[36m121.1042\u001b[0m      124.6021  0.0001  15.9499\n",
      "     30       \u001b[36m62.8777\u001b[0m      140.5293  0.0001  15.7471\n",
      "     30      102.9917      119.8753  0.0001  15.4954\n",
      "     30      129.6611      136.6544  0.0005  16.0531\n",
      "     30      \u001b[36m120.1394\u001b[0m      124.7887  0.0001  15.7151\n",
      "     31       \u001b[36m62.3183\u001b[0m      140.5679  0.0001  15.8059\n",
      "     31      100.1192      117.1602  0.0001  15.8808\n",
      "     31      128.5565      128.7361  0.0005  15.9120\n",
      "     31      \u001b[36m119.1670\u001b[0m      124.8678  0.0001  15.9041\n",
      "     32       63.9959      125.8364  0.0000  16.2885\n",
      "     32       97.1092      117.1251  0.0001  16.7065\n",
      "     32      125.3772      134.3685  0.0005  17.1257\n",
      "     32      \u001b[36m116.7835\u001b[0m      \u001b[32m120.9360\u001b[0m  0.0001  16.8978\n",
      "     33       94.7096      117.3223  0.0001  15.1487\n",
      "     33      121.8324      134.2186  0.0005  14.1378\n",
      "     33      \u001b[36m109.4859\u001b[0m      \u001b[32m119.9365\u001b[0m  0.0001  13.9907\n",
      "     34       92.6357      124.1518  0.0001  12.8769\n",
      "     34      126.3693      134.2436  0.0003  13.1831\n",
      "     34      \u001b[36m108.6877\u001b[0m      \u001b[32m119.5996\u001b[0m  0.0001  12.9611\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "     35      \u001b[36m108.1411\u001b[0m      \u001b[32m119.3497\u001b[0m  0.0001  12.5058\n",
      "     36      \u001b[36m107.5939\u001b[0m      \u001b[32m119.2502\u001b[0m  0.0001  8.9452\n",
      "     37      \u001b[36m107.0626\u001b[0m      \u001b[32m119.1614\u001b[0m  0.0001  9.0423\n",
      "     38      \u001b[36m106.5250\u001b[0m      \u001b[32m119.1099\u001b[0m  0.0001  8.8320\n",
      "     39      \u001b[36m105.9874\u001b[0m      \u001b[32m119.0765\u001b[0m  0.0001  8.8083\n",
      "     40      \u001b[36m105.4436\u001b[0m      119.0783  0.0001  8.4111\n",
      "     41      \u001b[36m104.9029\u001b[0m      \u001b[32m119.0548\u001b[0m  0.0001  8.4552\n",
      "     42      \u001b[36m104.3523\u001b[0m      \u001b[32m119.0271\u001b[0m  0.0001  8.8954\n",
      "     43      \u001b[36m103.7908\u001b[0m      119.0894  0.0001  8.4765\n",
      "     44      \u001b[36m103.2348\u001b[0m      119.1182  0.0001  8.4712\n",
      "     45      \u001b[36m102.6679\u001b[0m      119.2044  0.0001  9.6784\n",
      "     46      \u001b[36m102.0976\u001b[0m      119.2857  0.0001  8.9723\n",
      "     47       \u001b[36m99.3514\u001b[0m      120.0014  0.0000  9.3349\n",
      "     48       \u001b[36m96.8522\u001b[0m      120.1882  0.0000  8.9476\n",
      "     49       \u001b[36m96.1231\u001b[0m      120.2545  0.0000  8.7226\n",
      "     50       \u001b[36m95.7426\u001b[0m      120.3171  0.0000  8.8481\n",
      "     51       \u001b[36m93.8444\u001b[0m      124.7992  0.0000  8.7880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m462.9703\u001b[0m      \u001b[32m390.3278\u001b[0m  0.0010  0.1966\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m469.5848\u001b[0m      \u001b[32m394.8664\u001b[0m  0.0010  0.2041\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m464.2453\u001b[0m      \u001b[32m389.5264\u001b[0m  0.0010  0.2168\n",
      "      2      \u001b[36m266.4830\u001b[0m      \u001b[32m173.9500\u001b[0m  0.0010  0.2124\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m469.2983\u001b[0m      \u001b[32m396.6325\u001b[0m  0.0010  0.2050\n",
      "      2      \u001b[36m284.4776\u001b[0m      \u001b[32m179.1603\u001b[0m  0.0010  0.1953\n",
      "      2      \u001b[36m277.9635\u001b[0m      \u001b[32m177.7488\u001b[0m  0.0010  0.1858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3      \u001b[36m163.7255\u001b[0m      \u001b[32m137.3624\u001b[0m  0.0010  0.1953\n",
      "      2      \u001b[36m282.2866\u001b[0m      \u001b[32m181.3409\u001b[0m  0.0010  0.1857\n",
      "      3      \u001b[36m169.5155\u001b[0m      \u001b[32m136.7774\u001b[0m  0.0010  0.1746\n",
      "      3      \u001b[36m164.7251\u001b[0m      \u001b[32m132.9131\u001b[0m  0.0010  0.1791\n",
      "      4      \u001b[36m135.6826\u001b[0m      \u001b[32m123.1577\u001b[0m  0.0010  0.1732\n",
      "      3      \u001b[36m168.3540\u001b[0m      \u001b[32m137.6782\u001b[0m  0.0010  0.1729\n",
      "      4      \u001b[36m142.2732\u001b[0m      \u001b[32m123.4406\u001b[0m  0.0010  0.1901\n",
      "      4      \u001b[36m133.5907\u001b[0m      \u001b[32m118.0761\u001b[0m  0.0010  0.1936\n",
      "      5      \u001b[36m119.6804\u001b[0m      \u001b[32m113.0015\u001b[0m  0.0010  0.1952\n",
      "      4      \u001b[36m143.1091\u001b[0m      \u001b[32m126.6988\u001b[0m  0.0010  0.1843\n",
      "      5      \u001b[36m125.5256\u001b[0m      \u001b[32m113.9233\u001b[0m  0.0010  0.1683\n",
      "      5      \u001b[36m116.8672\u001b[0m      \u001b[32m106.8612\u001b[0m  0.0010  0.1910\n",
      "      6      \u001b[36m108.3462\u001b[0m      \u001b[32m105.7491\u001b[0m  0.0010  0.1762\n",
      "      5      \u001b[36m124.6038\u001b[0m      \u001b[32m117.1253\u001b[0m  0.0010  0.1954\n",
      "      6      \u001b[36m113.6533\u001b[0m      \u001b[32m105.7958\u001b[0m  0.0010  0.1713\n",
      "      6      \u001b[36m107.6166\u001b[0m       \u001b[32m96.7545\u001b[0m  0.0010  0.1800\n",
      "      7       \u001b[36m99.6961\u001b[0m       \u001b[32m99.3076\u001b[0m  0.0010  0.1981\n",
      "      6      \u001b[36m110.2604\u001b[0m      \u001b[32m107.7569\u001b[0m  0.0010  0.2011\n",
      "      7      \u001b[36m105.3039\u001b[0m       \u001b[32m98.2235\u001b[0m  0.0010  0.2094\n",
      "      7       \u001b[36m97.3796\u001b[0m       \u001b[32m89.0591\u001b[0m  0.0010  0.2014\n",
      "      8       \u001b[36m93.9109\u001b[0m       \u001b[32m93.6717\u001b[0m  0.0010  0.2176\n",
      "      7      \u001b[36m102.2390\u001b[0m       \u001b[32m99.2727\u001b[0m  0.0010  0.1989\n",
      "      8       \u001b[36m96.3797\u001b[0m       \u001b[32m90.1644\u001b[0m  0.0010  0.2066\n",
      "      8       \u001b[36m90.5111\u001b[0m       \u001b[32m83.1913\u001b[0m  0.0010  0.2064\n",
      "      9       \u001b[36m90.0819\u001b[0m       \u001b[32m88.9143\u001b[0m  0.0010  0.2099\n",
      "      8       \u001b[36m93.6531\u001b[0m       \u001b[32m93.7822\u001b[0m  0.0010  0.2052\n",
      "      9       \u001b[36m90.9017\u001b[0m       \u001b[32m85.4687\u001b[0m  0.0010  0.2013\n",
      "      9       \u001b[36m85.6797\u001b[0m       \u001b[32m78.5373\u001b[0m  0.0010  0.1901\n",
      "     10       \u001b[36m87.2512\u001b[0m       \u001b[32m84.4195\u001b[0m  0.0010  0.1794\n",
      "      9       \u001b[36m90.2326\u001b[0m       \u001b[32m85.0097\u001b[0m  0.0010  0.1892\n",
      "     10       \u001b[36m87.4152\u001b[0m       \u001b[32m81.5312\u001b[0m  0.0010  0.1800\n",
      "     10       \u001b[36m81.5971\u001b[0m       \u001b[32m74.7163\u001b[0m  0.0010  0.1919\n",
      "     11       \u001b[36m81.1967\u001b[0m       \u001b[32m79.3849\u001b[0m  0.0010  0.2076\n",
      "     10       \u001b[36m83.8845\u001b[0m       \u001b[32m81.3712\u001b[0m  0.0010  0.1953\n",
      "     11       \u001b[36m84.6237\u001b[0m       \u001b[32m76.8897\u001b[0m  0.0010  0.1987\n",
      "     11       \u001b[36m77.7868\u001b[0m       \u001b[32m70.9369\u001b[0m  0.0010  0.1746\n",
      "     12       \u001b[36m78.2919\u001b[0m       \u001b[32m75.1076\u001b[0m  0.0010  0.1851\n",
      "     11       \u001b[36m81.4277\u001b[0m       \u001b[32m77.4430\u001b[0m  0.0010  0.1896\n",
      "     12       \u001b[36m80.8578\u001b[0m       \u001b[32m73.4605\u001b[0m  0.0010  0.1805\n",
      "     12       \u001b[36m75.1440\u001b[0m       \u001b[32m67.4750\u001b[0m  0.0010  0.1955\n",
      "     13       \u001b[36m75.1929\u001b[0m       \u001b[32m72.2219\u001b[0m  0.0010  0.2017\n",
      "     12       \u001b[36m76.6485\u001b[0m       \u001b[32m73.2920\u001b[0m  0.0010  0.2073\n",
      "     13       \u001b[36m78.3419\u001b[0m       \u001b[32m71.8813\u001b[0m  0.0010  0.2743\n",
      "     13       \u001b[36m72.4262\u001b[0m       \u001b[32m65.4235\u001b[0m  0.0010  0.2720\n",
      "     14       \u001b[36m72.9544\u001b[0m       \u001b[32m69.2318\u001b[0m  0.0010  0.2688\n",
      "     13       \u001b[36m73.9854\u001b[0m       \u001b[32m69.2005\u001b[0m  0.0010  0.2601\n",
      "     14       \u001b[36m74.7597\u001b[0m       \u001b[32m68.3827\u001b[0m  0.0010  0.2287\n",
      "     14       \u001b[36m69.2655\u001b[0m       \u001b[32m61.9506\u001b[0m  0.0010  0.2324\n",
      "     15       \u001b[36m69.8753\u001b[0m       \u001b[32m66.0698\u001b[0m  0.0010  0.2330\n",
      "     14       \u001b[36m72.3519\u001b[0m       \u001b[32m65.9223\u001b[0m  0.0010  0.2234\n",
      "     15       \u001b[36m70.9044\u001b[0m       \u001b[32m64.9367\u001b[0m  0.0010  0.2334\n",
      "     15       \u001b[36m64.7686\u001b[0m       \u001b[32m60.2137\u001b[0m  0.0010  0.2290\n",
      "     16       \u001b[36m68.2733\u001b[0m       \u001b[32m63.8213\u001b[0m  0.0010  0.2220\n",
      "     15       \u001b[36m68.0753\u001b[0m       \u001b[32m63.9012\u001b[0m  0.0010  0.2230\n",
      "     16       \u001b[36m68.5474\u001b[0m       \u001b[32m63.3794\u001b[0m  0.0010  0.2333\n",
      "     16       \u001b[36m62.5923\u001b[0m       \u001b[32m56.7126\u001b[0m  0.0010  0.2482\n",
      "     17       \u001b[36m65.0417\u001b[0m       \u001b[32m60.6962\u001b[0m  0.0010  0.2334\n",
      "     16       \u001b[36m66.9443\u001b[0m       \u001b[32m60.8065\u001b[0m  0.0010  0.2293\n",
      "     17       \u001b[36m66.7256\u001b[0m       \u001b[32m62.6104\u001b[0m  0.0010  0.2501\n",
      "     17       \u001b[36m60.8843\u001b[0m       \u001b[32m54.4076\u001b[0m  0.0010  0.2413\n",
      "     18       \u001b[36m62.0461\u001b[0m       \u001b[32m58.8750\u001b[0m  0.0010  0.2466\n",
      "     17       \u001b[36m64.1155\u001b[0m       \u001b[32m58.6706\u001b[0m  0.0010  0.2450\n",
      "     18       \u001b[36m63.9716\u001b[0m       \u001b[32m59.6758\u001b[0m  0.0010  0.2371\n",
      "     18       \u001b[36m58.7636\u001b[0m       \u001b[32m52.6174\u001b[0m  0.0010  0.2743\n",
      "     19       \u001b[36m60.6681\u001b[0m       \u001b[32m56.8675\u001b[0m  0.0010  0.2494\n",
      "     18       \u001b[36m63.6022\u001b[0m       \u001b[32m57.2753\u001b[0m  0.0010  0.2735\n",
      "     19       \u001b[36m63.2153\u001b[0m       \u001b[32m56.9791\u001b[0m  0.0010  0.2402\n",
      "     19       \u001b[36m55.8842\u001b[0m       \u001b[32m49.7001\u001b[0m  0.0010  0.2037\n",
      "     20       \u001b[36m59.5228\u001b[0m       \u001b[32m54.9892\u001b[0m  0.0010  0.2083\n",
      "     19       \u001b[36m60.2501\u001b[0m       \u001b[32m54.5504\u001b[0m  0.0010  0.2020\n",
      "     20       \u001b[36m60.9817\u001b[0m       \u001b[32m56.3411\u001b[0m  0.0010  0.2089\n",
      "     20       \u001b[36m54.2603\u001b[0m       \u001b[32m48.6710\u001b[0m  0.0010  0.2050\n",
      "     21       \u001b[36m57.8175\u001b[0m       \u001b[32m53.6586\u001b[0m  0.0010  0.2160\n",
      "     20       \u001b[36m60.0498\u001b[0m       55.3172  0.0010  0.2035\n",
      "     21       \u001b[36m59.5512\u001b[0m       56.7457  0.0010  0.2020\n",
      "     21       \u001b[36m53.6269\u001b[0m       \u001b[32m47.5119\u001b[0m  0.0010  0.2031\n",
      "     21       \u001b[36m58.8000\u001b[0m       \u001b[32m52.8638\u001b[0m  0.0010  0.1999\n",
      "     22       \u001b[36m56.6828\u001b[0m       \u001b[32m52.4622\u001b[0m  0.0010  0.2175\n",
      "     22       \u001b[36m58.4036\u001b[0m       \u001b[32m54.7967\u001b[0m  0.0010  0.2108\n",
      "     22       \u001b[36m51.7976\u001b[0m       \u001b[32m46.5059\u001b[0m  0.0010  0.2257\n",
      "     22       \u001b[36m55.9306\u001b[0m       \u001b[32m52.2202\u001b[0m  0.0010  0.2072\n",
      "     23       \u001b[36m54.9259\u001b[0m       \u001b[32m51.0999\u001b[0m  0.0010  0.2167\n",
      "     23       \u001b[36m57.2681\u001b[0m       \u001b[32m52.6305\u001b[0m  0.0010  0.2188\n",
      "     23       \u001b[36m49.7653\u001b[0m       \u001b[32m44.1300\u001b[0m  0.0010  0.1969\n",
      "     23       \u001b[36m55.7373\u001b[0m       \u001b[32m51.1204\u001b[0m  0.0010  0.1822\n",
      "     24       \u001b[36m54.3795\u001b[0m       \u001b[32m50.1138\u001b[0m  0.0010  0.1780\n",
      "     24       \u001b[36m55.5270\u001b[0m       \u001b[32m51.2785\u001b[0m  0.0010  0.2585\n",
      "     24       \u001b[36m49.5330\u001b[0m       \u001b[32m42.6533\u001b[0m  0.0010  0.3051\n",
      "     25       \u001b[36m51.9409\u001b[0m       \u001b[32m49.3017\u001b[0m  0.0010  0.2962\n",
      "     24       \u001b[36m55.5920\u001b[0m       \u001b[32m49.0803\u001b[0m  0.0010  0.3123\n",
      "     25       57.4339       \u001b[32m50.0275\u001b[0m  0.0010  0.2404\n",
      "     25       \u001b[36m49.3931\u001b[0m       \u001b[32m41.1785\u001b[0m  0.0010  0.1755\n",
      "     25       \u001b[36m53.4317\u001b[0m       \u001b[32m47.1661\u001b[0m  0.0010  0.1792\n",
      "     26       \u001b[36m50.9773\u001b[0m       \u001b[32m48.6398\u001b[0m  0.0010  0.1867\n",
      "     26       \u001b[36m54.2322\u001b[0m       51.5278  0.0010  0.1767\n",
      "     26       \u001b[36m45.8721\u001b[0m       \u001b[32m39.9146\u001b[0m  0.0010  0.1732\n",
      "     26       \u001b[36m51.7560\u001b[0m       \u001b[32m46.6892\u001b[0m  0.0010  0.1909\n",
      "     27       \u001b[36m49.3732\u001b[0m       \u001b[32m47.9567\u001b[0m  0.0010  0.1940\n",
      "     27       54.5641       \u001b[32m48.6574\u001b[0m  0.0010  0.1938\n",
      "     27       \u001b[36m44.8937\u001b[0m       \u001b[32m39.0828\u001b[0m  0.0010  0.1943\n",
      "     27       \u001b[36m50.7469\u001b[0m       \u001b[32m44.8726\u001b[0m  0.0010  0.1983\n",
      "     28       50.2074       \u001b[32m47.9271\u001b[0m  0.0010  0.2109\n",
      "     28       \u001b[36m52.5906\u001b[0m       \u001b[32m47.2087\u001b[0m  0.0010  0.2042\n",
      "     28       44.9070       \u001b[32m37.2928\u001b[0m  0.0010  0.2030\n",
      "     28       \u001b[36m50.6600\u001b[0m       \u001b[32m44.7188\u001b[0m  0.0010  0.2097\n",
      "     29       \u001b[36m47.5252\u001b[0m       \u001b[32m45.7148\u001b[0m  0.0010  0.1968\n",
      "     29       \u001b[36m51.7097\u001b[0m       48.8192  0.0010  0.1930\n",
      "     29       \u001b[36m44.8387\u001b[0m       37.5603  0.0010  0.1794\n",
      "     29       \u001b[36m50.6309\u001b[0m       \u001b[32m44.1304\u001b[0m  0.0010  0.1977\n",
      "     30       \u001b[36m47.0657\u001b[0m       \u001b[32m44.8630\u001b[0m  0.0010  0.2036\n",
      "     30       53.7385       \u001b[32m45.6307\u001b[0m  0.0010  0.1890\n",
      "     30       \u001b[36m43.2922\u001b[0m       \u001b[32m36.7291\u001b[0m  0.0010  0.2013\n",
      "     30       \u001b[36m47.7324\u001b[0m       \u001b[32m42.7624\u001b[0m  0.0010  0.2169\n",
      "     31       \u001b[36m46.2947\u001b[0m       \u001b[32m42.8587\u001b[0m  0.0010  0.2298\n",
      "     31       \u001b[36m50.6148\u001b[0m       \u001b[32m44.1493\u001b[0m  0.0010  0.2092\n",
      "     31       \u001b[36m40.9475\u001b[0m       \u001b[32m35.3432\u001b[0m  0.0010  0.2401\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m458.4067\u001b[0m      \u001b[32m368.1911\u001b[0m  0.0010  0.1987\n",
      "     31       \u001b[36m47.6030\u001b[0m       \u001b[32m42.5126\u001b[0m  0.0010  0.2117\n",
      "     32       \u001b[36m43.1922\u001b[0m       \u001b[32m42.2235\u001b[0m  0.0010  0.1915\n",
      "     32       \u001b[36m48.8205\u001b[0m       \u001b[32m42.6545\u001b[0m  0.0010  0.1954\n",
      "     32       41.1594       35.4703  0.0010  0.1803\n",
      "      2      \u001b[36m266.3160\u001b[0m      \u001b[32m167.6730\u001b[0m  0.0010  0.1884\n",
      "     32       \u001b[36m47.5658\u001b[0m       \u001b[32m41.9417\u001b[0m  0.0010  0.1895\n",
      "     33       43.9783       \u001b[32m42.1711\u001b[0m  0.0010  0.1978\n",
      "     33       49.8105       \u001b[32m40.9103\u001b[0m  0.0010  0.2034\n",
      "     33       41.8870       36.5187  0.0010  0.1765\n",
      "      3      \u001b[36m166.7313\u001b[0m      \u001b[32m135.9954\u001b[0m  0.0010  0.1746\n",
      "     33       \u001b[36m47.5452\u001b[0m       \u001b[32m40.0587\u001b[0m  0.0010  0.1898\n",
      "     34       \u001b[36m42.9250\u001b[0m       \u001b[32m41.1167\u001b[0m  0.0010  0.1890\n",
      "     34       \u001b[36m40.0800\u001b[0m       \u001b[32m33.5627\u001b[0m  0.0010  0.1852\n",
      "     34       \u001b[36m47.9095\u001b[0m       43.4687  0.0010  0.2058\n",
      "      4      \u001b[36m146.8518\u001b[0m      \u001b[32m126.8123\u001b[0m  0.0010  0.1729\n",
      "     35       \u001b[36m42.8576\u001b[0m       \u001b[32m39.8657\u001b[0m  0.0010  0.1960\n",
      "     34       \u001b[36m45.8708\u001b[0m       \u001b[32m39.5916\u001b[0m  0.0010  0.2049\n",
      "     35       \u001b[36m39.8361\u001b[0m       \u001b[32m32.9989\u001b[0m  0.0010  0.1985\n",
      "     35       \u001b[36m47.0369\u001b[0m       \u001b[32m40.1785\u001b[0m  0.0010  0.1988\n",
      "      5      \u001b[36m133.7611\u001b[0m      \u001b[32m115.5341\u001b[0m  0.0010  0.1968\n",
      "     35       \u001b[36m44.3016\u001b[0m       40.6752  0.0010  0.1968\n",
      "     36       \u001b[36m41.1933\u001b[0m       \u001b[32m39.4190\u001b[0m  0.0010  0.2013\n",
      "     36       40.8354       \u001b[32m32.0971\u001b[0m  0.0010  0.1900\n",
      "     36       \u001b[36m46.7201\u001b[0m       \u001b[32m39.2381\u001b[0m  0.0010  0.1844\n",
      "      6      \u001b[36m119.8295\u001b[0m      \u001b[32m103.7591\u001b[0m  0.0010  0.1873\n",
      "     37       \u001b[36m40.8528\u001b[0m       \u001b[32m38.7315\u001b[0m  0.0010  0.1871\n",
      "     36       45.3636       \u001b[32m39.5198\u001b[0m  0.0010  0.1947\n",
      "     37       \u001b[36m45.8745\u001b[0m       39.9594  0.0010  0.2238\n",
      "     37       \u001b[36m39.0351\u001b[0m       \u001b[32m31.7288\u001b[0m  0.0010  0.2313\n",
      "      7      \u001b[36m107.0802\u001b[0m       \u001b[32m94.7418\u001b[0m  0.0010  0.2064\n",
      "     37       44.4084       \u001b[32m38.7292\u001b[0m  0.0010  0.1996\n",
      "     38       \u001b[36m40.6571\u001b[0m       38.7913  0.0010  0.2247\n",
      "      8       \u001b[36m98.5501\u001b[0m       \u001b[32m87.8484\u001b[0m  0.0010  0.1670\n",
      "     38       39.0569       \u001b[32m31.2859\u001b[0m  0.0010  0.1953\n",
      "     38       47.1006       \u001b[32m37.3212\u001b[0m  0.0010  0.2091\n",
      "     38       \u001b[36m44.1754\u001b[0m       \u001b[32m38.4892\u001b[0m  0.0010  0.1951\n",
      "     39       \u001b[36m39.3624\u001b[0m       \u001b[32m38.5200\u001b[0m  0.0010  0.1975\n",
      "      9       \u001b[36m91.9249\u001b[0m       \u001b[32m80.3557\u001b[0m  0.0010  0.1941\n",
      "     39       45.9034       39.1577  0.0010  0.1804\n",
      "     39       \u001b[36m37.9279\u001b[0m       \u001b[32m31.1028\u001b[0m  0.0010  0.1999\n",
      "     39       \u001b[36m43.5652\u001b[0m       \u001b[32m37.4337\u001b[0m  0.0010  0.1984\n",
      "     40       39.7861       \u001b[32m37.3825\u001b[0m  0.0010  0.1816\n",
      "     10       \u001b[36m88.3797\u001b[0m       \u001b[32m75.2357\u001b[0m  0.0010  0.1919\n",
      "     40       \u001b[36m36.3111\u001b[0m       \u001b[32m29.8145\u001b[0m  0.0010  0.1815\n",
      "     40       45.9965       38.9573  0.0010  0.2045\n",
      "     40       \u001b[36m42.7510\u001b[0m       \u001b[32m36.0483\u001b[0m  0.0010  0.1778\n",
      "     41       40.2364       37.7988  0.0010  0.1738\n",
      "     11       \u001b[36m84.8040\u001b[0m       \u001b[32m71.0602\u001b[0m  0.0010  0.1733\n",
      "     41       37.5895       30.2502  0.0010  0.2065\n",
      "     41       \u001b[36m44.4715\u001b[0m       38.8379  0.0010  0.1967\n",
      "     41       \u001b[36m41.2825\u001b[0m       \u001b[32m35.7894\u001b[0m  0.0010  0.1890\n",
      "     42       \u001b[36m38.9572\u001b[0m       \u001b[32m36.6387\u001b[0m  0.0010  0.1978\n",
      "     12       \u001b[36m81.9152\u001b[0m       \u001b[32m68.0367\u001b[0m  0.0010  0.1723\n",
      "     42       45.2586       \u001b[32m37.3134\u001b[0m  0.0010  0.1892\n",
      "     42       36.7485       29.8322  0.0010  0.2040\n",
      "     42       42.0187       \u001b[32m34.9045\u001b[0m  0.0010  0.2013\n",
      "     43       40.0887       \u001b[32m36.5329\u001b[0m  0.0010  0.1927\n",
      "     13       \u001b[36m78.4595\u001b[0m       \u001b[32m64.8436\u001b[0m  0.0010  0.1882\n",
      "     43       \u001b[36m43.3139\u001b[0m       \u001b[32m36.5356\u001b[0m  0.0010  0.2254\n",
      "     43       37.3787       \u001b[32m29.3651\u001b[0m  0.0010  0.2259\n",
      "     43       \u001b[36m40.5692\u001b[0m       \u001b[32m34.8042\u001b[0m  0.0010  0.2093\n",
      "     44       \u001b[36m38.9369\u001b[0m       \u001b[32m36.0085\u001b[0m  0.0010  0.2159\n",
      "     14       \u001b[36m75.8203\u001b[0m       \u001b[32m62.5194\u001b[0m  0.0010  0.2222\n",
      "     44       \u001b[36m35.7238\u001b[0m       \u001b[32m27.9891\u001b[0m  0.0010  0.1682\n",
      "     44       44.3918       \u001b[32m34.8079\u001b[0m  0.0010  0.1983\n",
      "     45       \u001b[36m37.4732\u001b[0m       \u001b[32m35.6994\u001b[0m  0.0010  0.1703\n",
      "     44       41.7723       \u001b[32m34.2707\u001b[0m  0.0010  0.1841\n",
      "     15       \u001b[36m71.5645\u001b[0m       \u001b[32m59.5486\u001b[0m  0.0010  0.1978\n",
      "     45       \u001b[36m35.2201\u001b[0m       29.0520  0.0010  0.1724\n",
      "     45       \u001b[36m43.1505\u001b[0m       35.1017  0.0010  0.1848\n",
      "     45       \u001b[36m40.5620\u001b[0m       34.3745  0.0010  0.1726\n",
      "     46       38.6190       35.7729  0.0010  0.1773\n",
      "     16       \u001b[36m68.3856\u001b[0m       \u001b[32m57.1363\u001b[0m  0.0010  0.1755\n",
      "     46       35.3575       \u001b[32m27.7665\u001b[0m  0.0010  0.1956\n",
      "     46       43.7737       36.3556  0.0010  0.1789\n",
      "     47       37.6808       \u001b[32m35.3000\u001b[0m  0.0010  0.1708\n",
      "     46       \u001b[36m39.5349\u001b[0m       \u001b[32m34.0649\u001b[0m  0.0010  0.1921\n",
      "     17       \u001b[36m66.2180\u001b[0m       \u001b[32m54.1893\u001b[0m  0.0010  0.1680\n",
      "     47       36.1188       \u001b[32m27.0603\u001b[0m  0.0010  0.1938\n",
      "     48       \u001b[36m36.9862\u001b[0m       35.4791  0.0010  0.1779\n",
      "     47       \u001b[36m42.5830\u001b[0m       \u001b[32m33.1628\u001b[0m  0.0010  0.1880\n",
      "     47       \u001b[36m39.3622\u001b[0m       \u001b[32m33.2108\u001b[0m  0.0010  0.1980\n",
      "     18       \u001b[36m65.3774\u001b[0m       \u001b[32m53.4355\u001b[0m  0.0010  0.1974\n",
      "     49       37.5852       35.8220  0.0010  0.1618\n",
      "     48       \u001b[36m34.0546\u001b[0m       \u001b[32m27.0221\u001b[0m  0.0010  0.1665\n",
      "     48       42.7101       \u001b[32m32.8117\u001b[0m  0.0010  0.1772\n",
      "     48       39.4877       \u001b[32m31.7698\u001b[0m  0.0010  0.1892\n",
      "     19       \u001b[36m62.7924\u001b[0m       \u001b[32m51.1931\u001b[0m  0.0010  0.2569\n",
      "     50       \u001b[36m36.2534\u001b[0m       \u001b[32m34.5975\u001b[0m  0.0010  0.2740\n",
      "     49       34.8521       27.5968  0.0010  0.2829\n",
      "     49       \u001b[36m41.0532\u001b[0m       33.2960  0.0010  0.2806\n",
      "     49       \u001b[36m37.7902\u001b[0m       \u001b[32m30.8526\u001b[0m  0.0010  0.2761\n",
      "     20       \u001b[36m61.4996\u001b[0m       \u001b[32m50.3692\u001b[0m  0.0010  0.2291\n",
      "     51       36.4520       \u001b[32m34.2115\u001b[0m  0.0010  0.2304\n",
      "     50       34.5371       \u001b[32m26.4572\u001b[0m  0.0010  0.2223\n",
      "     50       42.3333       34.5750  0.0010  0.2319\n",
      "     50       38.6964       31.1963  0.0010  0.2169\n",
      "     21       \u001b[36m59.5806\u001b[0m       \u001b[32m48.9796\u001b[0m  0.0010  0.2046\n",
      "     52       \u001b[36m35.7671\u001b[0m       34.2806  0.0010  0.1871\n",
      "     51       35.4690       \u001b[32m26.1987\u001b[0m  0.0010  0.1991\n",
      "     51       \u001b[36m40.7609\u001b[0m       \u001b[32m32.7036\u001b[0m  0.0010  0.1893\n",
      "     51       37.8130       30.9435  0.0010  0.1992\n",
      "     22       59.7824       \u001b[32m47.0757\u001b[0m  0.0010  0.1891\n",
      "     53       36.0534       34.7578  0.0010  0.1877\n",
      "     52       34.3087       26.8290  0.0010  0.1859\n",
      "     52       41.3435       \u001b[32m31.2367\u001b[0m  0.0010  0.1905\n",
      "     52       \u001b[36m36.5963\u001b[0m       \u001b[32m30.0533\u001b[0m  0.0010  0.1752\n",
      "     23       \u001b[36m56.4566\u001b[0m       \u001b[32m46.1844\u001b[0m  0.0010  0.1982\n",
      "     54       \u001b[36m34.8579\u001b[0m       \u001b[32m33.0255\u001b[0m  0.0010  0.2185\n",
      "     53       \u001b[36m32.9145\u001b[0m       26.6234  0.0010  0.2080\n",
      "     53       \u001b[36m40.6260\u001b[0m       \u001b[32m31.0921\u001b[0m  0.0010  0.2182\n",
      "     53       37.9640       30.3631  0.0010  0.2250\n",
      "     24       \u001b[36m55.6144\u001b[0m       \u001b[32m44.9611\u001b[0m  0.0010  0.2122\n",
      "     54       34.2512       \u001b[32m24.1774\u001b[0m  0.0010  0.2000\n",
      "     55       35.1088       33.0648  0.0010  0.2016\n",
      "     54       40.9760       \u001b[32m30.9967\u001b[0m  0.0010  0.2072\n",
      "     54       37.5590       \u001b[32m29.7000\u001b[0m  0.0010  0.1893\n",
      "     25       56.9134       \u001b[32m43.9962\u001b[0m  0.0010  0.1642\n",
      "     55       \u001b[36m32.8235\u001b[0m       25.7780  0.0010  0.1681\n",
      "     56       \u001b[36m34.5901\u001b[0m       33.3515  0.0010  0.2016\n",
      "     55       \u001b[36m37.6704\u001b[0m       \u001b[32m30.6795\u001b[0m  0.0010  0.1842\n",
      "     55       \u001b[36m35.2864\u001b[0m       \u001b[32m29.5143\u001b[0m  0.0010  0.2032\n",
      "     26       \u001b[36m54.0545\u001b[0m       \u001b[32m42.1923\u001b[0m  0.0010  0.1952\n",
      "     56       \u001b[36m32.5155\u001b[0m       24.6376  0.0010  0.2238\n",
      "     57       \u001b[36m33.5430\u001b[0m       \u001b[32m32.5589\u001b[0m  0.0010  0.2265\n",
      "     56       39.4984       \u001b[32m29.5823\u001b[0m  0.0010  0.2103\n",
      "     56       36.1881       \u001b[32m29.5006\u001b[0m  0.0010  0.2064\n",
      "     27       55.8258       \u001b[32m41.4937\u001b[0m  0.0010  0.2111\n",
      "     57       \u001b[36m31.7491\u001b[0m       \u001b[32m24.0329\u001b[0m  0.0010  0.1857\n",
      "     57       40.6480       29.6059  0.0010  0.1811\n",
      "     58       34.6168       \u001b[32m31.2890\u001b[0m  0.0010  0.1903\n",
      "     57       36.1273       \u001b[32m29.0874\u001b[0m  0.0010  0.1801\n",
      "     28       \u001b[36m52.4721\u001b[0m       \u001b[32m40.7128\u001b[0m  0.0010  0.1941\n",
      "     58       32.3197       24.0611  0.0010  0.2006\n",
      "     59       \u001b[36m33.3772\u001b[0m       32.3517  0.0010  0.1983\n",
      "     58       39.9230       \u001b[32m28.9544\u001b[0m  0.0010  0.2276\n",
      "     58       36.8894       \u001b[32m28.4520\u001b[0m  0.0010  0.2089\n",
      "     29       \u001b[36m51.6887\u001b[0m       \u001b[32m39.5617\u001b[0m  0.0010  0.1993\n",
      "     59       32.1173       24.2186  0.0010  0.1830\n",
      "     60       33.5160       31.9830  0.0010  0.1764\n",
      "     59       37.9524       29.1147  0.0010  0.1886\n",
      "     59       35.4774       28.8497  0.0010  0.1915\n",
      "     30       \u001b[36m50.7469\u001b[0m       \u001b[32m38.2512\u001b[0m  0.0010  0.1724\n",
      "     60       33.2332       24.1628  0.0010  0.1953\n",
      "     61       34.2511       32.1215  0.0010  0.1847\n",
      "     60       \u001b[36m34.7414\u001b[0m       \u001b[32m27.7195\u001b[0m  0.0010  0.1757\n",
      "     60       38.7433       \u001b[32m28.7696\u001b[0m  0.0010  0.1942\n",
      "     31       \u001b[36m49.0415\u001b[0m       \u001b[32m36.9838\u001b[0m  0.0010  0.1664\n",
      "     61       \u001b[36m31.6680\u001b[0m       \u001b[32m22.4993\u001b[0m  0.0010  0.1835\n",
      "     62       \u001b[36m32.9272\u001b[0m       31.6828  0.0010  0.1987\n",
      "     61       34.9705       28.4664  0.0010  0.1855\n",
      "     61       37.8315       29.0517  0.0010  0.1865\n",
      "     32       \u001b[36m48.3478\u001b[0m       \u001b[32m36.9151\u001b[0m  0.0010  0.1787\n",
      "     62       \u001b[36m31.6159\u001b[0m       23.3445  0.0010  0.2620\n",
      "     63       33.5119       \u001b[32m30.5267\u001b[0m  0.0005  0.2815\n",
      "     62       \u001b[36m33.8038\u001b[0m       \u001b[32m27.6218\u001b[0m  0.0010  0.2693\n",
      "     33       \u001b[36m47.6108\u001b[0m       \u001b[32m35.2000\u001b[0m  0.0010  0.2652\n",
      "     62       \u001b[36m37.2349\u001b[0m       \u001b[32m28.2773\u001b[0m  0.0010  0.2884\n",
      "     63       32.1290       \u001b[32m22.4971\u001b[0m  0.0010  0.2167\n",
      "     64       \u001b[36m31.9518\u001b[0m       30.8879  0.0005  0.2058\n",
      "     63       34.8611       \u001b[32m27.0637\u001b[0m  0.0010  0.2105\n",
      "     63       37.2975       29.5030  0.0010  0.1945\n",
      "     34       \u001b[36m46.3574\u001b[0m       \u001b[32m34.1917\u001b[0m  0.0010  0.2047\n",
      "     64       \u001b[36m31.3287\u001b[0m       22.8435  0.0010  0.2025\n",
      "     65       \u001b[36m31.1373\u001b[0m       30.9073  0.0005  0.1845\n",
      "     64       34.7486       28.1723  0.0010  0.1832\n",
      "     35       \u001b[36m45.3733\u001b[0m       \u001b[32m33.2045\u001b[0m  0.0010  0.1820\n",
      "     64       \u001b[36m36.4595\u001b[0m       \u001b[32m26.9186\u001b[0m  0.0010  0.1984\n",
      "     65       \u001b[36m29.6492\u001b[0m       23.1354  0.0010  0.1923\n",
      "     65       34.1894       \u001b[32m26.5497\u001b[0m  0.0010  0.1711\n",
      "     66       31.9898       \u001b[32m30.3828\u001b[0m  0.0005  0.1992\n",
      "     36       \u001b[36m43.1127\u001b[0m       \u001b[32m32.7198\u001b[0m  0.0010  0.1830\n",
      "     65       37.7993       \u001b[32m26.2507\u001b[0m  0.0010  0.1963\n",
      "     66       \u001b[36m33.2766\u001b[0m       26.8606  0.0010  0.1796\n",
      "     37       45.3594       \u001b[32m32.0275\u001b[0m  0.0010  0.1701\n",
      "     67       32.1347       \u001b[32m29.6625\u001b[0m  0.0005  0.2075\n",
      "     66       36.8904       \u001b[32m25.8472\u001b[0m  0.0010  0.1834\n",
      "     67       33.5111       26.8347  0.0010  0.1860\n",
      "     38       44.0489       \u001b[32m31.1562\u001b[0m  0.0010  0.1897\n",
      "     67       37.3063       26.8219  0.0010  0.1774\n",
      "     68       32.2117       30.5164  0.0005  0.2111\n",
      "     39       \u001b[36m42.4037\u001b[0m       \u001b[32m30.3935\u001b[0m  0.0010  0.1705\n",
      "     68       33.9026       26.8934  0.0010  0.1936\n",
      "     68       \u001b[36m35.3042\u001b[0m       27.4798  0.0010  0.2217\n",
      "     69       32.3267       30.0538  0.0005  0.2126\n",
      "     40       42.4122       \u001b[32m29.7403\u001b[0m  0.0010  0.1871\n",
      "     69       \u001b[36m33.0593\u001b[0m       \u001b[32m26.5237\u001b[0m  0.0010  0.2222\n",
      "     70       \u001b[36m30.8134\u001b[0m       30.1279  0.0005  0.1675\n",
      "     69       36.3952       \u001b[32m25.0895\u001b[0m  0.0010  0.1950\n",
      "     41       43.2342       \u001b[32m29.4477\u001b[0m  0.0010  0.1693\n",
      "     70       \u001b[36m32.0596\u001b[0m       \u001b[32m25.7956\u001b[0m  0.0010  0.1886\n",
      "     71       31.7592       30.3131  0.0005  0.1826\n",
      "     70       36.5372       25.6313  0.0010  0.1659\n",
      "     42       \u001b[36m41.2323\u001b[0m       \u001b[32m29.0070\u001b[0m  0.0010  0.1543\n",
      "     71       33.0269       \u001b[32m25.5388\u001b[0m  0.0010  0.1593\n",
      "     71       \u001b[36m34.8216\u001b[0m       26.2530  0.0010  0.1745\n",
      "     43       41.6317       \u001b[32m28.1404\u001b[0m  0.0010  0.1553\n",
      "     72       32.4567       \u001b[32m25.3359\u001b[0m  0.0010  0.1727\n",
      "     72       35.5352       25.3593  0.0010  0.1310\n",
      "     44       \u001b[36m39.8269\u001b[0m       \u001b[32m27.9217\u001b[0m  0.0010  0.2412\n",
      "     73       \u001b[36m32.0494\u001b[0m       25.3851  0.0010  0.2682\n",
      "     73       34.9700       25.5701  0.0010  0.3078\n",
      "     45       \u001b[36m38.9151\u001b[0m       \u001b[32m27.1114\u001b[0m  0.0010  0.2247\n",
      "     74       \u001b[36m31.6597\u001b[0m       \u001b[32m24.9010\u001b[0m  0.0010  0.2007\n",
      "     46       \u001b[36m38.6365\u001b[0m       \u001b[32m26.7518\u001b[0m  0.0010  0.1655\n",
      "     75       32.2812       25.0934  0.0010  0.1778\n",
      "     47       39.1834       \u001b[32m26.4563\u001b[0m  0.0010  0.1918\n",
      "     76       31.8590       \u001b[32m24.3147\u001b[0m  0.0010  0.1921\n",
      "     48       \u001b[36m38.3279\u001b[0m       \u001b[32m26.2487\u001b[0m  0.0010  0.1936\n",
      "     77       \u001b[36m31.0161\u001b[0m       24.5821  0.0010  0.1629\n",
      "     49       \u001b[36m37.3160\u001b[0m       \u001b[32m25.7446\u001b[0m  0.0010  0.1453\n",
      "     78       \u001b[36m30.8699\u001b[0m       \u001b[32m24.2073\u001b[0m  0.0010  0.1558\n",
      "     50       38.0992       \u001b[32m25.4011\u001b[0m  0.0010  0.1586\n",
      "     79       31.8435       \u001b[32m24.1703\u001b[0m  0.0010  0.1393\n",
      "     51       \u001b[36m37.2806\u001b[0m       25.7833  0.0010  0.1087\n",
      "     80       \u001b[36m30.6886\u001b[0m       \u001b[32m24.0753\u001b[0m  0.0010  0.1321\n",
      "     52       38.2321       25.4681  0.0010  0.1214\n",
      "     81       30.9184       \u001b[32m23.4002\u001b[0m  0.0010  0.1096\n",
      "     53       37.6549       \u001b[32m25.0471\u001b[0m  0.0010  0.1002\n",
      "     82       30.7659       24.3807  0.0010  0.1032\n",
      "     54       \u001b[36m36.9754\u001b[0m       25.1020  0.0010  0.1126\n",
      "     83       \u001b[36m30.2908\u001b[0m       23.9999  0.0010  0.1161\n",
      "     55       \u001b[36m35.7335\u001b[0m       \u001b[32m24.2389\u001b[0m  0.0010  0.1088\n",
      "     84       30.5324       \u001b[32m23.3133\u001b[0m  0.0010  0.1339\n",
      "     56       37.0589       \u001b[32m23.6475\u001b[0m  0.0010  0.1265\n",
      "     85       \u001b[36m30.2265\u001b[0m       23.6203  0.0010  0.1037\n",
      "     57       36.1853       24.6160  0.0010  0.1062\n",
      "     86       31.4114       23.3204  0.0010  0.1443\n",
      "     58       \u001b[36m34.2405\u001b[0m       24.0371  0.0010  0.1557\n",
      "     87       \u001b[36m30.0974\u001b[0m       23.6980  0.0010  0.1441\n",
      "     59       35.2502       \u001b[32m23.4772\u001b[0m  0.0010  0.1271\n",
      "     88       \u001b[36m30.0765\u001b[0m       23.4518  0.0010  0.1150\n",
      "     60       34.7021       \u001b[32m23.3871\u001b[0m  0.0010  0.1158\n",
      "     89       30.4052       \u001b[32m22.9232\u001b[0m  0.0005  0.0965\n",
      "     61       34.3875       \u001b[32m23.0282\u001b[0m  0.0010  0.1146\n",
      "     90       \u001b[36m29.6513\u001b[0m       \u001b[32m22.5519\u001b[0m  0.0005  0.1239\n",
      "     62       35.3390       \u001b[32m22.6558\u001b[0m  0.0010  0.1166\n",
      "     91       \u001b[36m28.7927\u001b[0m       22.7081  0.0005  0.0978\n",
      "     63       34.5152       \u001b[32m22.2858\u001b[0m  0.0010  0.0874\n",
      "     92       29.4652       \u001b[32m22.4969\u001b[0m  0.0005  0.0919\n",
      "     64       \u001b[36m33.5906\u001b[0m       \u001b[32m21.8970\u001b[0m  0.0010  0.0998\n",
      "     93       \u001b[36m28.4011\u001b[0m       22.5164  0.0005  0.0977\n",
      "     65       33.6954       \u001b[32m21.8049\u001b[0m  0.0010  0.0884\n",
      "     94       28.8968       \u001b[32m22.3255\u001b[0m  0.0005  0.1008\n",
      "     66       33.7498       \u001b[32m21.7474\u001b[0m  0.0010  0.1100\n",
      "     95       28.4862       22.7410  0.0005  0.1019\n",
      "     67       \u001b[36m33.0021\u001b[0m       22.0091  0.0010  0.0915\n",
      "     96       28.5590       \u001b[32m22.1352\u001b[0m  0.0005  0.0825\n",
      "     68       \u001b[36m32.9737\u001b[0m       \u001b[32m21.6785\u001b[0m  0.0010  0.0806\n",
      "     97       \u001b[36m28.3575\u001b[0m       22.2861  0.0005  0.0921\n",
      "     69       \u001b[36m32.5434\u001b[0m       \u001b[32m21.4175\u001b[0m  0.0010  0.1014\n",
      "     98       29.0394       22.3630  0.0005  0.1154\n",
      "     70       \u001b[36m32.2265\u001b[0m       21.8886  0.0010  0.1049\n",
      "     99       28.6903       \u001b[32m22.1254\u001b[0m  0.0005  0.1009\n",
      "     71       32.3995       \u001b[32m20.9193\u001b[0m  0.0010  0.1009\n",
      "    100       28.3705       22.5546  0.0005  0.1178\n",
      "     72       32.3813       \u001b[32m20.6827\u001b[0m  0.0010  0.1108\n",
      "    101       \u001b[36m28.2431\u001b[0m       22.3860  0.0005  0.0930\n",
      "     73       \u001b[36m31.9017\u001b[0m       20.9179  0.0010  0.0964\n",
      "    102       29.1629       23.3125  0.0005  0.0971\n",
      "     74       32.8124       20.7166  0.0010  0.0894\n",
      "    103       \u001b[36m27.2443\u001b[0m       22.3545  0.0005  0.0840\n",
      "     75       32.2817       20.8259  0.0010  0.0802\n",
      "     76       \u001b[36m30.8757\u001b[0m       \u001b[32m20.6022\u001b[0m  0.0010  0.0922\n",
      "     77       31.7388       \u001b[32m20.4223\u001b[0m  0.0010  0.0778\n",
      "     78       \u001b[36m30.3228\u001b[0m       20.4477  0.0010  0.1042\n",
      "     79       30.9437       \u001b[32m19.9584\u001b[0m  0.0010  0.0944\n",
      "     80       31.1873       20.1613  0.0010  0.1126\n",
      "     81       31.4203       \u001b[32m19.5236\u001b[0m  0.0010  0.0873\n",
      "     82       30.5431       20.5436  0.0010  0.0890\n",
      "     83       \u001b[36m29.9724\u001b[0m       19.8174  0.0010  0.0718\n",
      "     84       30.7837       19.6782  0.0010  0.0714\n",
      "     85       \u001b[36m29.4479\u001b[0m       19.6299  0.0010  0.0654\n",
      "     86       29.8955       \u001b[32m19.1987\u001b[0m  0.0005  0.0642\n",
      "     87       \u001b[36m29.1211\u001b[0m       19.3758  0.0005  0.0648\n",
      "     88       29.7725       19.6049  0.0005  0.0766\n",
      "     89       \u001b[36m28.8647\u001b[0m       \u001b[32m19.0204\u001b[0m  0.0005  0.0627\n",
      "     90       29.1147       19.2976  0.0005  0.0628\n",
      "     91       29.4802       \u001b[32m18.7881\u001b[0m  0.0005  0.0636\n",
      "     92       29.0947       18.9586  0.0005  0.0650\n",
      "     93       29.9764       \u001b[32m18.7386\u001b[0m  0.0005  0.0620\n",
      "     94       29.5176       18.9188  0.0005  0.0610\n",
      "     95       29.1022       18.9004  0.0005  0.0603\n",
      "     96       \u001b[36m27.7198\u001b[0m       \u001b[32m18.7251\u001b[0m  0.0005  0.1117\n",
      "     97       27.9261       \u001b[32m18.6582\u001b[0m  0.0005  0.0707\n",
      "     98       27.9832       18.8386  0.0005  0.0848\n",
      "     99       28.6212       18.9279  0.0005  0.0692\n",
      "    100       27.7948       18.6593  0.0005  0.0650\n",
      "    101       \u001b[36m27.4773\u001b[0m       \u001b[32m18.5393\u001b[0m  0.0005  0.0685\n",
      "    102       27.7801       \u001b[32m18.4964\u001b[0m  0.0005  0.0636\n",
      "    103       28.4905       19.1072  0.0005  0.0665\n",
      "    104       \u001b[36m27.4069\u001b[0m       18.7009  0.0005  0.0634\n",
      "    105       28.5435       \u001b[32m18.4488\u001b[0m  0.0005  0.0607\n",
      "    106       28.4082       \u001b[32m18.2056\u001b[0m  0.0005  0.0607\n",
      "    107       28.2232       \u001b[32m18.1102\u001b[0m  0.0005  0.0622\n",
      "    108       28.8528       18.6535  0.0005  0.0612\n",
      "    109       28.4849       18.3479  0.0005  0.0618\n",
      "    110       27.4082       \u001b[32m17.9405\u001b[0m  0.0005  0.0672\n",
      "    111       27.4181       18.1301  0.0005  0.0689\n",
      "    112       \u001b[36m27.2928\u001b[0m       18.1421  0.0005  0.0693\n",
      "    113       28.2698       18.0781  0.0005  0.0627\n",
      "    114       \u001b[36m27.1628\u001b[0m       18.0572  0.0005  0.0606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n",
      "/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/skorch/net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m681.3940\u001b[0m      \u001b[32m458.4304\u001b[0m  0.0010  0.2275\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m689.2100\u001b[0m      \u001b[32m435.2148\u001b[0m  0.0010  0.2204\n",
      "      2      \u001b[36m369.4364\u001b[0m      \u001b[32m179.7775\u001b[0m  0.0010  0.1962\n",
      "      2      \u001b[36m354.2456\u001b[0m      \u001b[32m181.3777\u001b[0m  0.0010  0.2440\n",
      "      3      \u001b[36m181.1057\u001b[0m      \u001b[32m145.8905\u001b[0m  0.0010  0.2057\n",
      "      3      \u001b[36m179.7771\u001b[0m      \u001b[32m143.8402\u001b[0m  0.0010  0.2161\n",
      "      4      \u001b[36m159.6705\u001b[0m      \u001b[32m137.5356\u001b[0m  0.0010  0.1775\n",
      "      4      \u001b[36m161.0263\u001b[0m      \u001b[32m135.0010\u001b[0m  0.0010  0.1820\n",
      "      5      \u001b[36m152.6489\u001b[0m      \u001b[32m133.5921\u001b[0m  0.0010  0.1994\n",
      "      5      \u001b[36m153.4517\u001b[0m      \u001b[32m131.2604\u001b[0m  0.0010  0.1597\n",
      "      6      \u001b[36m145.9169\u001b[0m      \u001b[32m130.5462\u001b[0m  0.0010  0.1963\n",
      "      6      \u001b[36m148.4606\u001b[0m      \u001b[32m127.8963\u001b[0m  0.0010  0.2221\n",
      "      7      \u001b[36m140.5924\u001b[0m      \u001b[32m127.7048\u001b[0m  0.0010  0.2289\n",
      "      7      \u001b[36m144.5016\u001b[0m      \u001b[32m125.8295\u001b[0m  0.0010  0.1916\n",
      "      8      \u001b[36m135.5665\u001b[0m      \u001b[32m126.6102\u001b[0m  0.0010  0.2244\n",
      "      8      \u001b[36m139.7324\u001b[0m      \u001b[32m124.1581\u001b[0m  0.0010  0.2098\n",
      "      9      \u001b[36m133.4819\u001b[0m      \u001b[32m125.0181\u001b[0m  0.0010  0.2379\n",
      "      9      \u001b[36m136.7888\u001b[0m      \u001b[32m122.9304\u001b[0m  0.0010  0.2476\n",
      "     10      \u001b[36m128.4702\u001b[0m      \u001b[32m123.9164\u001b[0m  0.0010  0.2108\n",
      "     10      \u001b[36m132.9537\u001b[0m      \u001b[32m121.4545\u001b[0m  0.0010  0.2008\n",
      "     11      \u001b[36m126.3820\u001b[0m      \u001b[32m123.2279\u001b[0m  0.0010  0.1815\n",
      "     11      \u001b[36m129.3771\u001b[0m      \u001b[32m119.9302\u001b[0m  0.0010  0.1803\n",
      "     12      \u001b[36m123.1065\u001b[0m      \u001b[32m122.6314\u001b[0m  0.0010  0.1840\n",
      "     12      \u001b[36m128.7049\u001b[0m      \u001b[32m118.5741\u001b[0m  0.0010  0.1859\n",
      "     13      123.3218      \u001b[32m121.2459\u001b[0m  0.0010  0.1782\n",
      "     13      \u001b[36m126.8398\u001b[0m      \u001b[32m117.4828\u001b[0m  0.0010  0.1928\n",
      "     14      \u001b[36m118.9160\u001b[0m      \u001b[32m120.8721\u001b[0m  0.0010  0.1847\n",
      "     14      \u001b[36m122.2172\u001b[0m      \u001b[32m116.9748\u001b[0m  0.0010  0.1680\n",
      "     15      \u001b[36m118.4450\u001b[0m      \u001b[32m120.5335\u001b[0m  0.0010  0.1788\n",
      "     15      122.6732      \u001b[32m116.6835\u001b[0m  0.0010  0.1764\n",
      "     16      \u001b[36m116.1429\u001b[0m      120.7686  0.0010  0.1672\n",
      "     16      123.9279      \u001b[32m116.4013\u001b[0m  0.0010  0.1898\n",
      "     17      \u001b[36m114.2446\u001b[0m      \u001b[32m120.3382\u001b[0m  0.0010  0.1675\n",
      "     17      122.6721      \u001b[32m115.5259\u001b[0m  0.0010  0.1819\n",
      "     18      114.3933      \u001b[32m119.8541\u001b[0m  0.0010  0.1488\n",
      "     18      \u001b[36m120.5475\u001b[0m      \u001b[32m115.1580\u001b[0m  0.0010  0.1737\n",
      "     19      \u001b[36m113.2255\u001b[0m      120.3593  0.0010  0.1933\n",
      "     19      \u001b[36m116.6379\u001b[0m      \u001b[32m114.7627\u001b[0m  0.0010  0.2146\n",
      "     20      \u001b[36m110.6642\u001b[0m      \u001b[32m119.4328\u001b[0m  0.0010  0.1979\n",
      "     20      118.9968      \u001b[32m114.2981\u001b[0m  0.0010  0.2283\n",
      "     21      112.1706      119.6214  0.0010  0.2407\n",
      "     21      117.7963      114.5364  0.0010  0.2542\n",
      "     22      \u001b[36m109.5321\u001b[0m      \u001b[32m118.9720\u001b[0m  0.0010  0.2329\n",
      "     22      116.8688      114.4449  0.0010  0.2418\n",
      "     23      \u001b[36m108.2863\u001b[0m      \u001b[32m118.7977\u001b[0m  0.0010  0.2347\n",
      "     23      117.3921      114.5634  0.0010  0.2451\n",
      "     24      \u001b[36m108.2820\u001b[0m      \u001b[32m118.3790\u001b[0m  0.0010  0.2286\n",
      "     24      116.6744      \u001b[32m114.2579\u001b[0m  0.0010  0.2622\n",
      "     25      \u001b[36m108.1349\u001b[0m      \u001b[32m118.2477\u001b[0m  0.0010  0.2318\n",
      "     25      \u001b[36m115.8938\u001b[0m      \u001b[32m114.1435\u001b[0m  0.0010  0.2216\n",
      "     26      108.2064      118.4056  0.0010  0.2279\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m559.7707\u001b[0m      \u001b[32m812.4731\u001b[0m  0.0010  0.2426\n",
      "     26      \u001b[36m114.5116\u001b[0m      \u001b[32m113.8827\u001b[0m  0.0010  0.2366\n",
      "     27      108.6156      \u001b[32m117.8767\u001b[0m  0.0010  0.2400\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m641.4553\u001b[0m      \u001b[32m471.7131\u001b[0m  0.0010  0.2323\n",
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1      \u001b[36m546.4995\u001b[0m      \u001b[32m468.1339\u001b[0m  0.0010  0.2401\n",
      "      2      \u001b[36m300.1753\u001b[0m      \u001b[32m301.6613\u001b[0m  0.0010  0.2232\n",
      "     27      \u001b[36m113.8446\u001b[0m      \u001b[32m113.7271\u001b[0m  0.0010  0.2332\n",
      "     28      \u001b[36m106.5506\u001b[0m      \u001b[32m117.1416\u001b[0m  0.0010  0.2505\n",
      "      2      \u001b[36m352.1658\u001b[0m      \u001b[32m200.1130\u001b[0m  0.0010  0.2429\n",
      "      3      \u001b[36m159.4980\u001b[0m      \u001b[32m220.1060\u001b[0m  0.0010  0.2202\n",
      "      2      \u001b[36m292.5907\u001b[0m      \u001b[32m189.4694\u001b[0m  0.0010  0.2407\n",
      "     28      \u001b[36m113.8270\u001b[0m      113.9682  0.0010  0.2631\n",
      "     29      \u001b[36m105.9035\u001b[0m      \u001b[32m116.8207\u001b[0m  0.0010  0.2573\n",
      "      3      \u001b[36m167.3910\u001b[0m      \u001b[32m152.8476\u001b[0m  0.0010  0.2417\n",
      "      3      \u001b[36m153.1821\u001b[0m      \u001b[32m153.4194\u001b[0m  0.0010  0.2459\n",
      "      4      \u001b[36m142.4542\u001b[0m      \u001b[32m200.3805\u001b[0m  0.0010  0.2622\n",
      "     29      \u001b[36m113.4319\u001b[0m      \u001b[32m113.6476\u001b[0m  0.0010  0.2617\n",
      "      4      \u001b[36m148.0567\u001b[0m      \u001b[32m142.2621\u001b[0m  0.0010  0.2292\n",
      "     30      \u001b[36m103.8198\u001b[0m      \u001b[32m116.3377\u001b[0m  0.0010  0.2637\n",
      "      4      \u001b[36m136.8515\u001b[0m      \u001b[32m143.0713\u001b[0m  0.0010  0.2547\n",
      "      5      \u001b[36m136.5954\u001b[0m      \u001b[32m191.3602\u001b[0m  0.0010  0.2439\n",
      "     30      114.0543      \u001b[32m113.1920\u001b[0m  0.0010  0.2491\n",
      "      5      \u001b[36m140.8883\u001b[0m      \u001b[32m136.2289\u001b[0m  0.0010  0.2342\n",
      "     31      104.5887      \u001b[32m116.2806\u001b[0m  0.0010  0.2328\n",
      "      6      \u001b[36m132.6735\u001b[0m      \u001b[32m184.5600\u001b[0m  0.0010  0.2150\n",
      "      5      \u001b[36m134.1964\u001b[0m      \u001b[32m136.5712\u001b[0m  0.0010  0.2253\n",
      "     31      \u001b[36m110.6075\u001b[0m      113.6999  0.0010  0.2055\n",
      "     32      104.6446      \u001b[32m115.7656\u001b[0m  0.0010  0.1836\n",
      "      6      \u001b[36m134.1279\u001b[0m      \u001b[32m131.9188\u001b[0m  0.0010  0.2083\n",
      "      7      \u001b[36m128.9484\u001b[0m      \u001b[32m179.7912\u001b[0m  0.0010  0.1914\n",
      "      6      \u001b[36m127.3711\u001b[0m      \u001b[32m132.6148\u001b[0m  0.0010  0.1922\n",
      "     32      112.7471      \u001b[32m113.1313\u001b[0m  0.0010  0.1939\n",
      "      8      \u001b[36m126.5436\u001b[0m      \u001b[32m173.4102\u001b[0m  0.0010  0.1837\n",
      "      7      \u001b[36m130.9579\u001b[0m      \u001b[32m129.6614\u001b[0m  0.0010  0.1931\n",
      "     33      104.2675      116.1850  0.0010  0.2071\n",
      "      7      \u001b[36m123.3112\u001b[0m      \u001b[32m130.2622\u001b[0m  0.0010  0.2078\n",
      "      8      \u001b[36m126.5323\u001b[0m      \u001b[32m127.0503\u001b[0m  0.0010  0.1693\n",
      "      9      \u001b[36m123.3147\u001b[0m      \u001b[32m167.8878\u001b[0m  0.0010  0.1826\n",
      "     33      \u001b[36m110.3370\u001b[0m      113.7725  0.0010  0.2235\n",
      "     34      \u001b[36m103.3189\u001b[0m      \u001b[32m115.6541\u001b[0m  0.0010  0.2085\n",
      "      8      \u001b[36m121.2866\u001b[0m      \u001b[32m127.8669\u001b[0m  0.0010  0.1959\n",
      "      9      \u001b[36m121.5367\u001b[0m      \u001b[32m125.2317\u001b[0m  0.0010  0.1922\n",
      "     10      \u001b[36m121.0761\u001b[0m      \u001b[32m165.6759\u001b[0m  0.0010  0.1943\n",
      "     34      111.9399      114.0180  0.0010  0.1928\n",
      "     35      \u001b[36m102.3199\u001b[0m      \u001b[32m114.9392\u001b[0m  0.0010  0.1970\n",
      "      9      \u001b[36m118.1472\u001b[0m      \u001b[32m126.1624\u001b[0m  0.0010  0.2011\n",
      "     10      \u001b[36m118.3676\u001b[0m      \u001b[32m124.6669\u001b[0m  0.0010  0.2158\n",
      "     11      \u001b[36m117.5405\u001b[0m      \u001b[32m159.6383\u001b[0m  0.0010  0.2019\n",
      "     36      103.0303      \u001b[32m114.7311\u001b[0m  0.0010  0.1907\n",
      "     35      111.2994      113.8858  0.0010  0.2183\n",
      "     10      \u001b[36m115.6079\u001b[0m      \u001b[32m124.1855\u001b[0m  0.0010  0.2127\n",
      "     11      \u001b[36m115.7408\u001b[0m      \u001b[32m124.0586\u001b[0m  0.0010  0.1965\n",
      "     12      \u001b[36m115.7161\u001b[0m      \u001b[32m154.9006\u001b[0m  0.0010  0.1961\n",
      "     36      110.3438      \u001b[32m113.0270\u001b[0m  0.0010  0.1740\n",
      "     37      \u001b[36m102.1385\u001b[0m      115.1922  0.0010  0.1893\n",
      "     11      \u001b[36m112.5131\u001b[0m      \u001b[32m123.4360\u001b[0m  0.0010  0.1827\n",
      "     13      \u001b[36m114.0629\u001b[0m      \u001b[32m153.3067\u001b[0m  0.0010  0.1926\n",
      "     12      \u001b[36m113.8282\u001b[0m      \u001b[32m123.6953\u001b[0m  0.0010  0.1948\n",
      "     37      \u001b[36m109.2244\u001b[0m      \u001b[32m112.7422\u001b[0m  0.0010  0.1906\n",
      "     38      \u001b[36m101.3156\u001b[0m      \u001b[32m114.3159\u001b[0m  0.0010  0.2480\n",
      "     12      \u001b[36m110.5307\u001b[0m      \u001b[32m122.0101\u001b[0m  0.0010  0.2398\n",
      "     14      \u001b[36m112.8379\u001b[0m      \u001b[32m150.5190\u001b[0m  0.0010  0.2229\n",
      "     13      \u001b[36m110.3518\u001b[0m      124.0777  0.0010  0.2274\n",
      "     38      109.3822      112.8265  0.0010  0.2306\n",
      "     39      \u001b[36m100.1247\u001b[0m      \u001b[32m114.2244\u001b[0m  0.0010  0.1994\n",
      "     13      \u001b[36m109.1186\u001b[0m      \u001b[32m120.6880\u001b[0m  0.0010  0.2147\n",
      "     15      \u001b[36m111.5881\u001b[0m      \u001b[32m147.7160\u001b[0m  0.0010  0.1844\n",
      "     39      111.3809      112.7854  0.0010  0.1944\n",
      "     14      \u001b[36m110.3095\u001b[0m      124.0788  0.0010  0.2105\n",
      "     14      \u001b[36m109.0113\u001b[0m      \u001b[32m120.1138\u001b[0m  0.0010  0.1916\n",
      "     40      102.4991      \u001b[32m113.8488\u001b[0m  0.0010  0.2285\n",
      "     16      \u001b[36m108.7561\u001b[0m      \u001b[32m145.1401\u001b[0m  0.0010  0.2012\n",
      "     15      \u001b[36m108.6994\u001b[0m      \u001b[32m122.4088\u001b[0m  0.0010  0.1901\n",
      "     40      \u001b[36m109.0347\u001b[0m      \u001b[32m112.2639\u001b[0m  0.0010  0.1985\n",
      "     15      \u001b[36m107.3793\u001b[0m      \u001b[32m118.6545\u001b[0m  0.0010  0.1926\n",
      "     41      100.8265      114.7839  0.0010  0.2032\n",
      "     17      \u001b[36m106.9111\u001b[0m      \u001b[32m144.9336\u001b[0m  0.0010  0.1862\n",
      "     16      \u001b[36m105.9537\u001b[0m      \u001b[32m122.1888\u001b[0m  0.0010  0.1929\n",
      "     41      \u001b[36m108.5845\u001b[0m      112.8422  0.0010  0.1975\n",
      "     16      \u001b[36m105.8171\u001b[0m      \u001b[32m118.0986\u001b[0m  0.0010  0.1729\n",
      "     42       \u001b[36m99.9860\u001b[0m      114.7877  0.0010  0.1824\n",
      "     18      108.4981      \u001b[32m143.3678\u001b[0m  0.0010  0.1938\n",
      "     17      \u001b[36m104.8952\u001b[0m      \u001b[32m121.9674\u001b[0m  0.0010  0.1876\n",
      "     42      \u001b[36m107.9124\u001b[0m      112.4565  0.0010  0.1918\n",
      "     17      \u001b[36m103.8430\u001b[0m      \u001b[32m117.4005\u001b[0m  0.0010  0.2033\n",
      "     43       \u001b[36m99.6600\u001b[0m      114.9150  0.0010  0.1838\n",
      "     19      \u001b[36m105.3710\u001b[0m      \u001b[32m143.2578\u001b[0m  0.0010  0.1680\n",
      "     18      \u001b[36m104.0307\u001b[0m      122.1150  0.0010  0.1753\n",
      "     43      108.2477      \u001b[32m111.9683\u001b[0m  0.0010  0.2115\n",
      "     18      \u001b[36m102.9854\u001b[0m      \u001b[32m116.1542\u001b[0m  0.0010  0.1788\n",
      "     44       \u001b[36m99.3582\u001b[0m      113.9561  0.0010  0.2012\n",
      "     20      106.0962      143.3153  0.0010  0.1873\n",
      "     19      \u001b[36m103.9639\u001b[0m      122.0858  0.0010  0.1989\n",
      "     44      \u001b[36m106.8440\u001b[0m      \u001b[32m111.6296\u001b[0m  0.0010  0.1910\n",
      "     19      104.3142      \u001b[32m115.9704\u001b[0m  0.0010  0.2038\n",
      "     21      \u001b[36m103.5404\u001b[0m      \u001b[32m140.3419\u001b[0m  0.0010  0.1908\n",
      "     20      104.1019      122.5641  0.0010  0.1719\n",
      "     45      108.6235      112.0292  0.0010  0.2028\n",
      "     20      \u001b[36m102.9143\u001b[0m      \u001b[32m115.9634\u001b[0m  0.0010  0.2025\n",
      "     22      \u001b[36m103.2680\u001b[0m      140.6783  0.0010  0.1947\n",
      "     21      \u001b[36m102.3235\u001b[0m      122.3960  0.0010  0.1840\n",
      "     46      \u001b[36m105.6257\u001b[0m      \u001b[32m111.5757\u001b[0m  0.0010  0.1883\n",
      "     21      \u001b[36m102.1204\u001b[0m      116.3450  0.0010  0.1896\n",
      "     23      103.7966      140.6260  0.0010  0.1863\n",
      "     22       \u001b[36m99.7576\u001b[0m      \u001b[32m120.3528\u001b[0m  0.0005  0.1927\n",
      "     47      107.0445      111.6705  0.0010  0.1993\n",
      "     22      \u001b[36m100.1530\u001b[0m      \u001b[32m115.6280\u001b[0m  0.0010  0.1817\n",
      "     24      \u001b[36m102.3756\u001b[0m      140.6615  0.0010  0.1656\n",
      "     23      100.9187      120.4212  0.0005  0.1872\n",
      "     48      106.8761      \u001b[32m110.6462\u001b[0m  0.0010  0.1794\n",
      "     23      100.7379      \u001b[32m115.3983\u001b[0m  0.0010  0.1898\n",
      "     25      \u001b[36m101.0688\u001b[0m      \u001b[32m138.9302\u001b[0m  0.0010  0.1811\n",
      "     24       \u001b[36m99.5686\u001b[0m      \u001b[32m120.2765\u001b[0m  0.0005  0.2015\n",
      "     49      106.5982      \u001b[32m110.5796\u001b[0m  0.0010  0.1568\n",
      "     26       \u001b[36m99.5599\u001b[0m      \u001b[32m138.1641\u001b[0m  0.0010  0.1459\n",
      "     24       \u001b[36m98.8472\u001b[0m      115.5059  0.0010  0.1942\n",
      "     25       \u001b[36m99.5603\u001b[0m      \u001b[32m119.9960\u001b[0m  0.0005  0.1614\n",
      "     50      \u001b[36m104.4417\u001b[0m      110.8122  0.0010  0.1797\n",
      "     27       \u001b[36m99.0506\u001b[0m      \u001b[32m136.7616\u001b[0m  0.0010  0.1537\n",
      "     25       \u001b[36m98.1310\u001b[0m      115.4674  0.0010  0.1744\n",
      "     26       \u001b[36m99.0136\u001b[0m      \u001b[32m119.6209\u001b[0m  0.0005  0.1590\n",
      "     51      106.6161      110.9313  0.0010  0.1719\n",
      "     28       99.9119      \u001b[32m136.2904\u001b[0m  0.0010  0.1625\n",
      "     26       \u001b[36m97.2408\u001b[0m      \u001b[32m114.9218\u001b[0m  0.0010  0.1542\n",
      "     27       99.6754      120.1927  0.0005  0.2021\n",
      "     52      \u001b[36m104.2838\u001b[0m      111.3600  0.0010  0.2467\n",
      "     29      100.7525      \u001b[32m135.7807\u001b[0m  0.0010  0.2289\n",
      "     27       97.9472      \u001b[32m114.5947\u001b[0m  0.0010  0.2567\n",
      "     28       \u001b[36m98.9831\u001b[0m      119.9116  0.0005  0.1885\n",
      "     53      106.1438      111.0579  0.0010  0.1442\n",
      "     30       \u001b[36m97.9619\u001b[0m      \u001b[32m134.7122\u001b[0m  0.0010  0.1730\n",
      "     28       97.5095      115.1450  0.0010  0.1422\n",
      "     29       \u001b[36m96.7860\u001b[0m      119.8391  0.0005  0.1532\n",
      "     31       \u001b[36m97.8299\u001b[0m      134.7516  0.0010  0.1438\n",
      "     54      104.3034      \u001b[32m110.5592\u001b[0m  0.0005  0.2007\n",
      "     29       97.9402      115.1961  0.0010  0.1717\n",
      "     30       98.1084      120.1726  0.0005  0.1547\n",
      "     32       \u001b[36m97.7501\u001b[0m      \u001b[32m133.7038\u001b[0m  0.0010  0.1572\n",
      "     55      \u001b[36m103.7811\u001b[0m      \u001b[32m110.5431\u001b[0m  0.0005  0.1572\n",
      "     30       \u001b[36m96.7220\u001b[0m      115.6819  0.0010  0.1623\n",
      "     33       \u001b[36m97.5415\u001b[0m      134.6082  0.0010  0.1646\n",
      "     56      104.2357      \u001b[32m110.2167\u001b[0m  0.0005  0.1554\n",
      "     31       96.9160      114.9147  0.0010  0.1717\n",
      "     34       \u001b[36m96.5999\u001b[0m      \u001b[32m132.1813\u001b[0m  0.0010  0.1654\n",
      "     57      \u001b[36m103.4571\u001b[0m      \u001b[32m109.4715\u001b[0m  0.0005  0.1739\n",
      "     35       \u001b[36m96.5191\u001b[0m      135.2148  0.0010  0.1609\n",
      "     58      104.4032      109.7473  0.0005  0.1536\n",
      "     36       97.3104      134.3491  0.0010  0.1358\n",
      "     59      104.1554      109.8132  0.0005  0.1577\n",
      "     37       \u001b[36m95.5015\u001b[0m      134.5182  0.0010  0.1362\n",
      "     60      103.8078      109.9144  0.0005  0.1888\n",
      "     38       \u001b[36m94.6274\u001b[0m      134.4419  0.0010  0.1653\n",
      "     61      \u001b[36m102.2933\u001b[0m      \u001b[32m109.1727\u001b[0m  0.0005  0.1684\n",
      "     39       \u001b[36m93.7015\u001b[0m      \u001b[32m127.9554\u001b[0m  0.0005  0.1907\n",
      "     62      103.1376      109.6814  0.0005  0.1734\n",
      "     40       93.8242      127.9917  0.0005  0.1419\n",
      "     63      103.1763      109.2956  0.0005  0.1235\n",
      "     41       \u001b[36m92.8190\u001b[0m      \u001b[32m127.8607\u001b[0m  0.0005  0.1146\n",
      "     64      103.1029      109.5630  0.0005  0.1093\n",
      "     42       93.9163      \u001b[32m127.3367\u001b[0m  0.0005  0.1113\n",
      "     65      \u001b[36m100.9155\u001b[0m      109.3774  0.0005  0.1179\n",
      "     43       93.3251      127.7085  0.0005  0.1115\n",
      "     44       93.1446      127.8100  0.0005  0.1036\n",
      "     45       \u001b[36m92.5841\u001b[0m      127.4706  0.0005  0.1016\n",
      "     46       93.1533      127.4908  0.0005  0.1018\n",
      "     47       \u001b[36m91.7288\u001b[0m      \u001b[32m126.1169\u001b[0m  0.0003  0.0993\n",
      "     48       \u001b[36m91.2993\u001b[0m      \u001b[32m125.1853\u001b[0m  0.0003  0.0924\n",
      "     49       92.8183      125.5683  0.0003  0.0902\n",
      "     50       91.3066      \u001b[32m125.1523\u001b[0m  0.0003  0.0849\n",
      "     51       91.6706      125.1695  0.0003  0.0830\n",
      "     52       91.5163      \u001b[32m125.1010\u001b[0m  0.0003  0.0871\n",
      "     53       91.6620      125.4146  0.0003  0.0757\n",
      "     54       91.4406      125.9881  0.0003  0.0787\n",
      "     55       \u001b[36m90.8314\u001b[0m      125.3483  0.0003  0.0845\n",
      "     56       92.5220      125.6272  0.0003  0.0793\n"
     ]
    }
   ],
   "source": [
    "# Prediction of each model for the guided and free dataset \n",
    "\n",
    "...\n",
    "\n",
    "y_g_predict_cnn = cross_val_predict(pipe_cnn,x, y, groups=groups, cv=logo, n_jobs=-1)\n",
    "y_f_predict_cnn = cross_val_predict(pipe_cnn,x_f, y_f, groups=groups_f, cv=logo, n_jobs=-1)\n",
    "\n",
    "y_g_predict_nn_covariance = cross_val_predict(pipe_fusion, x_cov_nn, y, groups=groups, cv=logo, n_jobs=-1)\n",
    "y_f_predict_nn_covariance = cross_val_predict(pipe_fusion, x_cov_nn_f, y_f, groups=groups_f, cv=logo, n_jobs=-1)\n",
    "\n",
    "\n",
    "# Rajouter les prédictions de vos modéles \n",
    "y_g_predict_ensemble = (y_g_predict_cnn + y_g_predict_nn_covariance) / 2 \n",
    "y_f_predict_ensemble = (y_f_predict_cnn + y_f_predict_nn_covariance) / 2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "663b6d7d-2915-465a-bbb8-220eac33dfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.530913873241622 5.19298548390846 11.823693517406255 10.702586807864854\n",
      "0.044197626 0.05805793 0.24166672 0.19801041\n",
      "RMSE scores of the average method guided: 4.462801515453383 and free : 10.670169096360276\n",
      "NMSE scores of the average method guided: 0.042878784 and free : 0.1968127\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(y,y_predict):\n",
    "    mse  = mean_squared_error(y, y_predict)\n",
    "    rmse = np.sqrt(mse)\n",
    "    nmse = mse / np.var(y)  \n",
    "    return rmse, nmse \n",
    "\n",
    "\n",
    "# Metrics Guided\n",
    "rmse_g_cnn,   nmse_g_cnn   = calculate_metrics(y, y_g_predict_cnn)\n",
    "rmse_g_nn,    nmse_g_nn    = calculate_metrics(y, y_g_predict_nn_covariance)\n",
    "rmse_g_ens,   nmse_g_ens   = calculate_metrics(y, y_g_predict_ensemble)\n",
    "\n",
    "\n",
    "# RMSE Free\n",
    "rmse_f_cnn,   nmse_f_cnn   = calculate_metrics(y_f, y_f_predict_cnn)\n",
    "rmse_f_nn,    nmse_f_nn    = calculate_metrics(y_f, y_f_predict_nn_covariance)\n",
    "rmse_f_ens,   nmse_f_ens   = calculate_metrics(y_f, y_f_predict_ensemble)\n",
    "\n",
    "\n",
    "print(rmse_g_cnn,rmse_g_nn,rmse_f_cnn, rmse_f_nn)\n",
    "print(nmse_g_cnn,nmse_g_nn,nmse_f_cnn, nmse_f_nn)\n",
    "\n",
    "print(\"RMSE scores of the average method guided:\", rmse_g_ens, \"and free :\",rmse_f_ens)\n",
    "print(\"NMSE scores of the average method guided:\", nmse_g_ens, \"and free :\",nmse_f_ens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcfe570-c122-49e5-97cd-df0cc4f7733f",
   "metadata": {},
   "source": [
    "At this point we have our ensemble predictions (y_ensemble) and all the metrics we need for the average method. The power of this approach depends not only on each models individual performance but also on the diversity of their errors. Because we weight every model equally, a very weak model can drag the average down.\n",
    "\n",
    "However, when all the models are accurate enough and make different kinds of mistakes, averaging becomes extremely effective. For example, our CNN excels at learning local, spatial patterns in the raw filtered signal, while our nn + covariance network captures global statistical dependencies over time. Their errors rarely coincide, so when the CNN prediction is off for a given sample, the nn model often compensates it, and vice-versa. By averaging their outputs, we reduce the total variance and achieve a lower rmse/nmse than either model on its own, all without any extra hyperparameter tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db5d2bb-967f-4213-b2f5-84c46be9f023",
   "metadata": {},
   "source": [
    "For the second method, we concatenate the predictions of our models into a new dataset. This dataset will have the form (n_windows, targets × M), with M being the number of different models. After that, we train our model on this data and calculate the RMSE compared to the true y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45e80d0e-410f-49a8-8f90-6c86d5af21fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Creation of the new dataset with the prediction of our models \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m x_g_meta \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mconcatenate([y_g_predict_cnn, y_g_predict_nn_covariance], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m x_f_meta \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([y_f_predict_cnn, y_f_predict_nn_covariance], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# model \u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Creation of the new dataset with the prediction of our models \n",
    "x_g_meta = np.concatenate([y_g_predict_cnn, y_g_predict_nn_covariance], axis=1)\n",
    "x_f_meta = np.concatenate([y_f_predict_cnn, y_f_predict_nn_covariance], axis=1) \n",
    "\n",
    "# model \n",
    "lasso_model = MultiOutputRegressor(\n",
    "    Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('lasso',  Lasso(alpha=0.1, max_iter=20000))\n",
    "    ]),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Calculate the prediction\n",
    "y_g_predict_stack = cross_val_predict(lasso_model,x_g_meta,y,groups=groups,cv=logo,n_jobs=-1)\n",
    "y_f_predict_stack = cross_val_predict(lasso_model,x_f_meta,y_f,groups=groups_f,cv=logo,n_jobs=-1)\n",
    "\n",
    "\n",
    "#Calculate the metrics \n",
    "rmse_g_stack, nmse_g_stack = calculate_metrics(y, y_g_predict_stack)\n",
    "rmse_f_stack, nmse_f_stack = calculate_metrics(y_f, y_f_predict_stack)\n",
    "\n",
    "print(\"Stacking Guided RMSE:\",rmse_g_stack, \"NMSE:\",nmse_g_stack)\n",
    "print(\"Stacking Free RMSE:\",rmse_f_stack,\" NMSE:\", nmse_f_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a8b16d-4605-49a5-b1f5-de60823fa035",
   "metadata": {},
   "source": [
    "| Model / Ensembling       | RMSE Guided       | NMSE Guided       | RMSE Free         | NMSE Free         |\n",
    "| ---------------------- | ----------------- | ----------------- | ----------------- | ----------------- |\n",
    "| ...    |     |     |     |     |\n",
    "| Covariance matrices + Neural Network   | 5.19      | 0.044      | 10.70      | 0.24      |\n",
    "| CNN | 4.53   |  0.05  | 11.82   | 0.19   |\n",
    "| Averaging   | 4.46 | 0.042 | 10.67 | 0.196 |\n",
    "| Stacking    | 4.40    | 0.040    | 11.55    | 0.23    |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcfa3eb-c771-4065-b45a-7d3da757802d",
   "metadata": {},
   "source": [
    "# à completer \n",
    "We can see a small upgrade "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7e4785-3c25-4626-8b9b-d8e24a39591f",
   "metadata": {},
   "source": [
    "While the averaging method assigns equal weight to each base learner, stacking learns an optimal combination of their predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d014d8bd-e2f8-4633-b2b1-650516a719bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
